# MTMM and Error Structure Modelling

In this chapter we'll learn some workflows for situations where we're worried our measured variables are confounded by other unmeasured things besides just the unmeasured 'factors' we're interested in, and how we can address that and reassure ourselves that our inferences about the factor structure are ok. 

```{r message = FALSE, results = FALSE, warning = FALSE}

library(tidyverse)
library(lavaan)
library(ggdag)

```

In the previous example we saw how we can sometimes improve model fit by freeing-up some of the residual covariance terms, rather than doing the typical thing of fixing them at 0. But this feels a bit icky to me -- just pumping out some modification indexes and using that as a basis for opening up some free parameters feels pretty overfitty, because we don't have a strong theory-driven reason for changing the model in that way.

But there *are* more kosher-feeling theory-driven reasons for freeing up some of the residual covariance parameters. Let's talk about two of them: the first relates to convergent validity, the second relates to discriminant validity.

Here's the first example: imagine I have a theory where there's a thing called 'exceptional leadership', and it is made up of 3 unobservable features, like 'self-confidence', 'oratorical skill', and 'robust compassionateness'. So I make up a survey where I ask 12 questions total, 4 per imagined factor. Then I fit a CFA model and find that it does a great job recreating the empirical variance-covariance matrix. There's lots of great convergent validity between the questions I imagine to define the 3 factors. So I get published! But there's a first problem: what if my within-factor variables are correlated not because they are cleanly confounded by 'self-confidence' (which is what I'm trying to convince you of), but instead because the within-factor survey questions are just worded in a really similar way, IE they are confounded by a latent factor we might call 'wording similarity'? This possibility undermines my case for clean confounding.

Now the second example: imagine I do the same analysis described above, but I find my discriminant validity actually doesn't look so hot, IE there are some high between-factor correlations. It is possible that this is just being caused by some of the variables used in different factors being confounded by their shared **measurement approach,** which creates a backdoor path between the factors.

As @Brown2006 puts it:

> "when each construct is assessed by the same measurement approach (e.g., observer rating), it cannot be determined how much of the observed overlap (i.e., factor correlations) is due to method effects as opposed to "true" covariance of the traits."

So we have these two risks:

1.  Maybe some of my within-factor variables are confounded by method effects, which creates the *illusion* of convergent validity. If I go to publish my paper and someone raises this concern, then maybe I won't get published! I'll need to find a way to make my model control for possible method-confounding and *still* show good convergent validity.
2.  Maybe some of my variables of different factors are confounded by method effects, so I don't end up with great discriminant validity. This would be bad, but fitting a model that controls for method effects can maybe make things better.

Fear not: there are two ways of adjusting the model to control for measurement confounding, thereby addressing the above risks.

1.  Add method-specific factors to my model (to control for them in the linear model of each variable). @Brown2006 calls this a **Correlated Methods Model**;

2.  Just freely fit the residual covariances between the observed variables that share a method. @Brown2006 calls this a **Correlated Uniqueness Model.** Because remember, 'Uniqueness' is just a fancy term for variable-specific residual variance.

It's all still just basic linear modelling, and trying to show that the model's results are consistent with the DAG of clean confounding. By adding a method factors or allowing some of the error residuals to be freely fit, I'm controlling for sources of confounding that a reviewer might bring up as a concern, or that might be pulling down my discriminant validity.

Here's how these approaches can improve convergent or divergent validity:

**Convergent validity:** By adding method-factors to the model or freely fitting the residual covariances between the within-factor questions can help me make the case that "see, even when I allow for correlated errors due to *other* unobserved confounders (like common wording or common methods), the factors still do a good job recreating the empirical covariance structure, IE the loadings still look good, so my argument for *mostly* clean confounding is still reasonable." I think this makes sense?

**Divergent validity:** Maybe I can get better discriminant validity, IE reduce the between-factor correlations, by adding those method effects to the linear models, thereby controlling for them. I can do this either by literally adding in some new factors to represent each method, or just by allowing the residual covariances of like-method variables to be freely estimated.

### Simulating Data Based on a DAG {.unnumbered}

Now let's look at an example in detail. This example is taken from @Brown2006, chapter 6.

Some researchers were curious about whether 'happiness' and 'sadness' are totally separate things vs two sides of a single shared spectrum. I guess the implication is that if they are totally separate things then I could be [happy and sad at the same time](https://www.youtube.com/watch?v=U5oIvfraRrU), whereas if they're two sides of a spectrum then I can only ever be one or the other.

This feels like a good factor analysis question! I can collect a bunch of data that I think map to 'happy' and a bunch of other data that I think map to 'sad', fit a CFA, and see whether the two factors have discriminant validity.

This is exactly what @Green-et-al-1993 did. They collected a few columns each for 'happy' and 'sad', fit a factor model, and fit a CFA. Each within-factor column had its own measurement approach, but shared a measurement approach with one of the columns of the other factor. So we are at risk of our estimate of between-factor correlations being confounding due to shared measurement approach, which could be hurting my case for discriminant validity!

Here's how we can show this situation in a DAG:

```{r}

# Set DAG coordinates
dag_coords <- list(
  x = c(
    F1 = 1, 
    F2 = 1,
    H1 = 2,
    H2 = 2,
    H3 = 2,
    S1 = 2,
    S2 = 2,
    S3 = 2,
    M1 = 3,
    M2 = 3,
    M3 = 3),
  y = c(
    F1 = 2.5,
    F2 = 1.5,
    H1 = 2.8,
    H2 = 2.5,
    H3 = 2.2,
    S1 = 1.8,
    S2 = 1.5,
    S3 = 1.2,
    M1 = 2.6,
    M2 = 2,
    M3 = 1.4
  )
)

# Set DAG relationships and aesthetics
measurement_confounding_dag <- ggdag::dagify(
  H1 ~ F1,
  H2 ~ F1,
  H3 ~ F1,
  S1 ~ F2,
  S2 ~ F2,
  S3 ~ F2,
  H1 ~ M1,
  S1 ~ M1,
  H2 ~ M2,
  S2 ~ M2,
  H3 ~ M3,
  S3 ~ M3,
  coords = dag_coords
) %>% 
  
  tidy_dagitty() %>% 
  
  mutate(
    
    node_colour = case_when(
      grepl("^F|M", name) ~ "latent",
      grepl("^H|S", name) ~ "observed"
    ),
    
    edge_colour = case_when(
      grepl("^M", name) & grepl("1$", to) ~ "cornflower blue",
      grepl("^M", name) & grepl("2$", to) ~ "#daed64",
      grepl("^M", name) & grepl("3$", to) ~ "#ed7864",
      grepl("^F", name)                   ~ "black"
    )
  )

# Plot the DAG
measurement_confounding_dag %>%
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point(aes(colour = node_colour)) +
  scale_colour_manual(values = c("dark blue", "#edbc64")) + 
  geom_dag_edges(aes(edge_colour = edge_colour)) +
  geom_dag_text() +
  theme_void()

```

See how the measurment effects M1, M2, and M3 each create a backdoor path between the two factors F1 and F2. So if I want to get better-seeming (and, under the DAG, more accurate) estimate of between-factor correlation, then I need to find a way to close those backdoor paths. The classic way to close these paths would be to condition on the measurement effects by adding them to the linear model, but I can't directly do this because they are unmeasured. But, as discussed above, I can still sort of do it by adding them as factors to my CFA model, or by freely estimating residual correlation between the observed variables that share a measurement approach, which should work if my DAG is mostly accurate.

Unfortunately, the authors of this paper haven't published their data. But we can take this as an opportunity to practice simulating a dataset with relationships implied by a DAG.

```{r}

### Simulate Data from the DAG

# Set seed for replicable results
set.seed(233)

# Set sample size
N <- 305

# Create the dataset
dat_fake <- tibble(
  
  # The factors are uncorrelated in reality, but
  # will be confounded by the measurement effects!
  F1 = rnorm(N, 0, 1),
  F2 = rnorm(N, 0, 1),
  
  # The measurement effects
  M1 = rnorm(N, 0, 1),
  M2 = rnorm(N, 0, 1),
  M3 = rnorm(N, 0, 1),
  
  # The DAG says the measurements are fully determined by the latent factors and measurement effects
  H1 = .8*F1 + 0.7*M1 + rnorm(N, 0, .3),
  H2 = .7*F1 + 0.7*M2 + rnorm(N, 0, .3),
  H3 = .9*F1 + 0.7*M3 + rnorm(N, 0, .3),
  S1 = .8*F2 + 0.7*M1 + rnorm(N, 0, .3),
  S2 = .7*F2 + 0.7*M2 + rnorm(N, 0, .3),
  S3 = .9*F2 + 0.7*M3 + rnorm(N, 0, .3) 
) 

```

Fun! Now we have our fake data to play with. For starters, since we actually *do* have the values of the latent variables in our dataset, we can demonstrate how directly controlling for the measurement effects in a regression model can close the backdoor path between the factors.

```{r}

list(
  lm(H1 ~ S1, dat_fake), 
  lm(H1 ~ S1 + M1, dat_fake)
) %>% 
  
  map(broom::tidy) %>% 
  
  knitr::kable()

```

When we just do the simple regression of H1 on S1 we get a big effect with a highly statistically significant p-value, despite the fact that we *know* there's no causal relationship there! But then when we include the confounding measurement effect in the model this effect vanishes in smoke.

That's all well and good. But in reality we won't have measurements of the latent variables, so we won't be able to directly control for them. Thankfully, we have Factor Analysis. We can control for the measurement effects by estimating the residual correlation between each pair of variables that share a measurement effect. Since, under the DAG, the measurement effects are the only source of correlation between these variables, this should close the backdoor path, IE we should get unbiased estimates of the factor loadings.

....\@Brown2006 calls this an "error theory".....


### Correlated Uniqueness Model {.unnumbered}

To illustrate, we'll fit 2 models: The first is a basic CFA model that just loads each measured variable on its corresponding factor. The second specifies that the residual correlation between the measurement-confounded variables should be freely estimated, IE not fixed at 0.

First let's define our utility function like we did in the previous chapter:

```{r}
### Define a custom function
fit_measures <- function(fit){
  
  summary <- summary(fit, fit.measures = TRUE, standardized = TRUE)
  
  res <- list(
    
    # Chi-Squared
    chi_squared = tibble(
      Test             = "standard chi-squared",
      `DF`             = summary$test$standard$df,
      `Test Statistic` = round(summary$test$standard$stat, 2),
      `p-value`        = summary$test$standard$pvalue) %>% 
      
      mutate(across(everything(), as.character)) %>% 
      
      pivot_longer(everything()),
    
    # RMSEA
    rmsea = summary$fit %>% 
      
      as_tibble(rownames = "stat") %>% 
      
      filter(str_detect(stat, "rmsea")),
    
    # CFI and TLI
    cfi_tli = summary$fit %>% 
      
      as_tibble(rownames = "stat") %>% 
      
      filter(str_detect(stat, "cfi|tli")) 
    
  )
  
  res
  
}

```

```{r}

basic.definition <- 
  'happy =~ H1 + H2 + H3
   sad =~ S1 + S2 + S3
   '

correlated_uniqueness.definition <- 
  'happy =~ H1 + H2 + H3
   sad =~ S1 + S2 + S3
   
   H1 ~~ S1
   H2 ~~ S2
   H3 ~~ S3
   '
basic.fit <- cfa(
  data = dat_fake %>% select(matches("^(H|S)")),
  model = basic.definition
)

correlated_uniqueness.fit <- cfa(
  data = dat_fake %>% select(matches("^(H|S)")),
  model = correlated_uniqueness.definition
)

summary.basic.fit <- summary(basic.fit, standardized = TRUE)
summary.correlated_uniqueness.fit <- summary(correlated_uniqueness.fit, standardized = TRUE)

summary.basic.fit
summary.correlated_uniqueness.fit

fit_measures(basic.fit) %>% 
  
  knitr::kable(caption = "Basic Model")

fit_measures(correlated_uniqueness.fit) %>% 
  
  knitr::kable(caption = "Correlated Uniqueness Model")


```

Here we see that under the basic model we have some moderate correlation between the `happy` and `sad` factors, which is a bit of a murky result: it doesn't tell us one way or the other whether happiness and sadness are separate constructs I can feel together or two extremes of the same feeling. But under the correlated uniqueness model this correlation evaporates because we've controlled for the measurement effects, closing the backdoor path between `happy` and `sad`. This model also greatly improves goodness-of-fit, which makes sense because it better reflects the true data-generating process we coded up.

We also could have controlled for the measurement effects by including measurement factors, IE by adopting a 'Correlated Methods Model'. I tried this but I actually I couldn't get this model to converge, regardless of whether its method factors were correlated or uncorrelated (an 'Uncorrelated Methods Model'. @Brown2006 actually mentions this as a common issue, and favours the Correlated Uniqueness Model for that reason. In his words:

> "an overriding drawback of the correlated methods model is that it is usually empirically underidentified. Consequently, a correlated methods solution will typically fail to converge. If it does converge, the solution will usually be associated with Heywood cases \[negative variance estimates\] and large standard errors"

Now let's consider the other case in which measurement effects might be hurting us: the case in which *within*-factor measurements are confounded by measurement effects. Here's the DAG:

```{r}

dag_coords <- list(
  x = c(
    F1 = 1, 
    F2 = 1,
    H1 = 2,
    H2 = 2,
    H3 = 2,
    S1 = 2,
    S2 = 2,
    S3 = 2,
    M1 = 3,
    M2 = 3),
  y = c(
    F1 = 2.5,
    F2 = 1.5,
    H1 = 2.8,
    H2 = 2.5,
    H3 = 2.2,
    S1 = 1.8,
    S2 = 1.5,
    S3 = 1.2,
    M1 = 2.5,
    M2 = 1.5
  )
)

# Set DAG relationships and aesthetics
measurement_confounding_dag <- ggdag::dagify(
  H3 ~ F1,
  S2 ~ F2,
  H1 ~ M1,
  H2 ~ M1,
  H3 ~ M1,
  S1 ~ M2,
  S2 ~ M2,
  S3 ~ M2,
  coords = dag_coords
) %>% 
  
  tidy_dagitty() %>% 
  
  mutate(
    
    node_colour = case_when(
      grepl("^F|M", name) ~ "latent",
      grepl("^H|S", name) ~ "observed"
    ),
    
    edge_colour = case_when(
      grepl("M1", name)  ~ "cornflower blue",
      grepl("M2", name) ~ "#ed7864",
      grepl("^XX", name) & grepl("3$", to) ~ "#ed7864",
      grepl("^F", name)                   ~ "black"
    )
  )

# Plot the DAG
measurement_confounding_dag %>%
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point(aes(colour = node_colour)) +
  scale_colour_manual(values = c("dark blue", "#edbc64")) + 
  geom_dag_edges(aes(edge_colour = edge_colour)) +
  geom_dag_text() +
  theme_void()

```

This is the 'true' data-generating process we'll be simulating data from in a moment. Notice that even though the researcher (who can't see this DAG) might think that the unobseved factor causally influences all 3 measured variables, the reality is that each factor only influences one of the measured variables. However, the purported within-factor variables are confounded by measurement method.

Let's simulate the data and analyze:

```{r}

# Set seed for replicable results
set.seed(233)

# Set sample size
N <- 30000

# Create the dataset
dat_fake <- tibble(
  
  # Create some uncorrelated factors
  F1 = rnorm(N, 0, 1),
  F2 = rnorm(N, 0, 1),
  
  # Create some measurement effects
  M1 = rnorm(N, 0, 1),
  M2 = rnorm(N, 0, 1),
  
  # The DAG says only H3 and S2 are influenced by the factors, but all variables are influenced by a measurement effect.
  H1 = 0.7*M1 + rnorm(N, 0, .3),
  H2 = 0.8*M1 + rnorm(N, 0, .3),
  H3 = 0.9*F1 + 0.8*M1 + rnorm(N, 0, .3),
  S1 = 0.7*M2 + rnorm(N, 0, .3),
  S2 = 0.7*F2 + 0.8*M2 + rnorm(N, 0, .3),
  S3 = 0.7*M2 + rnorm(N, 0, .3) 
) 

```

First let's fit a basic naive CFA model that does the standard thing of keeping the covariances between variables fixed at 0. Based on the DAG, we should expect this model to return a strong (publishable) but misleading answer -- it will notice the correlation between variables that are considered within-factor under our hypothesis, and say 'wow so correlated, that's consistent with them being *caused* by that factor'. But we know this is wrong: their correlation is simply driven by the shared measurement method:

```{r}

basic.definition <- 
  'happy =~ H1 + H2 + H3
   sad =~ S1 + S2 + S3
   '

basic.fit <- cfa(
  data = dat_fake,
  model = basic.definition
)

summary(basic.fit, standardized = TRUE)

```

And there you have it -- just as foretold, we have super strong factor loadings for all the variables, even those that are not actually causally influenced by the factor! So it may *look* like I have strong convergent validity, but hopefully if we try to publish this, a reviewer will raise the possibility that these correlations are confounded by measurement effects.

Now I'm going to try closing the backdoor paths between the non-factor-caused variables by allowing the model to learn the covariances, thereby hopefully controlling for unobserved sources of confounding (like the measurement effect). If the loadings stay strong, then my claims to convergent validity are more reasonable.

```{r}

correlated_uniqueness.definition <- 
  'happy =~ H1 + H2 + H3
   sad   =~ S1 + S2 + S3
   
   H1 ~~ H2
   H1 ~~ H3
   H2 ~~ H3
   S1 ~~ S2
   S1 ~~ S3
   S2 ~~ S3
   '

correlated_uniqueness.fit <- cfa(
  data = dat_fake,
  model = correlated_uniqueness.definition
)

summary(correlated_uniqueness.fit, standardized = TRUE)

```

Uh-oh...the model failed to converge :(. Apparently this is a common thing with CFA models that try to learn the correlation between within-factor variables -- the parameters are non-identified because you're asking the model to learn their correlation simultaneously in two different parameters: the factor loading and the covariance parameter. [This Stack Exchange thread](https://stackoverflow.com/questions/44114501/model-identification-in-lavaan-for-r) explains it nicely.