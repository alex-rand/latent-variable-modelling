[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Latent Variable Modelling Workflow Reference",
    "section": "",
    "text": "This document does two things:\n\nGive a conceptual overview of latent variable modelling, especially confirmatory factor analysis (CFA).\nProvide workflows I’ve cobbled together from a few different textbooks, including examples with data from those textbooks or from open datasets I found online.\n\n\n\n\nThe first book on latent variable modelling I read was Gorsuch (1983). This was a nice conceptual introduction, but the applied examples weren’t great. I’ve since found a few sources with data and R code to work with. I also cite these sources throughout as I cobble together the workflows.\n\nLatent Variable Modelling with R, by Finch (2015). They helpfully provide all of the datasets here.\nPrinciples and Practice of Structural Equation Modeling, by Kline (2011). The publisher provides data and code here.\nConfirmatory Factor Analysis for Applied Research, by Brown (2006). No R code available, but there’s some data at the university website\nQuantitative Analysis Using Structural Equation Modelling, a free online course provided by the Wetland and Aquatic Research Center of the United States Geological Survey.\nThe lavaan documentation has some nice worked examples too.\n\nI’ll mostly be using lavaan and tidyverse, but maybe also some brms at some point.\n\n\n\n\nBrown, Timothy A. 2006. Confirmatory Factor Analysis for Applied Research.\n\n\nFinch, French, W. Holmes. 2015. Latent Variable Modeling with r.\n\n\nGorsuch, Richard L. 1983. Factor Analysis, 2nd Edition.\n\n\nKline, Rex B. 2011. Principles and Practice of Structural Equation Modeling."
  },
  {
    "objectID": "cfa-intro.html",
    "href": "cfa-intro.html",
    "title": "Introduction to CFA",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggdag)"
  },
  {
    "objectID": "cfa-intro.html#the-whole-game",
    "href": "cfa-intro.html#the-whole-game",
    "title": "Introduction to CFA",
    "section": "The Whole Game",
    "text": "The Whole Game\nThe Whole Game of Confirmatory Factor Analysis (CFA) is that I’m trying to convince you my observed variables are confounded by some unmeasured variables. Usually I’m trying to show that the observed ariables are confounded in a very particular way, where a few small groups of variables are confounded only by one unmeasured variable per group.\nSo here’s the archetypal DAG of a CFA, where the Xs are observed variables, and F1 is an unmeasured variable I am trying to convince you exists. We can call this The Primordial CFA DAG:\n\n\nCode\n# Set DAG coordinates\ndag_coords <- list(\n  x = c(\n    F1 = 1, \n    X1 = 2,\n    X2 = 2,\n    X3 = 2\n  ),\n  y = c(\n    F1 = 1.5,\n    X1 = 1.8,\n    X2 = 1.5,\n    X3 = 1.2\n  )\n)\n\n# Set DAG relationships and aesthetics\nmeasurement_confounding_dag <- ggdag::dagify(\n  X1 ~ F1,\n  X2 ~ F1,\n  X3 ~ F1,\n  coords = dag_coords\n) %>% \n  \n  tidy_dagitty() %>% \n  \n  mutate(\n    ` ` = case_when(\n      grepl(\"^F\", name) ~ \"latent\",\n      grepl(\"^X\", name) ~ \"observed\"\n    ))\n\n# Plot the DAG\nmeasurement_confounding_dag %>%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(colour = ` `)) +\n  scale_colour_manual(values = c(\"dark blue\", \"#edbc64\")) + \n  geom_dag_edges() +\n  geom_dag_text() +\n  theme(legend.title = element_blank()) +\n  theme_void()\n\n\n\n\n\nCFA is just a way of trying to see whether the patterns of variance and covariance in your data are consistent with the above DAG, or a similar one.\nThe classic way of testing whether your data are consistent with a DAG is to condition on some of the variables, perhaps by including it as a predictor in a linear regression model, and see whether the patterns of correlation change in the ways the DAG expects based on the rules of d-separation. For the above DAG, this would mean controlling for F1 and seeing whether the correlations between X1, X2, and X3 decrease as a result. But in CFA we always assume the confounder is unmeasured, so we can’t directly control for it. Instead, we can only try to argue for our DAG in a more hand-wavy sort of way: we expect confounded variables to be correlated with each other, and uncounfounded variables to not be correlated with each other. This is why we focus on the empirical correlation matrix as the basis for our model: if a few of my variables are very correlated with each other then that is consistent with them being confounded by the same unobserved variable. But it is not proof! You can never prove a DAG, after all.\nSo interpreting a CFA model is all about checking to see whether the correlations between the variables are consistent with what we would expect to see under the DAG where each group of variables is confounded by a single unmeasured variable. In the next few chapters we’ll look at some examples of how people have liked to make the case for their unmeasured-confounder DAG."
  },
  {
    "objectID": "traditional-cfa-workflow.html",
    "href": "traditional-cfa-workflow.html",
    "title": "1  Traditional CFA Workflow",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(ggdag)"
  },
  {
    "objectID": "traditional-cfa-workflow.html#example-1-toxic-striving-energy",
    "href": "traditional-cfa-workflow.html#example-1-toxic-striving-energy",
    "title": "1  Traditional CFA Workflow",
    "section": "Example 1: Toxic Striving Energy",
    "text": "Example 1: Toxic Striving Energy\nThe first example we’ll look at is from Finch (2015), chapter 3. The practice dataset is introduced on page 10. It is from a study about human motivation. The dataset is a weird questionnaire called the ‘Achievement Goal Scale’ (AGS), which asks people 12 questions about how much toxic striving energy they have. The dataset provided seems to have lots of mysterious columns in it, but we’re probably good to just keep the columns with responses to the AGS questionnaire:\n\n\nCode\n### Load the data\ndat_raw <- foreign::read.spss('../data/finch-and-french/edps744.sav') \n  \n### Clean the data\ndat_ags <- dat_raw %>% \n\n  # Convert to a data frame for ease of use\n  as.data.frame() %>% \n  \n  # Keep only columns that start with the prefix 'ags' followed by a question number\n  select(matches(\"ags\\\\d\")) \n\n\n\nData Exploration\nWe don’t want to do too much exploration before fitting our factor models, because the whole game of CFA is to commit to our hypotheses before checking what the data looks like, so we don’t mislead ourselves with forking paths. But just for fun, we can explore the distributions of the answers to each of the 12 questions:\n\n\nCode\ndat_ags %>% \n\n  # Pivot to prepare the data for visualization\n  pivot_longer(\n    cols      = everything(),\n    names_to  = \"question\",\n    values_to = \"response\",\n    names_transform = list(question = fct_inorder)  \n  ) %>% \n\n  # Plot\n  ggplot() +\n  geom_histogram(aes(x = response)) + \n  theme_bw() + \n  facet_wrap(~question)\n\n\n\n\n\nSeems like some questions have different means and variances from each other. For example, the answers to ags11 and ags12 are relatively flat, while the answers to ags4 and ags5 are more bunched up around the highest values. The responses clearly skew towards higher values in aggregate.\nWe can also do some healthy exploration of missingness in the dataset. For starters: what proportion of values are missing in each row?\n\n\nCode\ndat_ags %>% \n  \n  # Calculate the proportion of missing values \n  summarise_all(~ sum(is.na(.)) / (sum(is.na(.) + sum(!is.na(.))))) %>% \n  \n  # Rounding to make the results more presentable\n  mutate(across(everything(), round, 6)) %>% \n  \n  # Create the table\n  knitr::kable(title = \"Proportion of Missing Responses in Each Column\") \n\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(everything(), round, 6)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nags1\nags2\nags3\nags4\nags5\nags6\nags7\nags8\nags9\nags10\nags11\nags12\n\n\n\n\n1.1e-05\n5e-06\n5e-06\n1.6e-05\n1.6e-05\n1.1e-05\n1.6e-05\n1.6e-05\n1.1e-05\n2.2e-05\n1.1e-05\n1.6e-05\n\n\n\n\n\nThat’s very little missingness. Probably no need to do multiple imputation here.\nThe authors also do a preliminary test of whether the responses are normally distributed, since this is one of the fundamental assumptions of maximum likelihood estimation. Kristoffer Magnusson has created a cool interactive teaching tool that nicely illustrates this point. It is worth remembering that we do not make this type of assumption for linear regression in general – only for maximum likelihood estimates. All we need assume for linear regression is that the residuals are normally distributed, as opposed to the data themselves. This common misunderstanding can lead researchers to commit what Richard McElreath has called ‘histomancy’.\nTo evaluate the assumption of normalness underlying maximum likelihood estimation, the authors do what seems to be a multivariate version of a classic ‘normal probability plot’. These are explained nicely in this stack exchange thread. They also produce some of the classic tests of skew and kurtosis, which I don’t want to get into here. This youtuber has nice introductory videos about these topics.\n\n\nCode\n# Run the Mardia tests for normalness\nmardia.object <- psych::mardia(dat_ags)\n\n\n\n\n\nCode\n# Plot the multivariate version of the normal probability plot\nplot(mardia.object)\n\n# Present the outputs we're interested in\ntibble(\n  \"Skew\" = mardia.object$skew,\n  \"Skew p-value\" = mardia.object$p.skew,\n  \"Kurtosis\" = mardia.object$kurtosis,\n  \"Kurtosis p-value\" = mardia.object$p.kurt\n) %>% \n  \n  knitr::kable()\n\n\n\n\n\nSkew\nSkew p-value\nKurtosis\nKurtosis p-value\n\n\n\n\n2359.475\n0\n40.52999\n0\n\n\n\n\n\nThe plotted points don’t seem to fit the straight line super well, which suggests that the normalness assumption may not hold here. Also, the hypothesis tests for skew and kurtosis return some mighty low p-values, suggesting that we’ve got lots of each of them. So maybe maximum likelihood estimation isn’t such a good idea here?\nThe authors proceed with it anyway for pedogogical reasons, because they want to illustrate how the maximum likelihood estimates differ from estimates arrived at using other methods.\n\n“In actual practice, given the lack of multivariate normality that seems apparent in the previous results, we would likely not use ML and instead rely on the alternative estimation approach.”\n\n\n\nModel Fitting\nThe researchers who collected the data do what good factor analysts do: they look to the literature to set up some clear and specific candidate hypotheses, and see the degree to which this new data is compatible with each of them.\nOne of the candidate hypotheses is that a person’s toxic striving energy (‘achievement goal orientedness’?) is secretly driven by four platonic unobservable things, namely:\n\nMastery Approach ‘MAP’ (eg. “I want to learn as much as possible”);\nMastery Avoidant ‘MAV’ (eg. “I want to avoid learning less than I possibly could”);\nPerformance Approach ‘PAP’ (eg. “I want to do well compared to other students”);\nPerformance Avoidant ‘PAV’ (eg. “It is important for me to avoid doing poorly compared to other students”)\n\nWe’ll call the above hypothesis H1. But there’s another hypothesis that says actually the ‘Mastery’ variables are just one monolithic thing, so really there are only 3 factors, namely ‘Mastery’, ‘PAP’, and ‘PAV’. We’ll call this one H2. These will be the two candidate hypotheses we’re gonna test via factor analysis.\nThe way lavaan works is that you need to separately define the model syntax as a string, and then feed that string to one of the model-fitting functions like cfa() . Then we can call the summary() function to get a big table of outputs.\n\n\nCode\n# Define the relationships from my hypothesis\nh1.definition <- \n'map=~ags1+ags5+ags7\nmav=~ags2+ags6+ags12\npap=~ags3+ags9+ags11\npav=~ags4+ags8+ags10'\n\n# Fit the model\nh1.fit <- cfa(\n  data  = dat_ags,\n  model = h1.definition\n)\n\n# Look at the results\nh1.summary <- summary(h1.fit, fit.measures = TRUE, standardized = TRUE)\n\nh1.summary\n\n\nlavaan 0.6.16 ended normally after 48 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        30\n\n                                                  Used       Total\n  Number of observations                           419         432\n\nModel Test User Model:\n                                                      \n  Test statistic                               328.312\n  Degrees of freedom                                48\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              3382.805\n  Degrees of freedom                                66\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.915\n  Tucker-Lewis Index (TLI)                       0.884\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -7014.070\n  Loglikelihood unrestricted model (H1)      -6849.914\n                                                      \n  Akaike (AIC)                               14088.141\n  Bayesian (BIC)                             14209.277\n  Sample-size adjusted Bayesian (SABIC)      14114.078\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.118\n  90 Percent confidence interval - lower         0.106\n  90 Percent confidence interval - upper         0.130\n  P-value H_0: RMSEA <= 0.050                    0.000\n  P-value H_0: RMSEA >= 0.080                    1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.055\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  map =~                                                                \n    ags1              1.000                               0.840    0.746\n    ags5              0.774    0.057   13.564    0.000    0.650    0.682\n    ags7              1.100    0.064   17.263    0.000    0.924    0.895\n  mav =~                                                                \n    ags2              1.000                               0.923    0.627\n    ags6              0.974    0.078   12.523    0.000    0.899    0.796\n    ags12             1.039    0.096   10.805    0.000    0.959    0.644\n  pap =~                                                                \n    ags3              1.000                               1.284    0.840\n    ags9              0.853    0.038   22.349    0.000    1.095    0.870\n    ags11             1.103    0.052   21.178    0.000    1.416    0.841\n  pav =~                                                                \n    ags4              1.000                               0.929    0.771\n    ags8              1.599    0.084   19.091    0.000    1.486    0.855\n    ags10             1.525    0.073   20.861    0.000    1.418    0.921\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  map ~~                                                                \n    mav               0.709    0.079    9.000    0.000    0.914    0.914\n    pap               0.066    0.060    1.093    0.274    0.061    0.061\n    pav               0.056    0.043    1.289    0.197    0.072    0.072\n  mav ~~                                                                \n    pap               0.163    0.072    2.265    0.023    0.138    0.138\n    pav               0.178    0.053    3.355    0.001    0.207    0.207\n  pap ~~                                                                \n    pav               1.143    0.102   11.236    0.000    0.958    0.958\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .ags1              0.562    0.047   11.951    0.000    0.562    0.443\n   .ags5              0.486    0.038   12.780    0.000    0.486    0.535\n   .ags7              0.211    0.032    6.602    0.000    0.211    0.198\n   .ags2              1.312    0.102   12.825    0.000    1.312    0.606\n   .ags6              0.469    0.049    9.537    0.000    0.469    0.367\n   .ags12             1.300    0.103   12.669    0.000    1.300    0.586\n   .ags3              0.690    0.059   11.671    0.000    0.690    0.295\n   .ags9              0.386    0.036   10.769    0.000    0.386    0.244\n   .ags11             0.831    0.071   11.639    0.000    0.831    0.293\n   .ags4              0.588    0.045   12.959    0.000    0.588    0.405\n   .ags8              0.815    0.070   11.602    0.000    0.815    0.269\n   .ags10             0.362    0.043    8.423    0.000    0.362    0.153\n    map               0.706    0.083    8.514    0.000    1.000    1.000\n    mav               0.852    0.128    6.655    0.000    1.000    1.000\n    pap               1.648    0.158   10.416    0.000    1.000    1.000\n    pav               0.864    0.094    9.198    0.000    1.000    1.000\n\n\nCode\nsemPlot::semPaths(h1.fit)\n\n\n\n\n\nThat’s a lot of outputs. Let’s break down the output into smaller bite-sized chunks.\n\n\nGoodness of Fit Statistics\n\nChi-Squared Statistic\nThe first thing to look at is the chi-squared statistic from the ‘User Model’, IE the model I, the user, have just fit. I like to think of this as a measure of how different the model’s reconstructed correlation matrix looks compared to the actual empirical correlation matrix of the data. So we use this statistic to test the null hypothesis “there is no significant difference between model’s reconstructed correlation matrix and the empirical one”. So, confusingly, we’re actually hoping to accept the null hypothesis here. This model returns a value of 328.312 with a vanishingly small p-value, so we reject the null hypothesis, which is bad: it suggests our model isn’t doing a good job replicating the empirical correlation matrix.\nHere’s a quote from Gorsuch (1983) that explains this stuff from the slightly different angle:\n\n“The test of significance [for a CFA model fit by maximum likelihood] gives a chi-square statistic with the null hypothesis being that all the population covariance has been extracted by the hypothesized number of factors. If the chi-square is significant at the designated probability level, then the residual matrix still has significant covariance in it.”\n\nSo this chi-squared statistic provides a first look at goodness-of-fit, but Finch (2015) say it is actually not very trustworthy in practice because the null hypothesis is sort of crazy: we want a more permissive test than just whether the model is perfectly recreating the empirical correlation matrix.\n\n“this statistic is not particularly useful in practice because it tests the null hypothesis that [the model-reconstructed correlation matrix is equal to the empirical correlation matrix], which is very restrictive. The test will almost certainly be rejected when the sample size is sufficiently large… In addition, the chi-square test relies on the assumption of multivariate normality of the indicators, which may not be tenable in many situations.”\n\nSo we’re gonna wanna look at statistics other than just chi-squared for goodness-of-fit, but it seems like a fine place to start. Let’s look at the chi-squared statistic of our model:\n\n\nCode\n### Create a nice summary table\ntibble(\n  Test             = \"standard chi-squared\",\n  `DF`             = h1.summary$test$standard$df,\n  `Test Statistic` = round(h1.summary$test$standard$stat, 2),\n  `p-value`        = h1.summary$test$standard$pvalue\n) %>% \n  \n  mutate(across(everything(), as.character)) %>% \n  \n  pivot_longer(everything()) %>% \n  \n  knitr::kable()\n\n\n\n\n\nname\nvalue\n\n\n\n\nTest\nstandard chi-squared\n\n\nDF\n48\n\n\nTest Statistic\n328.31\n\n\np-value\n0\n\n\n\n\n\nIt takes lots of skill and experience to have a sense of whether a test statistic is big or small given the degrees of freedom at play, but we can see from the p-value that we reject the null hypothesis in a big way. This is bad – it suggests that, given our assumptions, there’s a big difference between our model and the data.\n\n\nRoot Mean Squared Error Approximation (RMSEA)\nAnother one people like to go with is the Root Mean Squared Error Approximation (RMSEA). This statistic takes some math and background to understand, which I’m not going to go over here. I found this document to be the clearest (but also pretty mathy) explanation.\nEssentially, RMSEA is a weighted sum of the discrepancies between the model’s reconstructed correlation matrix and the empirical correlation matrix. But it also does a nice thing where it discounts model complexity and sample size to help us not overfit. Here’s the definition:\n\\(\\text{RMSEA} = \\sqrt{\\dfrac{χ^2 - \\text{df}}{\\text{df}(n-1)}}\\)\nSee how it takes the chi-squared statistic and divides it by degrees of freedom (as a proxy for model complexity) and sample size? This makes for a more conservative measure of goodness-of-fit. Apparently the square-root is used “to return the index to the same metric as the original standardized parameters”. I don’t really understand that part… is it because a Chi-squared random variable is the squared version of a normal standard variable?\nAs with the raw chi-squared statistic, we want RMSEA to be small because it is intended as a measure of the distance between the empirical correlation matrix and the model-estimated correlation matrix. According to Finch (2015), people like to say:\n\nRMSEA <= 0.05 is a ‘good fit’;\n0.05 < RMSEA <= 0.08 is an ‘ok fit’\nRMSEA > .08 is a ‘bad fit’.\n\nLet’s check the RMSEA of our model:\n\n\nCode\n# make a nice summary table\nh1.summary$fit %>% \n  \n  as_tibble(rownames = \"stat\") %>% \n  \n  filter(str_detect(stat, \"rmsea\")) %>% \n  \n  knitr::kable()\n\n\n\n\n\nstat\nvalue\n\n\n\n\nrmsea\n0.1180574\n\n\nrmsea.ci.lower\n0.1061525\n\n\nrmsea.ci.upper\n0.1303058\n\n\nrmsea.ci.level\n0.9000000\n\n\nrmsea.pvalue\n0.0000000\n\n\nrmsea.close.h0\n0.0500000\n\n\nrmsea.notclose.pvalue\n0.9999999\n\n\nrmsea.notclose.h0\n0.0800000\n\n\n\n\n\nYikes – looks like our whole RMSEA, as well as its confidence interval, are above the ‘bad fit’ conventional threshold of .08. This corroborates what we saw with the chi-squared statistic above.\n\n\nComparative Fit Index (CFI) and Tucker-Lewis Index (TLI)\nCFI seems to be the most trusted and widely-used tool for assessing goodness of fit in a CFA. Basically the idea is that we ask: “how much does the chi-squared statistic of my model differ from the chi-squared statistic of the worst model I can think of?”, where the conventional “worst model I can think of” is the model where I assume all of my observed variables are totally uncorrelated. This sort of has the opposite flavour of the deviance statistic I’m already familiar with, which compares the current model with “the best model I can think of.”\n\\(\\text{CFI} = 1 - \\dfrac{\\text{max}(χ^2_T - \\text{df}_T, 0)}{\\text{max}(χ^2_0 - \\text{df}_0, 0)}\\)\nActually, the numerator and denominator are both equal to the ‘non-centrality parameter’ of their respective candidate distributions. I’m not gonna get into this, but this is an idea that also shows up in power analysis as a way of comparing the null and candidate hypotheses.\nWe want to end up with a CFI as close to 1 as possible, because that suggests a big difference between my model and the worst possible model. So people say we can sort of think of this as analogous to \\(R^2\\) from linear regression. People seem to have adopted 0.95 as an arbitrary cutoff for ‘good fit’ for the CFI.\nIf you want to learn more about the CFI, I found this article a well-written resource.\nTucker-Lewis Index seems to be pretty similar to CFI, and we interpret it in the same way. Let’s look at both of them:\n\n\nCode\n# Make a nice summary table\nh1.summary$fit %>% \n  \n  as_tibble(rownames = \"stat\") %>% \n  \n  filter(str_detect(stat, \"cfi|tli\")) %>% \n  \n  knitr::kable()\n\n\n\n\n\nstat\nvalue\n\n\n\n\ncfi\n0.9154874\n\n\ntli\n0.8837951\n\n\n\n\n\nLooks like the CFI and TLI look ok, but don’t meet the conventional .95 cutoff. So they are in line with the chi-squared and RMSEA in suggesting that our goodness-of-fit isn’t so good.\n\n\n\nConvergent Validty\nLike I said before: when I’m doing factor analysis, my goal is to convince my research peers that my observed variables are confounded by an unobserved variable, and that therefore they provide a way of ‘measuring’ that unobserved variable. This seems like an ontologically dubious framing, and it also seems impossible to prove. But people who do research have settled on a few ways of trying to make this case.\nOne such way is to take all of the measured variables I’m imagining to be caused by the same unmeasured factor and show that they are indeed correlated with each other, because this is what we would expect under the simple DAG where they are all confounded by the same latent variable. When this happens, I can say that my factor has Convergent Validity. In the words of Gorsuch (1983):\n\n“Convergent validity occurs when several variables deemed to measure the same construct correlate with each other.”\n\nOr, as Kline (2011) puts it:\n\n“Variables presumed to measure the same construct show convergent validity if their intercorrelations are appreciable in magnitude.”\n\nIt seems like to make the jump from ‘these measured variables are correlated’ to ‘these measured variables are caused by a single shared latent factor’ I would need to be also making the further assumption that there aren’t other unmeasured confounders muddying up the observed covariances. It’s DAGs all the way down…\nBased on the textbooks I’m working from, here are a few questions I can answer if I want to make the case for Convergent Validity:\n\nAre the factor loadings statistically significant?\nAre the standardized factor loadings pretty big (IE pretty close to 1)?\nAre the standardized within-factor loadings pretty similar to each other?\nDo the measurements seem to have good ‘reliability’ as measured by something like Chronbach’s Alpha, Average Variance Extracted, or Composite Reliability?\nAre all of the residual variances less than .50, IE is the model explaining at least half the variance of each model?\n\nFirst we can look at the factor loadings. These are essentially just the regression coefficients of each factor on each of the outcome variables for which it was allowed to be a covariate. So we want them to be big and significant.\n\n\nCode\n### Make a nice summary table of the factor loadings\nh1.summary$pe %>% \n  \n  as_tibble() %>% \n  \n  # Keep only the rows with info on factor loadings\n  slice(1:12) %>% \n  \n  # Clean up the important values, then combine them into a single column\n  mutate(\n    std.all = round(std.all, 2),\n    std.all = paste0(std.all, \", pvalue = \", pvalue, \")\")\n  ) %>% \n  \n  # reformat the table\n  select(lhs, rhs, std.all) %>% \n  \n  pivot_wider(\n    names_from = \"lhs\", \n    values_from = \"std.all\",\n    values_fill = \"0\"\n  ) %>% \n  \n  column_to_rownames(\"rhs\") %>% \n  \n  knitr::kable(caption = \"Standardized factor loadings and p-values\")\n\n\n\nStandardized factor loadings and p-values\n\n\n\n\n\n\n\n\n\n\nmap\nmav\npap\npav\n\n\n\n\nags1\n0.75, pvalue = NA)\n0\n0\n0\n\n\nags5\n0.68, pvalue = 0)\n0\n0\n0\n\n\nags7\n0.9, pvalue = 0)\n0\n0\n0\n\n\nags2\n0\n0.63, pvalue = NA)\n0\n0\n\n\nags6\n0\n0.8, pvalue = 0)\n0\n0\n\n\nags12\n0\n0.64, pvalue = 0)\n0\n0\n\n\nags3\n0\n0\n0.84, pvalue = NA)\n0\n\n\nags9\n0\n0\n0.87, pvalue = 0)\n0\n\n\nags11\n0\n0\n0.84, pvalue = 0)\n0\n\n\nags4\n0\n0\n0\n0.77, pvalue = NA)\n\n\nags8\n0\n0\n0\n0.85, pvalue = 0)\n\n\nags10\n0\n0\n0\n0.92, pvalue = 0)\n\n\n\n\n\nFirstly, notice that all of the non-fixed loadings are highly statistically significant, with all p-values smaller than .01. This is good! Super statistically-significant loadings are a necessary sign that our measured variables are actually good proxies for the imaginary ‘latent’ factor we’re purporting to use them to measure.\nNext, Kline (2011) says that we can start assessing convergent validity by just looking at the standardized loadings can in isolation. In his words on page 344:\n\n“[with reference to a CFA model he has fit]: A few other standardized coefficients are rather low, such as .433 for the self-talk indicator of constructive thinking, so evidence for convergent validity is mixed.”\n\nTo my eye it looks like some of the standardized loadings on the ‘mav’ factor are pretty low. Also, it seems like only ‘pap’ has really consistent loadings across all of its measured variables: the other three factors all have a bunch of variance between their loadings. So this all seems like a bit of a red flag.\nKline (2011), on page 307, gives yet another way of assessing convergent validity: he fits a CFA, then asks whether “the majority” of the variances of the observed variables have been explained, IE whether the standardized residual variances are <50. I guess the idea is that the amount of variance explained for a variable by a factor depends on how correlated In his words:\n\n“[in reference to one of his models:] [the] model fails to explain the majority (> .50) of variance for a total of four out of eight indicators, which indicates poor convergent validity.”\n\nLet’s have a look at the residual variances. These are just the proportion of the empirical variance of each measured variable that is left unexplained by the linear models that make up the factor analysis.\n\n\nCode\nh1.summary$pe %>% \n  \n  as.data.frame() %>% \n  \n  filter(grepl(\"ags\\\\d\", lhs)) %>% \n  \n  mutate(factor = case_when(\n    lhs %in% c(\"ags1\", \"ags5\", \"ags7\") ~ \"map\",\n    lhs %in% c(\"ags2\", \"ags6\", \"ags12\") ~ \"mav\",\n    lhs %in% c(\"ags3\", \"ags9\", \"ags11\") ~ \"pap\",\n    lhs %in% c(\"ags4\", \"ags8\", \"ags10\") ~ \"pav\",\n  )) %>% \n  \n  select(factor, \"var\" = lhs, std.all) %>% \n  \n  knitr::kable()\n\n\n\n\n\nfactor\nvar\nstd.all\n\n\n\n\nmap\nags1\n0.4434896\n\n\nmap\nags5\n0.5345470\n\n\nmap\nags7\n0.1980907\n\n\nmav\nags2\n0.6063553\n\n\nmav\nags6\n0.3670913\n\n\nmav\nags12\n0.5858185\n\n\npap\nags3\n0.2950189\n\n\npap\nags9\n0.2436200\n\n\npap\nags11\n0.2927905\n\n\npav\nags4\n0.4050030\n\n\npav\nags8\n0.2694999\n\n\npav\nags10\n0.1526773\n\n\n\n\n\nLooks like the model has mostly done a good job for the ‘Performance’ factors, with all variables having at least ~60% of their variance explained. But the ‘Mastery’ factors are worse, especially ‘mav’, with two of its three variables having only ~40% of their variances explained. This is yet more evidence that the ‘mav’ factor isn’t doing so great a job.\n~~Lastly, Gorsuch suggests another way of testing for convergent validity:\n\nfactor loadings of several variables hypothesized to relate to the construct can also be tested for significance. They could be specified as equal for the one model and the chi-square for that model subtracted from another hypothesized factor structure where they are allowed to vary. If the two differ significantly from each other, then one or more of the variables is more related to the construct than one or more of the other variables.”\n\nLet’s try this out: we’ll fit another model that assumes all of the within-factor loadings are equal, and see if that results in a statistically significant reduction in goodness-of-fit. If it does, then we lose some evidence of convergent validity.~~\n\n\nReliability\nIn looking for Convergent Validity we were mostly just comparing the parameter estimates (loadings) and residual variances of individual variables. We were trying to figure out whether these variables belong together. But in addition to tests of validity to decide which columns belong in the scale, people like to test the overall scale itself by invoking the terrible concept named ‘Reliability’. Reliability purports to be ‘true variance in the underlying construct’ as a proportion of ‘total variance in the scores of a subscale’. This seems to me like an ontologically dubious concept, but it’s what we’re working with.\nThis all feels pretty similar to Convergent Validity to me. We’re just trying to show that the data are consistent with all of the factor-level columns being confounded by the same variable.\nThe all-time classic ‘reliability’ measure is called Cronbach’s Alpha. Cronbach didn’t actually invent it, so hello Stigler’s Law. Here’s what it looks like:\n\\(\\alpha = (\\dfrac{k}{1-k}) (1 - \\dfrac{\\sum\\sigma_y^2}{\\sigma_T^2})\\)\nThe term on the right is doing most of the work: its denominator is the variance of the column that contains the rowwise sums of my dataset. Its numerator is the sum of the variances of each column. So we’re asking: ‘is the variance of the sums larger than the variance of the individual columns?’ This will be true if the columns are generally pretty correlated, because the sums will stack up the raw values, instead of them cancelling each other out. So really we’re just asking: are the columns generally pretty correlated?‘. If my columns are pretty correlated and I make the standard assumption that no other latent factors are influencing my observed values (an insane assumption), then I can feel comfortable saying that Cronbach’s Alpha is useful for figuring out whether my measurements are all loading on the same ’latent’ variable. Since the observed values are gonna be consistent with each other if this is true, people like to say that Cronbach’s Alpha gives a picture of ‘Internal Consistency Reliability’.\nLike with Convergent Validity, this is all just another way of asking how correlated my within-factor measured variables are with each other.\nLet’s calculate Cronbach’s Alpha for each of the subscales I’ve used to define my supposed factors:\n\n\nCode\n### Split the dataset into the subscales assumed by my factor model\nsubscales <- list(\n  map = dat_ags %>% select(ags1, ags5, ags7),\n  mav = dat_ags %>% select(ags2, ags6, ags12),\n  pap = dat_ags %>% select(ags3, ags9, ags11),\n  pav = dat_ags %>% select(ags4, ags8, ags10)\n)\n\n### Calculate Chronbach's Alpha for each subscale, then analyze.\nalphas <- subscales %>% \n  \n  map(psych::alpha) %>% \n  \n  map(summary) %>% \n  \n  knitr::kable() \n\n\n\nReliability analysis   \n raw_alpha std.alpha G6(smc) average_r S/N   ase mean  sd median_r\n      0.82      0.82    0.76       0.6 4.6 0.015  5.9 0.9      0.6\n\nReliability analysis   \n raw_alpha std.alpha G6(smc) average_r S/N   ase mean  sd median_r\n      0.77      0.77    0.71      0.52 3.3 0.018  5.3 1.2     0.45\n\nReliability analysis   \n raw_alpha std.alpha G6(smc) average_r S/N    ase mean  sd median_r\n      0.88      0.89    0.84      0.73 7.9 0.0095  5.4 1.3     0.74\n\nReliability analysis   \n raw_alpha std.alpha G6(smc) average_r S/N  ase mean  sd median_r\n      0.87      0.88    0.85       0.7 7.1 0.01  5.6 1.3     0.72\n\n\nAccording to Kline (2011), these all look like good results, so they help me feel good about claiming convergent validity:\n\n“Generally, coefficients around .90 are considered”excellent,” values around .80 as “very good,” and values about .70 as “adequate.””\n\nCronbach’s Alpha has some drawbacks as a measure of ‘reliability’, which seem pretty clear to me. For example, as (Brown?) explains:\n\nWhat if some of the items in my subscale are confounded by something other than our latent factor? That could make things crazy, IE it could overstate or understate the ‘true’ Cronbach Alpha.\nEven if there’s no other confounding at all, my Cronbach Alpha is going to be understated if my variables are all influenced by the factor to different degrees, IE if the loadings are different, IE if the variables are not ‘Tau-Equivalent’. And this will almost always be the case”\n\n“The condition of tau equivalence is frequently not realized in actual data sets, in part because the units of measurement are often arbitrary.”\n\n\nso Kline (2011) says to also calculate the Average Variance Extracted (AVE), which is simply the average of the within-factor squared factor loadings. This is based on the idea that a squared factor loading is the variance explained of the variable by that factor. The convention is that if the AVE > 0.5, then you can feel good about claiming convergent validity. I guess this makes sense – seems like a pretty simple and ad-hoc way of asking whether your loadings are generally on the same page. But obviously if I have lots of observed variables defining the factor then I’m at risk of having a bunch of high loadings and a bunch of low loadings, resulting in a misleadingly moderate average? To me it seems like we might as well just look at the raw loadings themselves – no need to look at an average here.\nBut just for fun, let’s calculate the AVE. Rather than doing it manually, we can use a ready-made function from the semTools package\n\n\nCode\nsemTools::AVE(h1.fit) %>% \n  \n  knitr::kable()\n\n\n\n\n\n\nx\n\n\n\n\nmap\n0.6115914\n\n\nmav\n0.4556936\n\n\npap\n0.7179750\n\n\npav\n0.7422385\n\n\n\n\n\nBased on the rule-of-thumb that we want the AVE to be at least .50, it seems like the ‘mav’ factor is having some trouble. It also had the lowest Cronbach Alpha. So maybe the observed variables I’m using to measure it aren’t actually doing a great job? This hurts convergent validity for that factor.\nLastly, we can also try to measure this unicorn of ‘reliability’ by just directly asking “what proportion of the total variance is explained by the factor model?”. People like to do this by summing all the factor loadings, squaring that sum, and dividing it by itself plus the sum of the residual variances of the variables (IE dividing it by the total empirical variance of the variable). They call this one the Composite Reliability (CR). It is the one that Brown (2006) says to use.\n\n\nCode\nsemTools::compRelSEM(h1.fit) %>% \n  \n  knitr::kable()\n\n\n\n\n\n\nx\n\n\n\n\nmap\n0.8164263\n\n\nmav\n0.6689921\n\n\npap\n0.8803380\n\n\npav\n0.9016062\n\n\n\n\n\nApparently the rule of thumb for this one is the same as for Cronbach’s Alpha. So we can feel good about all of them except for ‘mav’, which has taken a beating via these 3 checks.\n\n\nDiscriminant Validity\nNext let’s look at the estimated correlations between the factors. If my hypothesis H1 is true then we should expect all of the factors to be pretty uncorrelated from each other, but if H2 is true then we should expect MAP and MAV to be super correlated with each other, because H2 thinks there’s no such thing as MAP and MAV – there’s just one big ‘Mastery’ factor:\n\n\nCode\n### Make a nicer version of the correlation matrix of the factors\n  \nh1.summary$pe %>% \n  \n  as_tibble() %>% \n \n  # Keep only the rows with info on factor loadings\n  slice(25:34) %>% \n \n  select(lhs, rhs, std.lv) %>% \n  \n  mutate(\n    std.lv = round(std.lv, 2),\n    across(everything(), as.character)\n  ) %>% \n  \n  pivot_wider(\n    names_from = \"lhs\", \n    values_from = \"std.lv\",\n    values_fill = \" \" \n  ) %>% \n  \n  column_to_rownames(\"rhs\") %>% \n  \n  knitr::kable(caption = \"Correlation matrix of the factors\")\n\n\n\nCorrelation matrix of the factors\n\n\n\nmap\nmav\npap\npav\n\n\n\n\nmap\n1\n\n\n\n\n\nmav\n0.91\n1\n\n\n\n\npap\n0.06\n0.14\n1\n\n\n\npav\n0.07\n0.21\n0.96\n1\n\n\n\n\n\nInteresting – the ‘Mastery’ factors and the ‘Performance’ factors each seem to be very correlated with each other, while being nice and uncorrelated with the two factors that make up the other. This suggests that we have bad discriminant validity between the imagined two types of ‘Mastery’ and two types of ‘Performance’ – the model can’t really tell them apart as separate things. This makes it harder for me to argue that they are in fact separate things. But then again, maybe my hypothesis is that the within-skill factors should be highly correlated. Anyhow, the fact that the ‘Mastery’ and ‘Performance’ factors are all pretty uncorrelated with each other is a good thing for both hypotheses.\nBrown (2006) gives some nice advice about how to assess discriminant validty, and how to deal with it if you have it:\n\n“In applied research, a factor correlation that exceeds .80 or .85 is often used as the criterion to define poor discriminant validity. When two factors are highly overlapping, a common research strategy is to respecify the model by collapsing the dimensions into a single factor and determine whether this modification results in a significant degradation in model fit. If the respecified model provides an acceptable fit to the data, it is usually favored because of its superior parsimony.”\nGorsuch (1983) suggests doing something similar:\n\n\n“[fit the model] with the qualification that the correlations between one or more of the constructs being tested for discriminant validity is one. The difference between chi-squares from [this model vs the model where the correlations are allowed to freely vary] tests whether the constructs have a correlation significantly less than 1.0. If the correlation between the factors for the two constructs is not significantly different from 1.0, the difference chi-square will be insignificant. This means the null hypothesis of no discriminatory validity would be accepted. If the difference chi-square is significant, then the null hypothesis is rejected and the model that assumes discriminatory validity by allowing the correlation to be less than one is the more appropriate one.”\n\nThis has the flavour of a likelihood-ratio test. Let’s do it. First we need to fit the model where the correlation between the Mastery factors and the correlation between the ‘Performance’ factors are both constrained to be 1:\n\n\nCode\n# Define the relationships from my hypothesis\nh1_orthogonal.definition <- \n'map=~ags1+ags5+ags7\nmav=~ags2+ags6+ags12\npap=~ags3+ags9+ags11\npav=~ags4+ags8+ags10\n\nmap ~~ 1*mav\npap ~~ 1*pav\n'\n\n# Fit the model\nh1_orthogonal.fit <- cfa(\n  data  = dat_ags,\n  model = h1_orthogonal.definition\n)\n\n# Compare the goodness-of-fit statistics for the two models\nanova(h1.fit, h1_orthogonal.fit) %>% \n  \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDf\nAIC\nBIC\nChisq\nChisq diff\nRMSEA\nDf diff\nPr(>Chisq)\n\n\n\n\nh1.fit\n48\n14088.14\n14209.28\n328.3120\nNA\nNA\nNA\nNA\n\n\nh1_orthogonal.fit\n50\n14096.44\n14209.50\n340.6065\n12.29456\n0.1108363\n2\n0.0021393\n\n\n\n\n\nLooks like the reduction in chi-squared goodness-of-fit is statistically significant when we force the within-skill factors to be perfectly correlated. So, according to the Gorsuch (1983) quote above, we can reject the null hypothesis that the within-skill factors are perfectly correlated. This gives a justification for continuing to distinguish between them as separate factors, and helps me make a believable claim that my posited factors are in fact different things.\nActually, I think another way we could have done this would be to just fit the model where we just define one big factor for ‘Mastery’ and one big factor for ‘Performance’. I tried this and it returned even worse fit, which means the extra parameters (the correlation parameters) are significantly improving fit in the pure h1 model.\n\n\nConclusion\nAll-in-all it seems like neither of these hypotheses do a great job. Sure, the ‘Performance’ factors have good convergent validity, and we see good discriminant validity between the ‘Performance’ and ‘Mastery’ factors, but the ‘Mastery’ factors don’t have great convergent validity and fitting a single monolithic ‘Mastery’ factor doesn’t improve things.\nI can make a better model by dropping the measured ‘Mastery’ variables that aren’t having lots of their variance explained by the ‘Mastery’ factors, but this is contrary to the spirit of CFA. If I want to test a different hypothesis then I should collect a different sample.\nFor a nice template of a more formal presentation of the results of a CFA, see Brown (2006) chapter 4 appendix 3.\n\n\n\n\nBrown, Timothy A. 2006. Confirmatory Factor Analysis for Applied Research.\n\n\nFinch, French, W. Holmes. 2015. Latent Variable Modeling with r.\n\n\nGorsuch, Richard L. 1983. Factor Analysis, 2nd Edition.\n\n\nKline, Rex B. 2011. Principles and Practice of Structural Equation Modeling."
  },
  {
    "objectID": "modification-indexes.html",
    "href": "modification-indexes.html",
    "title": "2  Modification Indexes",
    "section": "",
    "text": "In this chapter we’ll work through another example of the Traditional CFA Workflow to get more practice. We’ll also introduce the concept of ‘Modification Indexes’, which researchers often use to improve their model goodness of fit in a way that seems a bit suss to me. Probably a good thing to know about."
  },
  {
    "objectID": "modification-indexes.html#example-2-biodiversity",
    "href": "modification-indexes.html#example-2-biodiversity",
    "title": "2  Modification Indexes",
    "section": "Example 2: Biodiversity",
    "text": "Example 2: Biodiversity\nHere’s a fun example from the Wetland and Aquatic Research Center of the U.S. Geological Survey: given counts of different types of animals, can we fit a convincing CFA model for ‘diversity’? In other words: is the correlation structure of all my counts of various types of animals consistent with the possibility that those counts are confounded by a single unobserved thing called ‘diversity’?\n\n\nCode\ndat_raw <- read.csv('../data/grace/SEM_09_2-Ex1_CFA_exercise_data.csv')\n\ndat_clean <- dat_raw %>%  \n  \n  janitor::clean_names()\n\n\n\nData Exploration\nJust for fun let’s see if the relative proportions of the different animals varies between countries:\n\n\nCode\n### Proportions\ndat_clean %>% \n  \n  pivot_longer(\n    cols      = !matches(\"^c\"),\n    names_to  = \"animal\",\n    values_to = \"count\"\n  ) %>% \n  \n  group_by(country) %>% \n  mutate(\n    total = sum(count), \n    prop  = round(count / total, 2)\n  ) %>% \n  ungroup() %>% \n\n  ggplot() + \n  geom_bar(aes(x = animal, y = prop), stat = \"identity\") + \n  theme_bw() + \n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n  facet_wrap(~country)\n\n\n\n\n\nThe proportions are pretty stable. Finland seems like the weirdest one, and it isn’t even that weird.\n\n\nModel Fitting\nThe hypothesis we want to test here is simply that all of these counts are confounded by a single unmeasured ‘biodiversity’ variable. This is straightforward to fit:\n\n\nCode\nh1.definition <- \n'diversity =~ mammals + birds + amphibians + reptiles + beetles + butterflies'\n\nh1.fit <- cfa(\n  data  = dat_clean %>% select(-country) %>% scale(),\n  model = h1.definition\n)\n\nh1.summary <- summary(h1.fit)\n\nh1.summary\n\n\nlavaan 0.6.16 ended normally after 23 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        12\n\n  Number of observations                            20\n\nModel Test User Model:\n                                                      \n  Test statistic                                20.817\n  Degrees of freedom                                 9\n  P-value (Chi-square)                           0.013\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  diversity =~                                        \n    mammals           1.000                           \n    birds             0.825    0.277    2.978    0.003\n    amphibians        1.115    0.260    4.281    0.000\n    reptiles          0.780    0.279    2.793    0.005\n    beetles           1.135    0.259    4.380    0.000\n    butterflies       1.261    0.254    4.960    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .mammals           0.387    0.131    2.958    0.003\n   .birds             0.566    0.184    3.073    0.002\n   .amphibians        0.250    0.092    2.727    0.006\n   .reptiles          0.608    0.197    3.089    0.002\n   .beetles           0.224    0.085    2.645    0.008\n   .butterflies       0.054    0.054    1.010    0.313\n    diversity         0.563    0.278    2.025    0.043\n\n\nLet’s have a look at the same 4 goodness-of-fit measures we used in the previous example. We can bring them all together with a nice utility function:\n\n\nCode\n### Define a custom function\nfit_measures <- function(fit){\n  \n  summary <- summary(fit, fit.measures = TRUE, standardized = TRUE)\n  \n  res <- list(\n    \n    # Chi-Squared\n    chi_squared = tibble(\n      Test             = \"standard chi-squared\",\n      `DF`             = summary$test$standard$df,\n      `Test Statistic` = round(summary$test$standard$stat, 2),\n      `p-value`        = summary$test$standard$pvalue) %>% \n      \n      mutate(across(everything(), as.character)) %>% \n      \n      pivot_longer(everything()),\n    \n    # RMSEA\n    rmsea = summary$fit %>% \n      \n      as_tibble(rownames = \"stat\") %>% \n      \n      filter(str_detect(stat, \"rmsea\")),\n    \n    # CFI and TLI\n    cfi_tli = summary$fit %>% \n      \n      as_tibble(rownames = \"stat\") %>% \n      \n      filter(str_detect(stat, \"cfi|tli\")) \n    \n  )\n  \n  res\n  \n}\n\n### Call the function, then send its outputs to clean tables\nfit_measures(h1.fit) %>% \n  \n  map(knitr::kable)\n\n\n$chi_squared\n\n\n|name           |value                |\n|:--------------|:--------------------|\n|Test           |standard chi-squared |\n|DF             |9                    |\n|Test Statistic |20.82                |\n|p-value        |0.0134888288206897   |\n\n$rmsea\n\n\n|stat                  |      value|\n|:---------------------|----------:|\n|rmsea                 | 0.25622117|\n|rmsea.ci.lower        | 0.11034617|\n|rmsea.ci.upper        | 0.40224444|\n|rmsea.ci.level        | 0.90000000|\n|rmsea.pvalue          | 0.01890337|\n|rmsea.close.h0        | 0.05000000|\n|rmsea.notclose.pvalue | 0.97055608|\n|rmsea.notclose.h0     | 0.08000000|\n\n$cfi_tli\n\n\n|stat |     value|\n|:----|---------:|\n|cfi  | 0.8695573|\n|tli  | 0.7825955|\n\n\nThe model isn’t fitting very well – Chi-Squared is highly statistically significant (we fail to reject the null hypothesis that there is residual variance left to explain), RMSEA is well above its conventional threshold, and CFI and TLI are both well below their conventional thresholds.\n\n\nModification Indexes\nHere (Grace?) introduces a new method for tweaking our CFA model to improve goodness of fit. The idea is that we can use fancy math to ask “if I took a certain fixed parameter from my model definition and allowed it to be freely estimated, how much would my model’s chi-squared goodness of fit change?” People like to take this estimated change in goodness-of-fit and call it a modification index. As Brown (2006) puts it:\n\n“The modification index reflects an approximation of how much the overall model \\(χ^2\\) would decrease if the fixed or constrained parameter was freely estimated.”\n\nApparently conventional cutoff for a ‘good’ modification index is 3.84. So to get some ideas on how we might improve our goodness-of-fit, let’s print out the modification indexes for each of the fixed parameters in the model and see which of them pass that threshold:\n\n\nCode\n# Get the estimated change in chi-squared for each fixed parameter\nmodindices(h1.fit) %>% \n  \n  # Arrange them in order of modification index\n  arrange(desc(mi)) %>% \n  \n  select(lhs, op, rhs, mi) %>% \n  \n  knitr::kable(digits = 2)\n\n\n\n\n\nlhs\nop\nrhs\nmi\n\n\n\n\nbirds\n~~\nbeetles\n4.44\n\n\nbirds\n~~\namphibians\n3.99\n\n\nmammals\n~~\nbutterflies\n2.84\n\n\nbeetles\n~~\nbutterflies\n2.78\n\n\nmammals\n~~\namphibians\n2.31\n\n\nbirds\n~~\nreptiles\n2.05\n\n\namphibians\n~~\nbutterflies\n1.72\n\n\nbirds\n~~\nbutterflies\n1.55\n\n\nmammals\n~~\nreptiles\n1.29\n\n\nmammals\n~~\nbirds\n1.20\n\n\namphibians\n~~\nbeetles\n0.58\n\n\nmammals\n~~\nbeetles\n0.38\n\n\nreptiles\n~~\nbutterflies\n0.22\n\n\nreptiles\n~~\nbeetles\n0.15\n\n\namphibians\n~~\nreptiles\n0.14\n\n\n\n\n\nBased on the operation symbol “~~”, it seems like all of the modification indexes correspond to residual correlations between observed variables. This teaches me something about CFA models! I guess in the typical CFA model we fix the residual correlations to 0? This helps me understand why the Bayesian CFA model as implemented in brms specifies rescor = FALSE . I was confused about this!\nActually, I just realized Gorsuch (1983) already explained this to me! Think back to where he showed us the definition of the ‘Common Factor Model’:\n\\(R_{vv} = PR_{ff}P' + U_{vv}\\)\nAnd remember how Gorsuch specified that \\(U_{vv}\\) is assumed to be a diagonal matrix, IE the residual correlations is assumed to be uncorrelated for each variable. This is the whole thing about the ‘unique factors’, IE the error terms, of the linear models of each measured variable are gonna be uncorrelated. This recorded seminar and notes from UCLA give a nice clear walkthrough of the notation in a slightly different form from Gorsuch (1983).\nFrom the DAGs perspective of CFA, assuming uncorrelated residuals sort of makes sense to me: if I want to convince you that my measured variables are all confounded by the same single unmeasured variable, then I think fixing the residual errors at 0 is a way of committing my model to the idea that there aren’t other unmeasured variables confounding certain of my measured guys. It is a strong assumption that, if it holds up, provides better evidence that my variables really truly are just confounded by a single unmeasured thing.\nSo I guess I could write out this standard CFA model in a more McElreath fashion like so:\n[ \\[\\begin{align*}\n\\begin{bmatrix} \\text{mammals}_i \\\\ \\text{birds}_i \\\\ \\text{amphibians}_i \\\\ \\text{reptiles}_i \\\\ \\text{beetles}_i \\\\ \\end{bmatrix} & \\sim\n\\operatorname{MVNormal} \\left( \\begin{bmatrix} \\mu_{mammals} \\\\ \\mu_{birds} \\\\ \\mu_{amphibians} \\\\ \\mu_{reptiles} \\\\ \\mu_{beetles} \\end{bmatrix}, \\mathbf{\\Sigma} \\right)\\\\\n\\mu_{mammals} & = \\lambda_{mammals} F_i \\\\\n\\mu_{birds} & = \\lambda_{birds} F_i \\\\\n\\mu_{amphibians} & = \\lambda_{amphibians} F_i \\\\\n\\mu_{reptiles} & = \\lambda_{reptiles} F_i \\\\\n\\mu_{beetles} & = \\lambda_{beetles} F_i \\\\\n\\Sigma & = \\begin{pmatrix}\n\\sigma_{mammals}&0 &0 &0 &0 \\\\\n0 & \\sigma_{birds} &0 &0 &0 \\\\\n0 & 0 & \\sigma_{amphibians} &0 &0 \\\\\n0 & 0 & 0 & \\sigma_{reptiles} &0 \\\\\n0 & 0 & 0 & 0 & \\sigma_{beetles}\n\\end{pmatrix} \\\\\n\\end{align*}\\] ]\nIn human words: the observed counts of each of the 5 animal types are imagined to be drawn from a shared multivariate normal distribution. The mean of each dimension of that distribution is a linear function of a single shared factor, which we’re calling ‘biodiversity’. The variance of each dimension of that distribution is unique, and there is no covariance between the dimensions.\nBut now think back to our modification indexes: a few of them are saying that if we allow the residual covariances to be freely estimated rather than fixed at 0, then we can improve model fit by a whole lot. Specifically, if we allow the residual covariance between birds and beetles and/or between birds and amphibians to be freely estimated, then model fit as measured by the chi-squared statistic might be significantly improved. Here’s what the model is gonna look like now:\n[ \\[\\begin{align*}\n\\begin{bmatrix} \\text{mammals}_i \\\\ \\text{birds}_i \\\\ \\text{amphibians}_i \\\\ \\text{reptiles}_i \\\\ \\text{beetles}_i \\\\ \\end{bmatrix} & \\sim\n\\operatorname{MVNormal} \\left( \\begin{bmatrix} \\mu_{mammals} \\\\ \\mu_{birds} \\\\ \\mu_{amphibians} \\\\ \\mu_{reptiles} \\\\ \\mu_{beetles} \\end{bmatrix}, \\mathbf{\\Sigma} \\right)\\\\\n\\mu_{mammals} & = \\lambda_{mammals} F_i \\\\\n\\mu_{birds} & = \\lambda_{birds} F_i \\\\\n\\mu_{amphibians} & = \\lambda_{amphibians} F_i \\\\\n\\mu_{reptiles} & = \\lambda_{reptiles} F_i \\\\\n\\mu_{beetles} & = \\lambda_{beetles} F_i \\\\\n\\Sigma & = \\begin{pmatrix}\n\\sigma_{mammals}&0 &0 &0 &0 \\\\\n0 & \\sigma_{birds} &\\theta_\\text{b\\&a} &0 &\\theta_\\text{b\\&b} \\\\\n0 &\\theta_\\text{b\\&a} & \\sigma_{amphibians} &0 &0 \\\\\n0 & 0 & 0 & \\sigma_{reptiles} &0 \\\\\n0 &\\theta_\\text{b\\&b} & 0 & 0 & \\sigma_{beetles}\n\\end{pmatrix} \\\\\n\\end{align*}\\] ]\nSee how I’ve filled in the variance-covariance matrix of the likelihood to include a few more free parameters?\nActually, Grace proceeds by fitting two more models, one with each of these two candidate covariance parameters as freely fitting. Then he uses anova() to do a likelihood-ratio test for them. We can’t test all 3 models at once because models 2 and 3 aren’t nested with each other.\n\n\nCode\n### Letting the covariance between birds and beetles be freely estimated\nh2.definition <- \n'diversity =~ mammals + birds + amphibians + \n              reptiles + beetles + butterflies\n \n birds ~~ beetles'\n\n\nh2.fit <- cfa(\n  data  = dat_clean %>% select(-country) %>% scale(),\n  model = h2.definition\n)\n\n### Letting the covariance between birds and amphibians be freely estimated\nh3.definition <- \n  'diversity =~ mammals + birds + amphibians + \n              reptiles + beetles + butterflies\n \n birds ~~ amphibians'\n\n\nh3.fit <- cfa(\n data  = dat_clean %>% select(-country) %>% scale(),\n  model = h3.definition\n)\n\nanova(h1.fit, h2.fit)\n\n\n\nChi-Squared Difference Test\n\n       Df    AIC    BIC  Chisq Chisq diff   RMSEA Df diff Pr(>Chisq)  \nh2.fit  8 270.81 283.76 16.013                                        \nh1.fit  9 273.62 285.56 20.817      4.804 0.43612       1    0.02839 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\nanova(h1.fit, h3.fit)\n\n\n\nChi-Squared Difference Test\n\n       Df    AIC    BIC  Chisq Chisq diff   RMSEA Df diff Pr(>Chisq)   \nh3.fit  8 267.72 280.67 12.924                                         \nh1.fit  9 273.62 285.56 20.817     7.8934 0.58709       1   0.004961 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLooks like model H3 has the lowest AIC and the more significant improvement in chi-squared fit. So let’s continue working with that one in the following sections.\n\n\nValidity\nWe can do the same 5 checks of validity we used in the previous ‘Mastery and Performance’ example. Let’s start with the big summary printout:\n\n\nCode\nsummary.h3 <- summary(h3.fit, fit.measures = TRUE, standardized = TRUE)\n\nsummary.h3\n\n\nlavaan 0.6.16 ended normally after 25 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        13\n\n  Number of observations                            20\n\nModel Test User Model:\n                                                      \n  Test statistic                                12.923\n  Degrees of freedom                                 8\n  P-value (Chi-square)                           0.115\n\nModel Test Baseline Model:\n\n  Test statistic                               105.591\n  Degrees of freedom                                15\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.946\n  Tucker-Lewis Index (TLI)                       0.898\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)               -120.861\n  Loglikelihood unrestricted model (H1)       -114.400\n                                                      \n  Akaike (AIC)                                 267.723\n  Bayesian (BIC)                               280.667\n  Sample-size adjusted Bayesian (SABIC)        240.592\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.175\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.344\n  P-value H_0: RMSEA <= 0.050                    0.138\n  P-value H_0: RMSEA >= 0.080                    0.824\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.055\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  diversity =~                                                          \n    mammals           1.000                               0.706    0.725\n    birds             1.013    0.310    3.266    0.001    0.716    0.734\n    amphibians        1.209    0.306    3.956    0.000    0.854    0.876\n    reptiles          0.899    0.307    2.928    0.003    0.635    0.652\n    beetles           1.264    0.301    4.196    0.000    0.893    0.916\n    butterflies       1.261    0.301    4.187    0.000    0.891    0.914\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n .birds ~~                                                              \n   .amphibians       -0.245    0.094   -2.615    0.009   -0.245   -0.789\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .mammals           0.451    0.148    3.048    0.002    0.451    0.475\n   .birds             0.438    0.152    2.877    0.004    0.438    0.461\n   .amphibians        0.221    0.090    2.451    0.014    0.221    0.232\n   .reptiles          0.546    0.177    3.087    0.002    0.546    0.575\n   .beetles           0.153    0.061    2.506    0.012    0.153    0.161\n   .butterflies       0.156    0.062    2.526    0.012    0.156    0.165\n    diversity         0.499    0.267    1.866    0.062    1.000    1.000\n\n\nThe factor loadings are all highly statistically significant, which is the first thing to check to make sure nothing is going horribly wrong.\nThe standardized loadings are pretty big as well, but not super great for ‘reptiles’. Also there’s a lot of variance in the loadings, which is evidence that my simple DAG of confounding may not be perfect – there are other unmeasured variables influencing some of my animal counts to different degrees. I mean of course there are, but the degree to which this is apparent based on the factor loadings undermines my claims to convergent validity.\nNext we can look at the standardized residual variances. Some of them look great, and all but ‘reptiles’ pass the threshold of 0.5.\nI could look at the ‘reliability’ statistics too, but can’t be bothered right now. Onwards to another example!\n\n\n\n\nBrown, Timothy A. 2006. Confirmatory Factor Analysis for Applied Research.\n\n\nGorsuch, Richard L. 1983. Factor Analysis, 2nd Edition."
  },
  {
    "objectID": "mtmm-and-error-structure-modelling.html",
    "href": "mtmm-and-error-structure-modelling.html",
    "title": "3  MTMM and Error Structure Modelling",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(ggdag)\n\n\nIn the previous example we saw how we can sometimes improve model fit by freeing-up some of the residual covariance terms, rather than doing the typical thing of fixing them at 0. But this feels a bit icky to me – just pumping out some modification indexes and using that as a basis for opening up some free parameters feels pretty overfitty, because we don’t have a strong theory-driven reason for changing the model in that way.\nBut there are more kosher-feeling theory-driven reasons for freeing up some of the residual covariance parameters. Let’s talk about two of them: the first relates to convergent validity, the second relates to discriminant validity.\nHere’s the first example: imagine I have a theory where there’s a thing called ‘exceptional leadership’, and it is made up of 3 unobservable features, like ‘self-confidence’, ‘oratorical skill’, and ‘robust compassionateness’. So I make up a survey where I ask 12 questions total, 4 per imagined factor. Then I fit a CFA model and find that it does a great job recreating the empirical variance-covariance matrix. There’s lots of great convergent validity between the questions I imagine to define the 3 factors. So I get published! But there’s a first problem: what if my within-factor variables are correlated not because they are cleanly confounded by ‘self-confidence’ (which is what I’m trying to convince you of), but instead because the within-factor survey questions are just worded in a really similar way, IE they are confounded by a latent factor we might call ‘wording similarity’? This possibility undermines my case for clean confounding.\nNow the second example: imagine I do the same analysis described above, but I find my discriminant validity actually doesn’t look so hot, IE there are some high between-factor correlations. It is possible that this is just being caused by some of the variables used in different factors being confounded by their shared measurement approach, which creates a backdoor path between the factors.\nAs Brown (2006) puts it:\n\n“when each construct is assessed by the same measurement approach (e.g., observer rating), it cannot be determined how much of the observed overlap (i.e., factor correlations) is due to method effects as opposed to”true” covariance of the traits.”\n\nSo we have these two risks:\n\nMaybe some of my within-factor variables are confounded by method effects, which creates the illusion of convergent validity. If I go to publish my paper and someone raises this concern, then maybe I won’t get published! I’ll need to find a way to make my model control for possible method-confounding and still show good convergent validity.\nMaybe some of my variables of different factors are confounded by method effects, so I don’t end up with great discriminant validity. This would be bad, but fitting a model that controls for method effects can maybe make things better.\n\nFear not: there are two ways of adjusting the model to control for measurement confounding, thereby addressing the above risks.\n\nAdd method-specific factors to my model (to control for them in the linear model of each variable). Brown (2006) calls this a Correlated Methods Model;\nJust freely fit the residual covariances between the observed variables that share a method. Brown (2006) calls this a Correlated Uniqueness Model. Because remember, ‘Uniqueness’ is just a fancy term for variable-specific residual variance.\n\nIt’s all still just basic linear modelling, and trying to show that the model’s results are consistent with the DAG of clean confounding. By adding a method factors or allowing some of the error residuals to be freely fit, I’m controlling for sources of confounding that a reviewer might bring up as a concern, or that might be pulling down my discriminant validity.\nHere’s how these approaches can improve convergent or divergent validity:\nConvergent validity: By adding method-factors to the model or freely fitting the residual covariances between the within-factor questions can help me make the case that “see, even when I allow for correlated errors due to other unobserved confounders (like common wording or common methods), the factors still do a good job recreating the empirical covariance structure, IE the loadings still look good, so my argument for mostly clean confounding is still reasonable.” I think this makes sense?\nDivergent validity: Maybe I can get better discriminant validity, IE reduce the between-factor correlations, by adding those method effects to the linear models, thereby controlling for them. I can do this either by literally adding in some new factors to represent each method, or just by allowing the residual covariances of like-method variables to be freely estimated.\n\nSimulating Data Based on a DAG\nNow let’s look at an example in detail. This example is taken from Brown (2006), chapter 6.\nSome researchers were curious about whether ‘happiness’ and ‘sadness’ are totally separate things vs two sides of a single shared spectrum. I guess the implication is that if they are totally separate things then I could be happy and sad at the same time, whereas if they’re two sides of a spectrum then I can only ever be one or the other.\nThis feels like a good factor analysis question! I can collect a bunch of data that I think map to ‘happy’ and a bunch of other data that I think map to ‘sad’, fit a CFA, and see whether the two factors have discriminant validity.\nThis is exactly what al (n.d.) did. They collected a few columns each for ‘happy’ and ‘sad’, fit a factor model, and fit a CFA. Each within-factor column had its own measurement approach, but shared a measurement approach with one of the columns of the other factor. So we are at risk of our estimate of between-factor correlations being confounding due to shared measurement approach, which could be hurting my case for discriminant validity!\nHere’s how we can show this situation in a DAG:\n\n\nCode\n# Set DAG coordinates\ndag_coords <- list(\n  x = c(\n    F1 = 1, \n    F2 = 1,\n    H1 = 2,\n    H2 = 2,\n    H3 = 2,\n    S1 = 2,\n    S2 = 2,\n    S3 = 2,\n    M1 = 3,\n    M2 = 3,\n    M3 = 3),\n  y = c(\n    F1 = 2.5,\n    F2 = 1.5,\n    H1 = 2.8,\n    H2 = 2.5,\n    H3 = 2.2,\n    S1 = 1.8,\n    S2 = 1.5,\n    S3 = 1.2,\n    M1 = 2.6,\n    M2 = 2,\n    M3 = 1.4\n  )\n)\n\n# Set DAG relationships and aesthetics\nmeasurement_confounding_dag <- ggdag::dagify(\n  H1 ~ F1,\n  H2 ~ F1,\n  H3 ~ F1,\n  S1 ~ F2,\n  S2 ~ F2,\n  S3 ~ F2,\n  H1 ~ M1,\n  S1 ~ M1,\n  H2 ~ M2,\n  S2 ~ M2,\n  H3 ~ M3,\n  S3 ~ M3,\n  coords = dag_coords\n) %>% \n  \n  tidy_dagitty() %>% \n  \n  mutate(\n    \n    node_colour = case_when(\n      grepl(\"^F|M\", name) ~ \"latent\",\n      grepl(\"^H|S\", name) ~ \"observed\"\n    ),\n    \n    edge_colour = case_when(\n      grepl(\"^M\", name) & grepl(\"1$\", to) ~ \"cornflower blue\",\n      grepl(\"^M\", name) & grepl(\"2$\", to) ~ \"#daed64\",\n      grepl(\"^M\", name) & grepl(\"3$\", to) ~ \"#ed7864\",\n      grepl(\"^F\", name)                   ~ \"black\"\n    )\n  )\n\n# Plot the DAG\nmeasurement_confounding_dag %>%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(colour = node_colour)) +\n  scale_colour_manual(values = c(\"dark blue\", \"#edbc64\")) + \n  geom_dag_edges(aes(edge_colour = edge_colour)) +\n  geom_dag_text() +\n  theme_void()\n\n\n\n\n\nSee how the measurment effects M1, M2, and M3 each create a backdoor path between the two factors F1 and F2. So if I want to get better-seeming (and, under the DAG, more accurate) estimate of between-factor correlation, then I need to find a way to close those backdoor paths. The classic way to close these paths would be to condition on the measurement effects by adding them to the linear model, but I can’t directly do this because they are unmeasured. But, as discussed above, I can still sort of do it by adding them as factors to my CFA model, or by freely estimating residual correlation between the observed variables that share a measurement approach, which should work if my DAG is mostly accurate.\nUnfortunately, the authors of this paper haven’t published their data. But we can take this as an opportunity to practice simulating a dataset with relationships implied by a DAG.\n\n\nCode\n### Simulate Data from the DAG\n\n# Set seed for replicable results\nset.seed(233)\n\n# Set sample size\nN <- 305\n\n# Create the dataset\ndat_fake <- tibble(\n  \n  # The factors are uncorrelated in reality, but\n  # will be confounded by the measurement effects!\n  F1 = rnorm(N, 0, 1),\n  F2 = rnorm(N, 0, 1),\n  \n  # The measurement effects\n  M1 = rnorm(N, 0, 1),\n  M2 = rnorm(N, 0, 1),\n  M3 = rnorm(N, 0, 1),\n  \n  # The DAG says the measurements are fully determined by the latent factors and measurement effects\n  H1 = .8*F1 + 0.7*M1 + rnorm(N, 0, .3),\n  H2 = .7*F1 + 0.7*M2 + rnorm(N, 0, .3),\n  H3 = .9*F1 + 0.7*M3 + rnorm(N, 0, .3),\n  S1 = .8*F2 + 0.7*M1 + rnorm(N, 0, .3),\n  S2 = .7*F2 + 0.7*M2 + rnorm(N, 0, .3),\n  S3 = .9*F2 + 0.7*M3 + rnorm(N, 0, .3) \n) \n\n\nFun! Now we have our fake data to play with. For starters, since we actually do have the values of the latent variables in our dataset, we can demonstrate how directly controlling for the measurement effects in a regression model can close the backdoor path between the factors.\n\n\nCode\nlist(\n  lm(H1 ~ S1, dat_fake), \n  lm(H1 ~ S1 + M1, dat_fake)\n) %>% \n  \n  map(broom::tidy) %>% \n  \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.0249271\n0.0572795\n0.435184\n0.6637387\n\n\nS1\n0.4555127\n0.0491323\n9.271137\n0.0000000\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.0466841\n0.0456881\n1.0217995\n0.3076936\n\n\nS1\n-0.0137610\n0.0528505\n-0.2603763\n0.7947509\n\n\nM1\n0.7636731\n0.0577500\n13.2237680\n0.0000000\n\n\n\n\n\n\n\n\n\n\nWhen we just do the simple regression of H1 on S1 we get a big effect with a highly statistically significant p-value, despite the fact that we know there’s no causal relationship there! But then when we include the confounding measurement effect in the model this effect vanishes in smoke.\nThat’s all well and good. But in reality we won’t have measurements of the latent variables, so we won’t be able to directly control for them. Thankfully, we have Factor Analysis. We can control for the measurement effects by estimating the residual correlation between each pair of variables that share a measurement effect. Since, under the DAG, the measurement effects are the only source of correlation between these variables, this should close the backdoor path, IE we should get unbiased estimates of the factor loadings.\n….@Brown2006 calls this an “error theory”…..\n\n\nCorrelated Uniqueness Model\nTo illustrate, we’ll fit 2 models: The first is a basic CFA model that just loads each measured variable on its corresponding factor. The second specifies that the residual correlation between the measurement-confounded variables should be freely estimated, IE not fixed at 0.\nFirst let’s define our utility function like we did in the previous chapter:\n\n\nCode\n### Define a custom function\nfit_measures <- function(fit){\n  \n  summary <- summary(fit, fit.measures = TRUE, standardized = TRUE)\n  \n  res <- list(\n    \n    # Chi-Squared\n    chi_squared = tibble(\n      Test             = \"standard chi-squared\",\n      `DF`             = summary$test$standard$df,\n      `Test Statistic` = round(summary$test$standard$stat, 2),\n      `p-value`        = summary$test$standard$pvalue) %>% \n      \n      mutate(across(everything(), as.character)) %>% \n      \n      pivot_longer(everything()),\n    \n    # RMSEA\n    rmsea = summary$fit %>% \n      \n      as_tibble(rownames = \"stat\") %>% \n      \n      filter(str_detect(stat, \"rmsea\")),\n    \n    # CFI and TLI\n    cfi_tli = summary$fit %>% \n      \n      as_tibble(rownames = \"stat\") %>% \n      \n      filter(str_detect(stat, \"cfi|tli\")) \n    \n  )\n  \n  res\n  \n}\n\n\n\n\nCode\nbasic.definition <- \n  'happy =~ H1 + H2 + H3\n   sad =~ S1 + S2 + S3\n   '\n\ncorrelated_uniqueness.definition <- \n  'happy =~ H1 + H2 + H3\n   sad =~ S1 + S2 + S3\n   \n   H1 ~~ S1\n   H2 ~~ S2\n   H3 ~~ S3\n   '\nbasic.fit <- cfa(\n  data = dat_fake %>% select(matches(\"^(H|S)\")),\n  model = basic.definition\n)\n\ncorrelated_uniqueness.fit <- cfa(\n  data = dat_fake %>% select(matches(\"^(H|S)\")),\n  model = correlated_uniqueness.definition\n)\n\nsummary.basic.fit <- summary(basic.fit, standardized = TRUE)\nsummary.correlated_uniqueness.fit <- summary(correlated_uniqueness.fit, standardized = TRUE)\n\nsummary.basic.fit\n\n\nlavaan 0.6.16 ended normally after 23 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        13\n\n  Number of observations                           305\n\nModel Test User Model:\n                                                      \n  Test statistic                               802.905\n  Degrees of freedom                                 8\n  P-value (Chi-square)                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  happy =~                                                              \n    H1                1.000                               0.816    0.723\n    H2                0.741    0.090    8.236    0.000    0.605    0.615\n    H3                1.044    0.123    8.496    0.000    0.852    0.741\n  sad =~                                                                \n    S1                1.000                               0.906    0.778\n    S2                0.818    0.079   10.302    0.000    0.742    0.704\n    S3                0.980    0.093   10.522    0.000    0.888    0.752\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  happy ~~                                                              \n    sad               0.236    0.060    3.934    0.000    0.319    0.319\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .H1                0.609    0.084    7.214    0.000    0.609    0.478\n   .H2                0.601    0.062    9.662    0.000    0.601    0.621\n   .H3                0.597    0.089    6.721    0.000    0.597    0.451\n   .S1                0.537    0.077    6.989    0.000    0.537    0.395\n   .S2                0.560    0.062    8.964    0.000    0.560    0.505\n   .S3                0.604    0.078    7.726    0.000    0.604    0.434\n    happy             0.667    0.114    5.860    0.000    1.000    1.000\n    sad               0.822    0.119    6.886    0.000    1.000    1.000\n\n\nCode\nsummary.correlated_uniqueness.fit\n\n\nlavaan 0.6.16 ended normally after 47 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        16\n\n  Number of observations                           305\n\nModel Test User Model:\n                                                      \n  Test statistic                                13.448\n  Degrees of freedom                                 5\n  P-value (Chi-square)                           0.020\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  happy =~                                                              \n    H1                1.000                               0.738    0.669\n    H2                0.915    0.055   16.787    0.000    0.676    0.657\n    H3                1.157    0.066   17.625    0.000    0.854    0.755\n  sad =~                                                                \n    S1                1.000                               0.866    0.744\n    S2                0.863    0.042   20.433    0.000    0.747    0.724\n    S3                1.057    0.048   21.972    0.000    0.915    0.761\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n .H1 ~~                                                                 \n   .S1                0.551    0.061    9.007    0.000    0.551    0.866\n .H2 ~~                                                                 \n   .S2                0.458    0.051    8.973    0.000    0.458    0.831\n .H3 ~~                                                                 \n   .S3                0.501    0.062    8.133    0.000    0.501    0.868\n  happy ~~                                                              \n    sad               0.032    0.050    0.638    0.524    0.050    0.050\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .H1                0.672    0.070    9.641    0.000    0.672    0.552\n   .H2                0.600    0.061    9.813    0.000    0.600    0.568\n   .H3                0.550    0.071    7.769    0.000    0.550    0.430\n   .S1                0.604    0.066    9.121    0.000    0.604    0.446\n   .S2                0.507    0.053    9.538    0.000    0.507    0.476\n   .S3                0.607    0.069    8.789    0.000    0.607    0.420\n    happy             0.545    0.073    7.450    0.000    1.000    1.000\n    sad               0.749    0.086    8.762    0.000    1.000    1.000\n\n\nCode\nfit_measures(basic.fit) %>% \n  \n  knitr::kable(caption = \"Basic Model\")\n\n\n\n\n\nBasic Model\n\n\n\n\n\n\n\nname\nvalue\n\n\n\n\nTest\nstandard chi-squared\n\n\nDF\n8\n\n\nTest Statistic\n802.9\n\n\np-value\n0\n\n\n\n\n\n\n\n\nstat\nvalue\n\n\n\n\nrmsea\n0.5707720\n\n\nrmsea.ci.lower\n0.5377551\n\n\nrmsea.ci.upper\n0.6044999\n\n\nrmsea.ci.level\n0.9000000\n\n\nrmsea.pvalue\n0.0000000\n\n\nrmsea.close.h0\n0.0500000\n\n\nrmsea.notclose.pvalue\n1.0000000\n\n\nrmsea.notclose.h0\n0.0800000\n\n\n\n\n\n\n\n\nstat\nvalue\n\n\n\n\ncfi\n0.3742618\n\n\ntli\n-0.1732591\n\n\n\n\n\n\n\n\n\n\nCode\nfit_measures(correlated_uniqueness.fit) %>% \n  \n  knitr::kable(caption = \"Correlated Uniqueness Model\")\n\n\n\n\n\nCorrelated Uniqueness Model\n\n\n\n\n\n\n\nname\nvalue\n\n\n\n\nTest\nstandard chi-squared\n\n\nDF\n5\n\n\nTest Statistic\n13.45\n\n\np-value\n0.0195200124719597\n\n\n\n\n\n\n\n\nstat\nvalue\n\n\n\n\nrmsea\n0.07443086\n\n\nrmsea.ci.lower\n0.02734066\n\n\nrmsea.ci.upper\n0.12377511\n\n\nrmsea.ci.level\n0.90000000\n\n\nrmsea.pvalue\n0.16609111\n\n\nrmsea.close.h0\n0.05000000\n\n\nrmsea.notclose.pvalue\n0.47823144\n\n\nrmsea.notclose.h0\n0.08000000\n\n\n\n\n\n\n\n\nstat\nvalue\n\n\n\n\ncfi\n0.9933495\n\n\ntli\n0.9800485\n\n\n\n\n\n\n\n\n\n\nHere we see that under the basic model we have some moderate correlation between the happy and sad factors, which is a bit of a murky result: it doesn’t tell us one way or the other whether happiness and sadness are separate constructs I can feel together or two extremes of the same feeling. But under the correlated uniqueness model this correlation evaporates because we’ve controlled for the measurement effects, closing the backdoor path between happy and sad. This model also greatly improves goodness-of-fit, which makes sense because it better reflects the true data-generating process we coded up.\nWe also could have controlled for the measurement effects by including measurement factors, IE by adopting a ‘Correlated Methods Model’. I tried this but I actually I couldn’t get this model to converge, regardless of whether its method factors were correlated or uncorrelated (an ‘Uncorrelated Methods Model’. Brown (2006) actually mentions this as a common issue, and favours the Correlated Uniqueness Model for that reason. In his words:\n\n“an overriding drawback of the correlated methods model is that it is usually empirically underidentified. Consequently, a correlated methods solution will typically fail to converge. If it does converge, the solution will usually be associated with Heywood cases [negative variance estimates] and large standard errors”\n\nNow let’s consider the other case in which measurement effects might be hurting us: the case in which within-factor measurements are confounded by measurement effects. Here’s the DAG:\n\n\nCode\ndag_coords <- list(\n  x = c(\n    F1 = 1, \n    F2 = 1,\n    H1 = 2,\n    H2 = 2,\n    H3 = 2,\n    S1 = 2,\n    S2 = 2,\n    S3 = 2,\n    M1 = 3,\n    M2 = 3),\n  y = c(\n    F1 = 2.5,\n    F2 = 1.5,\n    H1 = 2.8,\n    H2 = 2.5,\n    H3 = 2.2,\n    S1 = 1.8,\n    S2 = 1.5,\n    S3 = 1.2,\n    M1 = 2.5,\n    M2 = 1.5\n  )\n)\n\n# Set DAG relationships and aesthetics\nmeasurement_confounding_dag <- ggdag::dagify(\n  H3 ~ F1,\n  S2 ~ F2,\n  H1 ~ M1,\n  H2 ~ M1,\n  H3 ~ M1,\n  S1 ~ M2,\n  S2 ~ M2,\n  S3 ~ M2,\n  coords = dag_coords\n) %>% \n  \n  tidy_dagitty() %>% \n  \n  mutate(\n    \n    node_colour = case_when(\n      grepl(\"^F|M\", name) ~ \"latent\",\n      grepl(\"^H|S\", name) ~ \"observed\"\n    ),\n    \n    edge_colour = case_when(\n      grepl(\"M1\", name)  ~ \"cornflower blue\",\n      grepl(\"M2\", name) ~ \"#ed7864\",\n      grepl(\"^XX\", name) & grepl(\"3$\", to) ~ \"#ed7864\",\n      grepl(\"^F\", name)                   ~ \"black\"\n    )\n  )\n\n# Plot the DAG\nmeasurement_confounding_dag %>%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(colour = node_colour)) +\n  scale_colour_manual(values = c(\"dark blue\", \"#edbc64\")) + \n  geom_dag_edges(aes(edge_colour = edge_colour)) +\n  geom_dag_text() +\n  theme_void()\n\n\n\n\n\nThis is the ‘true’ data-generating process we’ll be simulating data from in a moment. Notice that even though the researcher (who can’t see this DAG) might think that the unobseved factor causally influences all 3 measured variables, the reality is that each factor only influences one of the measured variables. However, the purported within-factor variables are confounded by measurement method.\nLet’s simulate the data and analyze:\n\n\nCode\n# Set seed for replicable results\nset.seed(233)\n\n# Set sample size\nN <- 30000\n\n# Create the dataset\ndat_fake <- tibble(\n  \n  # Create some uncorrelated factors\n  F1 = rnorm(N, 0, 1),\n  F2 = rnorm(N, 0, 1),\n  \n  # Create some measurement effects\n  M1 = rnorm(N, 0, 1),\n  M2 = rnorm(N, 0, 1),\n  \n  # The DAG says only H3 and S2 are influenced by the factors, but all variables are influenced by a measurement effect.\n  H1 = 0.7*M1 + rnorm(N, 0, .3),\n  H2 = 0.8*M1 + rnorm(N, 0, .3),\n  H3 = 0.9*F1 + 0.8*M1 + rnorm(N, 0, .3),\n  S1 = 0.7*M2 + rnorm(N, 0, .3),\n  S2 = 0.7*F2 + 0.8*M2 + rnorm(N, 0, .3),\n  S3 = 0.7*M2 + rnorm(N, 0, .3) \n) \n\n\nFirst let’s fit a basic naive CFA model that does the standard thing of keeping the covariances between variables fixed at 0. Based on the DAG, we should expect this model to return a strong (publishable) but misleading answer – it will notice the correlation between variables that are considered within-factor under our hypothesis, and say ‘wow so correlated, that’s consistent with them being caused by that factor’. But we know this is wrong: their correlation is simply driven by the shared measurement method:\n\n\nCode\nbasic.definition <- \n  'happy =~ H1 + H2 + H3\n   sad =~ S1 + S2 + S3\n   '\n\nbasic.fit <- cfa(\n  data = dat_fake,\n  model = basic.definition\n)\n\nsummary(basic.fit, standardized = TRUE)\n\n\nlavaan 0.6.16 ended normally after 25 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        13\n\n  Number of observations                         30000\n\nModel Test User Model:\n                                                      \n  Test statistic                                 7.086\n  Degrees of freedom                                 8\n  P-value (Chi-square)                           0.527\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  happy =~                                                              \n    H1                1.000                               0.705    0.920\n    H2                1.141    0.006  196.951    0.000    0.804    0.936\n    H3                1.142    0.009  129.172    0.000    0.805    0.646\n  sad =~                                                                \n    S1                1.000                               0.696    0.917\n    S2                1.142    0.008  151.721    0.000    0.795    0.722\n    S3                1.004    0.005  206.981    0.000    0.699    0.922\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  happy ~~                                                              \n    sad              -0.002    0.003   -0.765    0.444   -0.005   -0.005\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .H1                0.090    0.002   42.749    0.000    0.090    0.154\n   .H2                0.091    0.003   34.103    0.000    0.091    0.124\n   .H3                0.903    0.008  115.595    0.000    0.903    0.582\n   .S1                0.091    0.002   49.153    0.000    0.091    0.158\n   .S2                0.581    0.005  110.971    0.000    0.581    0.479\n   .S3                0.086    0.002   46.614    0.000    0.086    0.150\n    happy             0.497    0.005   96.783    0.000    1.000    1.000\n    sad               0.484    0.005   98.040    0.000    1.000    1.000\n\n\nAnd there you have it – just as foretold, we have super strong factor loadings for all the variables, even those that are not actually causally influenced by the factor! So it may look like I have strong convergent validity, but hopefully if we try to publish this, a reviewer will raise the possibility that these correlations are confounded by measurement effects.\nNow I’m going to try closing the backdoor paths between the non-factor-caused variables by allowing the model to learn the covariances, thereby hopefully controlling for unobserved sources of confounding (like the measurement effect). If the loadings stay strong, then my claims to convergent validity are more reasonable.\n\n\nCode\ncorrelated_uniqueness.definition <- \n  'happy =~ H1 + H2 + H3\n   sad   =~ S1 + S2 + S3\n   \n   H1 ~~ H2\n   H1 ~~ H3\n   H2 ~~ H3\n   S1 ~~ S2\n   S1 ~~ S3\n   S2 ~~ S3\n   '\n\ncorrelated_uniqueness.fit <- cfa(\n  data = dat_fake,\n  model = correlated_uniqueness.definition\n)\n\n\nWarning in lav_model_vcov(lavmodel = lavmodel, lavsamplestats = lavsamplestats, : lavaan WARNING:\n    Could not compute standard errors! The information matrix could\n    not be inverted. This may be a symptom that the model is not\n    identified.\n\n\nWarning in lav_object_post_check(object): lavaan WARNING: some estimated ov\nvariances are negative\n\n\nCode\nsummary(correlated_uniqueness.fit, standardized = TRUE)\n\n\nlavaan 0.6.16 ended normally after 593 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        19\n\n  Number of observations                         30000\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.453\n  Degrees of freedom                                 2\n  P-value (Chi-square)                           0.797\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  happy =~                                                              \n    H1                1.000                               0.922    1.203\n    H2                0.712       NA                      0.656    0.764\n    H3               -2.144       NA                     -1.976   -1.587\n  sad =~                                                                \n    S1                1.000                               0.571    0.752\n    S2                0.393       NA                      0.224    0.204\n    S3                2.194       NA                      1.252    1.653\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n .H1 ~~                                                                 \n   .H2               -0.038       NA                     -0.038   -0.135\n   .H3                2.388       NA                      2.388    3.038\n .H2 ~~                                                                 \n   .H3                1.944       NA                      1.944    2.288\n .S1 ~~                                                                 \n   .S2                0.425       NA                      0.425    0.789\n   .S3               -0.228       NA                     -0.228   -0.459\n .S2 ~~                                                                 \n   .S3                0.274       NA                      0.274    0.255\n  happy ~~                                                              \n    sad              -0.001       NA                     -0.002   -0.002\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .H1               -0.263       NA                     -0.263   -0.447\n   .H2                0.307       NA                      0.307    0.416\n   .H3               -2.353       NA                     -2.353   -1.517\n   .S1                0.250       NA                      0.250    0.434\n   .S2                1.162       NA                      1.162    0.958\n   .S3               -0.994       NA                     -0.994   -1.731\n    happy             0.850       NA                      1.000    1.000\n    sad               0.326       NA                      1.000    1.000\n\n\nUh-oh…the model failed to converge :(. Apparently this is a common thing with CFA models that try to learn the correlation between within-factor variables – the parameters are non-identified because you’re asking the model to learn their correlation simultaneously in two different parameters: the factor loading and the covariance parameter. This Stack Exchange thread explains it nicely.\n\n\n\n\nal, Green et. n.d. “Measurement Error Masks Bipolarity in Affect Ratings.” Journal of Personality and Social Psychology 64(6).\n\n\nBrown, Timothy A. 2006. Confirmatory Factor Analysis for Applied Research."
  },
  {
    "objectID": "measurement-invariance-testing.html",
    "href": "measurement-invariance-testing.html",
    "title": "4  MTMM and Error Structure Modelling",
    "section": "",
    "text": "In this chapter we’ll work through another example of the Traditional CFA Workflow to get more practice. We’ll also introduce the concept of ‘Modification Indexes’, which researchers often use to improve their model goodness of fit in a way that seems a bit suss to me. Probably a good thing to know about."
  },
  {
    "objectID": "measurement-invariance-testing.html#example-4-school-grades",
    "href": "measurement-invariance-testing.html#example-4-school-grades",
    "title": "4  MTMM and Error Structure Modelling",
    "section": "Example 4: School Grades",
    "text": "Example 4: School Grades\nNow let’s do an example taken from the Advanced Statistical Computing people at UCLA. The dataset comes from the High School and Beyond project, which tracks academic performance in the US along with some data about students.\nAs usual with CFA, my goal here is to convince somebody that some of my variables are confounded by a shared unmeasured (and unmeasurable) variable, and not by other unmeasured things in different ways from each other. Specifically, I want to convince you that four student grades, namely reading, writing, mathematics and science, are confounded by a shared unmeasurable variable called ‘academic performance’. Great.\n\nMeasurement Invariance\nBut there’s a problem: a reviewer might ask if it really makes sense to think of ‘academic performance’ as being the same thing for boy-labelled and girl-labelled people. So if I want to convince that reviewer of my usual ‘simple confounding’ DAG structure, then I’ll need to answer a few extra questions:\n\nDoes the model fit equally well when I fit it on the group-level sub-datasets in isolation?\nAre the data consistent with the idea that the different groups are actually confounded by the same latent thing? People like to test this by making sure the loadings are pretty similar across the models for the different groups. If the loadings are similar then I can can say they are ‘invariant’.\nDo the data themselves actually have stable properties across groups? If not, then even if the model fits the data equally well for different groups or at different times, and even if the loadings are pretty similar across groups, then that’s actually a bad thing if I want to convince you that the factor is the same thing for different groups! People generally just like to check this by including an intercept term in the linear regression for each variable in the CFA model. If these intercepts are pretty similar across groups or across timepoints then we can say they are ‘invariant’.\n\nWhen I’m worrying about these sorts of things, I am worrying about what people like to call measurement invariance. As Brown (2006) puts it, the big idea with ‘Measurement Invariance’ is the worry that:\n\n“if either the loading or the intercept [of a variable across groups] is noninvariant, [then the model thinks] the observed values of the indicator will differ between groups at a given level of the latent variable.”\n\nWe definitely don’t want a model that thinks that, because it is not consistent with what I’m trying to convince my reviewers of: that the observed variables are merely puppets, confounded by the same unmeasured variable in the same way across all groups or timepoints.\n\n\nMultigroup CFA\nThere are a few classical workflows for dealing with measurement invariance, which @Brown2006 details in chapter 7 of his book. But he recommends something called ‘Multigroup CFA’, so let’s go with that. We’ll be following the workflow for this type of model as presented in that chapter.\n\n‘Configural’ Invariance\nThe first step is to fit the model separately for the two groups in isolation and see whether they both have OK goodness of fit. So let’s split the data into two subsets based on the group we’re interested in, and then define the lavaan models with the usual syntax, but specifying that want the linear model of each variable to also have an intercept, as explained above:\n\n\nCode\n### Load the data\ndat <- read_csv('../data/ucla/hsbdemo.csv')\n\n### Load the data again but in split format, for what is to come.\ndat_split <- list(\n  boys  = dat %>% filter(female == \"female\"),\n  girls = dat %>% filter(female == \"male\")\n)\n\n### Define the basic CFA model\nonefac <- 'f1  =~ read + write + math + science'\n\n### Fit the model separately for each group\nonefac_models <- list(\n  onefac_boys  = cfa(onefac, data = dat_split$boys, meanstructure = TRUE),\n  onefac_girls = cfa(onefac, data = dat_split$girls, meanstructure = TRUE) \n)\n\n### Gaze at the parameter estimates\nonefac_models %>% map(summary, standardized = TRUE, fit.measures = TRUE)\n\n\n$onefac_boys\nlavaan 0.6.16 ended normally after 46 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        12\n\n  Number of observations                           109\n\nModel Test User Model:\n                                                      \n  Test statistic                                 1.903\n  Degrees of freedom                                 2\n  P-value (Chi-square)                           0.386\n\nModel Test Baseline Model:\n\n  Test statistic                               230.890\n  Degrees of freedom                                 6\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.001\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -1463.504\n  Loglikelihood unrestricted model (H1)      -1462.553\n                                                      \n  Akaike (AIC)                                2951.009\n  Bayesian (BIC)                              2983.305\n  Sample-size adjusted Bayesian (SABIC)       2945.387\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.187\n  P-value H_0: RMSEA <= 0.050                    0.479\n  P-value H_0: RMSEA >= 0.080                    0.400\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.013\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  f1 =~                                                                 \n    read              1.000                               8.006    0.800\n    write             0.801    0.091    8.769    0.000    6.414    0.792\n    math              0.985    0.102    9.621    0.000    7.889    0.866\n    science           0.863    0.102    8.453    0.000    6.912    0.768\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .read             51.734    0.959   53.949    0.000   51.734    5.167\n   .write            54.991    0.775   70.911    0.000   54.991    6.792\n   .math             52.394    0.872   60.052    0.000   52.394    5.752\n   .science          50.697    0.862   58.830    0.000   50.697    5.635\n    f1                0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .read             36.133    6.426    5.623    0.000   36.133    0.360\n   .write            24.410    4.271    5.716    0.000   24.410    0.372\n   .math             20.736    4.693    4.419    0.000   20.736    0.250\n   .science          33.165    5.555    5.971    0.000   33.165    0.410\n    f1               64.099   13.331    4.808    0.000    1.000    1.000\n\n\n$onefac_girls\nlavaan 0.6.16 ended normally after 44 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        12\n\n  Number of observations                            91\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.719\n  Degrees of freedom                                 2\n  P-value (Chi-square)                           0.698\n\nModel Test Baseline Model:\n\n  Test statistic                               176.055\n  Degrees of freedom                                 6\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.023\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -1275.517\n  Loglikelihood unrestricted model (H1)      -1275.157\n                                                      \n  Akaike (AIC)                                2575.033\n  Bayesian (BIC)                              2605.164\n  Sample-size adjusted Bayesian (SABIC)       2567.288\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.153\n  P-value H_0: RMSEA <= 0.050                    0.750\n  P-value H_0: RMSEA >= 0.080                    0.186\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.009\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  f1 =~                                                                 \n    read              1.000                               8.531    0.816\n    write             0.954    0.119    7.983    0.000    8.137    0.794\n    math              0.863    0.113    7.663    0.000    7.361    0.766\n    science           0.999    0.124    8.031    0.000    8.521    0.798\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .read             52.824    1.095   48.227    0.000   52.824    5.056\n   .write            50.121    1.074   46.653    0.000   50.121    4.891\n   .math             52.945    1.008   52.548    0.000   52.945    5.508\n   .science          53.231    1.119   47.577    0.000   53.231    4.987\n    f1                0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .read             36.404    7.769    4.686    0.000   36.404    0.333\n   .write            38.825    7.775    4.993    0.000   38.825    0.370\n   .math             38.197    7.210    5.298    0.000   38.197    0.413\n   .science          41.300    8.365    4.937    0.000   41.300    0.363\n    f1               72.774   16.251    4.478    0.000    1.000    1.000\n\n\nThe first thing I notice is that the models don’t fit great. Indeed, these are the first significant chi-squared test p-values I’ve ever seen in all of these examples, indicating that the results are consistent with there being lots of residual variance the model hasn’t accounted for. But the UCLA people don’t comment on this, so I guess neither will I.\nNext I notice that the factor loadings and residual variances look pretty good and consistent across the groups. This is suggestive of what people unfortunately like to call Configural Invariance, which just means the same model fits to the groups pretty much the same in isolation. As (Brown?) puts it:\n\n“equal form [aka ‘configural invariance’ is when] the number of factors and pattern of indicator–factor loadings are identical across groups)”\n\nThe main exception to this I notice in the above model is that there’s a bunch more residual variance in ‘math’ for boys than for girls. So maybe that’s something to look out for.\nThe next thing to do is fit the exact same model as above, but in a slightly fancier syntax. Specifically, we’re gonna fit it with a single command so that it can serve as the best-fitting big daddy model when we start constraining parameters to be equal across groups and doing the nested likelihood ratio test stuff we’ll be doing later. I think this is literally the exact same thing as the previous model but it serves that LRT-daddy role by giving us a single chi-squared goodness-of-fit statistic for the whole dataset, rather than one for each group in isolation. Honestly I’m not sure why both (Brown?) and UCLA have us fit the previous model at all.\n\n\nCode\nconfigural.fit <- cfa(onefac, data = dat, group = \"female\", meanstructure = TRUE)\n\n\nNotice how we just did the exact same thing as before, but we used the full dataset instead of the split sub-datasets, and we used cfa() function’s group parameter to tell the model we’re interested in group stuff. I’m not actually gonna print the outputs for this model because the loadings and residual variances are the exact same for the previous model, and the single chi-squared statistic is simply the sum of the chi-squared statistics from the previous model.\n\n\n4.0.0.1 ‘Metric’ / ‘Weak’ Invariance\nNext we’re gonna want to see if goodness-of-fit isn’t significantly reduced when we constrain the loading for each variable to be equal in both models. The idea is that if the loadings are pretty much equal then that’s consistent with the variables all being confounded to the same degree by the same unmeasured thing for both boys and girls. The conventional terrible name for this is ‘Metric’ invariance or ‘Weak’ invariance, but (Brown?) just calls it ‘equal loadings’, which seems fine to me.\nWe can fit this model in **lavaan* using the cfa() function’s group.equal argument.\n\n\nCode\nequal.loadings.fit <- cfa(onefac, data = dat, group = \"female\", \n  group.equal = c(\"loadings\"), meanstructure = TRUE) \n\nsummary(equal.loadings.fit, standardized = TRUE, fit.measures = TRUE)\n\n\nlavaan 0.6.16 ended normally after 74 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        24\n  Number of equality constraints                     3\n\n  Number of observations per group:                   \n    female                                         109\n    male                                            91\n\nModel Test User Model:\n                                                      \n  Test statistic                                 6.801\n  Degrees of freedom                                 7\n  P-value (Chi-square)                           0.450\n  Test statistic for each group:\n    female                                       3.692\n    male                                         3.109\n\nModel Test Baseline Model:\n\n  Test statistic                               406.945\n  Degrees of freedom                                12\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.001\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -2741.111\n  Loglikelihood unrestricted model (H1)      -2737.710\n                                                      \n  Akaike (AIC)                                5524.221\n  Bayesian (BIC)                              5593.486\n  Sample-size adjusted Bayesian (SABIC)       5526.956\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.121\n  P-value H_0: RMSEA <= 0.050                    0.612\n  P-value H_0: RMSEA >= 0.080                    0.212\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.044\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [female]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  f1 =~                                                                 \n    read              1.000                               7.844    0.790\n    write   (.p2.)    0.866    0.074   11.744    0.000    6.789    0.816\n    math    (.p3.)    0.939    0.077   12.214    0.000    7.365    0.836\n    science (.p4.)    0.928    0.080   11.590    0.000    7.277    0.790\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .read             51.734    0.951   54.410    0.000   51.734    5.212\n   .write            54.991    0.797   69.011    0.000   54.991    6.610\n   .math             52.394    0.843   62.128    0.000   52.394    5.951\n   .science          50.697    0.882   57.472    0.000   50.697    5.505\n    f1                0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .read             37.013    6.379    5.802    0.000   37.013    0.376\n   .write            23.119    4.238    5.455    0.000   23.119    0.334\n   .math             23.282    4.536    5.133    0.000   23.282    0.300\n   .science          31.858    5.499    5.794    0.000   31.858    0.376\n    f1               61.530   11.526    5.338    0.000    1.000    1.000\n\n\nGroup 2 [male]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  f1 =~                                                                 \n    read              1.000                               8.664    0.822\n    write   (.p2.)    0.866    0.074   11.744    0.000    7.498    0.760\n    math    (.p3.)    0.939    0.077   12.214    0.000    8.134    0.803\n    science (.p4.)    0.928    0.080   11.590    0.000    8.037    0.775\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .read             52.824    1.105   47.796    0.000   52.824    5.010\n   .write            50.121    1.034   48.484    0.000   50.121    5.082\n   .math             52.945    1.062   49.862    0.000   52.945    5.227\n   .science          53.231    1.088   48.942    0.000   53.231    5.130\n    f1                0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .read             36.095    7.641    4.724    0.000   36.095    0.325\n   .write            41.025    7.538    5.442    0.000   41.025    0.422\n   .math             36.439    7.293    4.996    0.000   36.439    0.355\n   .science          43.048    8.114    5.305    0.000   43.048    0.400\n    f1               75.057   14.697    5.107    0.000    1.000    1.000\n\n\nNotice how in this output the unstandardized loadings are the same in each group, except for the loading for the first variable, which we sacrificed to define the scale of the factor like we usually do. But notice how the standardized loadings are still different.\nThe loadings and residual variances still look pretty good in this model, but let’s do the likelihood ratio test to see if people will believe me when I tell them I have solid ‘metric’ invariance\n\n\nCode\nanova(configural.fit, equal.loadings.fit)\n\n\n\nChi-Squared Difference Test\n\n                   Df    AIC    BIC Chisq Chisq diff   RMSEA Df diff Pr(>Chisq)\nconfigural.fit      4 5526.0 5605.2 2.622                                      \nequal.loadings.fit  7 5524.2 5593.5 6.801      4.179 0.06269       3     0.2428\n\n\nThat p-value isn’t significant, so we’re off to the races. So far so good.\n\n\n4.0.0.2 ‘Scalar’ / ‘Strong’ Invariance\nMoving on now to test whether the goodness of fit is still ok when we constrain the variable-level means to be equal:\n\n\nCode\nequal.intercepts.fit <- cfa(onefac, data = dat, group = \"female\", \n                            group.equal = c(\"loadings\",\"intercepts\"), meanstructure = TRUE)\n\nsummary(equal.intercepts.fit, standardized = TRUE, fit.measures = TRUE)\n\n\nlavaan 0.6.16 ended normally after 108 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        25\n  Number of equality constraints                     7\n\n  Number of observations per group:                   \n    female                                         109\n    male                                            91\n\nModel Test User Model:\n                                                      \n  Test statistic                                47.779\n  Degrees of freedom                                10\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    female                                      14.313\n    male                                        33.466\n\nModel Test Baseline Model:\n\n  Test statistic                               406.945\n  Degrees of freedom                                12\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.904\n  Tucker-Lewis Index (TLI)                       0.885\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -2761.600\n  Loglikelihood unrestricted model (H1)      -2737.710\n                                                      \n  Akaike (AIC)                                5559.200\n  Bayesian (BIC)                              5618.569\n  Sample-size adjusted Bayesian (SABIC)       5561.543\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.194\n  90 Percent confidence interval - lower         0.141\n  90 Percent confidence interval - upper         0.251\n  P-value H_0: RMSEA <= 0.050                    0.000\n  P-value H_0: RMSEA >= 0.080                    1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.089\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [female]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  f1 =~                                                                 \n    read              1.000                               7.961    0.797\n    write   (.p2.)    0.828    0.076   10.884    0.000    6.592    0.788\n    math    (.p3.)    0.940    0.077   12.151    0.000    7.479    0.846\n    science (.p4.)    0.915    0.081   11.318    0.000    7.288    0.784\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .read    (.10.)   52.164    0.898   58.065    0.000   52.164    5.224\n   .write   (.11.)   53.633    0.766   70.021    0.000   53.633    6.412\n   .math    (.12.)   52.534    0.818   64.187    0.000   52.534    5.941\n   .science (.13.)   51.595    0.839   61.520    0.000   51.595    5.552\n    f1                0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .read             36.315    6.399    5.675    0.000   36.315    0.364\n   .write            26.507    4.602    5.759    0.000   26.507    0.379\n   .math             22.251    4.545    4.896    0.000   22.251    0.285\n   .science          33.247    5.715    5.818    0.000   33.247    0.385\n    f1               63.376   11.894    5.328    0.000    1.000    1.000\n\n\nGroup 2 [male]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  f1 =~                                                                 \n    read              1.000                               8.640    0.822\n    write   (.p2.)    0.828    0.076   10.884    0.000    7.155    0.681\n    math    (.p3.)    0.940    0.077   12.151    0.000    8.117    0.804\n    science (.p4.)    0.915    0.081   11.318    0.000    7.910    0.758\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .read    (.10.)   52.164    0.898   58.065    0.000   52.164    4.964\n   .write   (.11.)   53.633    0.766   70.021    0.000   53.633    5.103\n   .math    (.12.)   52.534    0.818   64.187    0.000   52.534    5.206\n   .science (.13.)   51.595    0.839   61.520    0.000   51.595    4.945\n    f1                0.152    1.272    0.119    0.905    0.018    0.018\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .read             35.798    7.935    4.512    0.000   35.798    0.324\n   .write            59.273   10.111    5.862    0.000   59.273    0.537\n   .math             35.924    7.495    4.793    0.000   35.924    0.353\n   .science          46.310    8.702    5.322    0.000   46.310    0.425\n    f1               74.649   14.704    5.077    0.000    1.000    1.000\n\n\nYup, as expected, each variable mean is constrained to be the same across groups. And how about that likelihood ratio test?\n\n\nCode\nanova(configural.fit, equal.loadings.fit, equal.intercepts.fit)\n\n\n\nChi-Squared Difference Test\n\n                     Df    AIC    BIC  Chisq Chisq diff   RMSEA Df diff\nconfigural.fit        4 5526.0 5605.2  2.622                           \nequal.loadings.fit    7 5524.2 5593.5  6.801      4.179 0.06269       3\nequal.intercepts.fit 10 5559.2 5618.6 47.779     40.978 0.35580       3\n                     Pr(>Chisq)    \nconfigural.fit                     \nequal.loadings.fit       0.2428    \nequal.intercepts.fit  6.609e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOh no! The p-value is highly significant, so nobody will believe me if I tell them I have ‘strong’ invariance. In other words, my data are consistent with the possibility that even though the variables all load on the factor to the same extent across groups, they still have different values at the same level of each variable. Going back to our primordial DAG of simple confounding, I think this is just another way of saying that the data are consistent with there being secret confounders influencing the variables in one group but not the other. So nobody is gonna believe my DAG.\nThis opens the door to what (Brown?) calls ‘Partial Invariance’. He encourages us to look at modification indexes like we saw in Example 2 above, and see if freeing up a couple of the fixed parameters would improve goodness of fit. He says this is a fine thing to do, while exposing us to the ever-present risk of noise-mining. As he puts it:\n\n“[Once you’ve freed a parameter from needing to be equal across groups and the LRT no longer returns a significant p-value], the invariance evaluation may proceed [in accordance with the usual workflow]. The researcher will freely estimate the [now free parameter] in both groups in subsequent [steps of the usual analysis]. Indeed, Byrne et al. (1989) note that such analyses may proceed as long as there exists at least one noninvariant parameter other than the marker indicator”.\n\nPersonally yeah this seems like noise-mining, but let’s give it a try just for fun.\n\n\nCode\nmodindices(equal.intercepts.fit, sort = TRUE) %>% \n  \n  # Arrange them in order of modification index\n  arrange(desc(mi)) %>% \n  \n  select(lhs, op, rhs, mi)\n\n\n     lhs op     rhs    mi\n1   read ~~    math 3.396\n2   read ~~    math 2.805\n3   read ~~ science 1.670\n4   read ~~ science 0.741\n5   read ~~   write 0.585\n6  write ~~    math 0.497\n7  write ~~ science 0.375\n8  write ~~ science 0.240\n9   read ~~   write 0.210\n10  math ~~ science 0.154\n11 write ~~    math 0.085\n12    f1 =~    read 0.024\n13    f1 =~    read 0.024\n14  math ~~ science 0.007\n\n\nHmm, looks like our old friend modindices() doesn’t return estimates for parameters constrained to be equal across groups. But it is showing some interesting stuff. Like maybe instead of freeing up a group-constrained parameter, I could just free up that reading <–> math residual correlation. It feels like a real education researcher could whip up a path diagram that makes this seem justified, and I just tested it and it makes it so that the measurement invariance actually works for the intercepts, even when they are still constrained across groups! So maybe I would just proceed that way.\nBut just for posterity, here’s how you can look at the modification indexes for the group-constrained parameters:\n\n\nCode\nlavTestScore(equal.intercepts.fit)\n\n\n$test\n\ntotal score test:\n\n   test     X2 df p.value\n1 score 40.018  7       0\n\n$uni\n\nunivariate score tests:\n\n    lhs op   rhs     X2 df p.value\n1  .p2. == .p16.  0.766  1   0.381\n2  .p3. == .p17.  2.674  1   0.102\n3  .p4. == .p18.  1.308  1   0.253\n4 .p10. == .p24.  1.722  1   0.189\n5 .p11. == .p25. 33.415  1   0.000\n6 .p12. == .p26.  0.407  1   0.524\n7 .p13. == .p27.  9.051  1   0.003\n\n\nAnnoyingly, it doesn’t tell you the variable names. So you’ll need to check and see what they are called in the model output. Also I think these aren’t technically ‘modification indexes’ per se, but they are analogous.\nLooks like that .p11 == .p25 constraint is a juicy one to free up – this corresponds to the reading variable. To free it up I’ll need to refit the model with more explicit syntax. Specifically, I’ll need to use the group.partial() argument to override the fixedness introduced in the group.equal() argument:\n\n\nCode\n### Partial invariance model\npartial.invariance.fit <- cfa(\n  onefac, \n  dat, \n  group = \"female\", \n  group.equal = c(\"loadings\", \"intercepts\"), \n  group.partial=c(\"read~1\"), # This frees up the desired intercepts\n  meanstructure = TRUE)\n\n\nNow we can re-run the likelihood ratio test and see if we’re good to proceed to testing for invariance of the residual variance terms:\n\n\nCode\nanova(configural.fit, equal.loadings.fit, partial.invariance.fit)\n\n\n\nChi-Squared Difference Test\n\n                       Df    AIC    BIC  Chisq Chisq diff   RMSEA Df diff\nconfigural.fit          4 5526.0 5605.2  2.622                           \nequal.loadings.fit      7 5524.2 5593.5  6.801      4.179 0.06269       3\npartial.invariance.fit  9 5559.3 5622.0 45.885     39.084 0.43060       2\n                       Pr(>Chisq)    \nconfigural.fit                       \nequal.loadings.fit         0.2428    \npartial.invariance.fit  3.259e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nGah, allowing the intercept to be freely estimated has improved the p-value, but it still looks like the data are consistent with the idea that the observed variables have different values across groups for the same value of the latent variable. Darn! We could keep going, IE checking the modification indexes and freeing up parameters until we pass the likelihood ratio test, but that doesn’t feel so good to me. These data just aren’t consistent with the theory offered by the primordial DAG of simple confounding.\n\n\n\n\nBrown, Timothy A. 2006. Confirmatory Factor Analysis for Applied Research."
  },
  {
    "objectID": "regression-with-latent-predictors.html",
    "href": "regression-with-latent-predictors.html",
    "title": "5  Regression With Latent Predictors",
    "section": "",
    "text": "Code\n# Load packages\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(blavaan)\nlibrary(brms)\nlibrary(tidybayes)\nlibrary(survival)\nlibrary(survminer)\n\n# Stan backend specifications\noptions(brms.backend = \"cmdstanr\")\noptions(mc.cores = parallel::detectCores())\n\n# Borrow the Gustav Klimt pallate from MetBrewer https://github.com/BlakeRMills/MetBrewer/blob/main/R/PaletteCode.R\nclrs <- list(pink = \"#df9ed4\", red = \"#c93f55\", yellow = \"#eacc62\", green = \"#469d76\", blue = \"#3c4b99\", purple = \"#924099\")\n\n# Make a reusable ggplot theme, borrowing from Andrew Heiss: https://www.andrewheiss.com/blog/2022/05/20/marginalia/\ntheme_nice <- function() {\n   theme_minimal(base_family = \"Lato\") +\n   theme(panel.grid.major = element_blank(),\n          plot.background = element_rect(fill = \"white\", color = NA),\n          plot.title = element_text(face = \"bold\"),\n          axis.title = element_text(face = \"bold\"),\n          strip.text = element_text(face = \"bold\"),\n          legend.title = element_text(face = \"bold\"))\n}\n\n# A function for density plots, of which we have many.\ntheme_posterior_densities <- function() {\n  theme(axis.text.y = element_blank()) \n}"
  },
  {
    "objectID": "regression-with-latent-predictors.html#summary",
    "href": "regression-with-latent-predictors.html#summary",
    "title": "5  Regression With Latent Predictors",
    "section": "Summary",
    "text": "Summary\nI recently completed an analysis for a client who designs labour market training programs. They had developed some scales to measure latent traits related to employability, and were interested in the relationship between those traits and time-to-employment among program graduates. I approached this using a Bayesian multilevel survival model with latent predictors, simultaneously estimating a factor model for the latent variables and including them as predictors in a time-to-event analysis for employment outcomes.\nThis document uses simulated data to implement a simplified version of that model and demonstrate its advantages over traditional two-stage approaches for regression with latent predictors."
  },
  {
    "objectID": "regression-with-latent-predictors.html#introduction",
    "href": "regression-with-latent-predictors.html#introduction",
    "title": "5  Regression With Latent Predictors",
    "section": "5.1 Introduction",
    "text": "5.1 Introduction\nIn the previous sections we’ve explored the traditional workflow for confirmatory factor analysis including the basic model structure, the concepts of validity and reliability, classic model goodness of fit tests, tests for measurement invariance, and experimenting with custom error covariance structures. These are all helpful tools for specifying quantitative theories about imaginary constructs, and testing those theories with data.\nBut often we want to do more than just assess the construct validity of our factors: we may also have theories about how those imagined constructs relate to other variables. If those other variables are themselves latent factors then we enter the world of ‘Structural Equation Modelling (SEM)’, which expands on factor analysis to account for relationships between latent predictors and latent outcome variables.\nI plan to add more chapters to this notebook covering a traditional SEM workflow, but as a first step it’s helpful to explore a more basic situation where we want to estimate the relationship between some latent variables and an observed outcome variable. A traditional way to incorporate latent variables into a regression analysis for a measured dependent variable proceeds like so:\n\nFit a CFA model for the latent variables;\nUse that CFA model to generate factor scores for each observation;\nUse those factor scores as predictors in a regression.\n\nThis is the approach used by Kankaraš, Feron, and Renbarger (2019) in their study of the relationship between latent social/emotional skills and life outcomes such as academic achievement: the authors use point-estimate factor scores as predictors in regressions on life outcome data. In some cases they even do this iteratively, fitting CFA models on point-estimate factor scores from CFA models fit on point-estimate factor scores, and using those point-estimates as regression predictors!\nThis approach is unsatisfactory because factor scores are a function of the CFA model’s parameter estimates, about which there is uncertainty. By only using point-estimate factor scores in their regression models for the measured life outcomes, the authors ignore this uncertainty.\nA better option is to fit a Bayesian model that estimates the factor analysis and the substantive regression model at the same time, incorporating its uncertainty from the CFA model into its substantive parameter estimates and predictions. As we’ll see below, this approach is essentially just a Bayesian missing data analysis, where the factor scores for each observation are treated as missing data for which the model estimates a unique observation-specific parameter, along with its own unique posterior distribution."
  },
  {
    "objectID": "regression-with-latent-predictors.html#the-plan",
    "href": "regression-with-latent-predictors.html#the-plan",
    "title": "5  Regression With Latent Predictors",
    "section": "5.2 The Plan",
    "text": "5.2 The Plan\nIn this document I’ll demonstrate how to implement this Bayesian approach with Stan. We’ll iterate on this model a few times, gradually incorporating concepts from previous chapters such as MTMM factor analysis. We’ll stick to an invented scenario: a client has designed scales for the latent skills “adaptability” and “collaboration”, and wants us to estimate the relationship between those skills and time-to-employment for their program graduates. As the model becomes more complex we’ll simulate more complex datasets as well, and show that the model can recover the true simulation parameter estimates. We’ll proceed as follows:\n\nFit a simple Bayesian CFA with one factor and a basic measurement error covariance structure. We use the brms package to illustrate how Bayesian factor analysis is the same as Bayesian missing data analysis, but where all of your observations are missing;\nFit a Bayesian CFA with two correlated factors. The brms package can’t yet handle custom covariance structures for latent factors, so we swtich to raw Stan;\nUpdate the model to include a custom error covariance structure à la MTMM;\nCombine the MTMM model from the previous step with a parametric proportional hazards regression model;\nExpand the model to include multilevel structure, with correlated varying effects for the regression predictors.\n\nThis document assumes familiarity with survival analysis, Bayesian statistics, and latent variable modelling at least up to the level covered in the previous sections of this workbook."
  },
  {
    "objectID": "regression-with-latent-predictors.html#simple-bayesian-cfa-with-brms",
    "href": "regression-with-latent-predictors.html#simple-bayesian-cfa-with-brms",
    "title": "5  Regression With Latent Predictors",
    "section": "5.3 Simple Bayesian CFA with brms",
    "text": "5.3 Simple Bayesian CFA with brms\nBefore diving into a full model with our two factors for “adaptability” and “collabortion”, in this section we’ll see how to fit a simple Bayesian CFA in brms. Nothing fancy, just one factor with 6 measurements. Here’s the full model definition:\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\nm_{1} \\\\\nm_{2} \\\\\nm_{3} \\\\\nm_{4} \\\\\nm_{5} \\\\\nm_{6}\n\\end{bmatrix}\n&\\sim \\text{MVNormal}\n\\left(\n\\begin{bmatrix}\n\\mu_{m1} \\\\\n\\mu_{m2} \\\\\n\\mu_{m3} \\\\\n\\mu_{m4} \\\\\n\\mu_{m5} \\\\\n\\mu_{m6}\n\\end{bmatrix},\n\\Sigma_m\n\\right) \\\\\n\\\\\n\\mu_{m1} &= \\lambda_1 \\text{f}_1, \\quad \\mu_{m2} = \\lambda_2 \\text{f}_1, \\quad \\mu_{m3} = \\lambda_3 \\text{f}_1 \\\\\n\\mu_{m4} &= \\lambda_4 \\text{f}_1, \\quad \\mu_{m5} = \\lambda_5 \\text{f}_1, \\quad \\mu_{m6} = \\lambda_6 \\text{f}_1 \\\\\n\\\\\n\\text{f}_1 &\\sim \\text{Normal}(\\mu_\\text{f}, \\sigma_\\text{f}^2) \\\\\n\\\\\n\\Sigma_m &=\n\\begin{bmatrix}\n\\sigma_{m1}^2 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & \\sigma_{m2}^2 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & \\sigma_{m3}^2 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & \\sigma_{m4}^2 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & \\sigma_{m5}^2 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & \\sigma_{m6}^2\n\\end{bmatrix}\n\\end{aligned}\n\\]\nFirst we can simulate some data with the given factor structure using the lavaan package’s handy simulateData() function:\n\n\nCode\n# Specify the factor structure in classic lavaan syntax\nbasic.model <- ' \n  f1 =~ .8*m1 + .1*m2 + .6*m3 + .2*m4 + .9*m5 + -.4*m6\n'\n\n# Simulate data from the specified model \nfake_dat <- lavaan::simulateData(model = basic.model, sample.nobs = 4000)\n\n\nNow we can specify a Bayesian model in brms that recovers the simulation parameter values. I’ve worked from the approach shared by Jack Bailey in a post on the Stan forum. I like this approach because it gives a nice conceptual perspective on what we’re actually doing when we’re doing latant variable modelling: we’re just fitting a linear regression with multiple dependent variables drawn from a shared distribution, and where the linear models of their location parameters include a covariate for which we only have missing data. So the first thing we need to do is add a column of all missing data for each of the latent variables. Then we define the model with the same approach to identifiability constraints that lavaan imposes by default, I.E. constraining the first loading to be constant. The jargon term for this approach to ensuring identifiability is the ‘marker variable approach’:\n\n\nCode\n# Add the latent variable to the dataset as an all NA column\nfake_dat$f1 <- NA_real_\n\n# Define the model, with the coefficient for m1 fied to a constant for identifiability\nbfit.1 <- brm(\n  formula =\n    bf(m1 ~ 0 + mi(f1)) +\n    bf(m2 ~ 0 + mi(f1)) +\n    bf(m3 ~ 0 + mi(f1)) +\n    bf(m4 ~ 0 + mi(f1)) +\n    bf(m5 ~ 0 + mi(f1)) +\n    bf(m6 ~ 0 + mi(f1)) +\n    bf(f1| mi() ~ 1) + \n    set_rescor(rescor = FALSE),\n  family = gaussian(),\n  prior =\n    prior(constant(1), class = \"b\", resp = \"m1\") + ## First loading fixed at 1, per the 'marker variable' approach. \n    prior(constant(1), class = \"sigma\", resp = \"m1\") +\n    prior(normal(0, 10), class = \"b\", resp = \"m2\") +\n    prior(constant(1), class = \"sigma\", resp = \"m2\") +\n    prior(normal(0, 10), class = \"b\", resp = \"m3\") +\n    prior(constant(1), class = \"sigma\", resp = \"m3\") +\n    prior(normal(0, 10), class = \"b\", resp = \"m4\") +\n    prior(constant(1), class = \"sigma\", resp = \"m4\") +\n    prior(normal(0, 10), class = \"b\", resp = \"m5\") +\n    prior(constant(1), class = \"sigma\", resp = \"m5\") +\n    prior(normal(0, 10), class = \"b\", resp = \"m6\") +\n    prior(constant(1), class = \"sigma\", resp = \"m6\") +\n    prior(normal(0, 10), class = \"Intercept\", resp = \"f1\") +\n    prior(cauchy(0, 1), class = \"sigma\", resp = \"f1\"),\n  data = fake_dat,\n  warmup = 1000,\n  iter = 6000,\n  file = \"fits/b07.01.rds\"\n)\n\n\nA Bayesian missing data model treats each missing observation as a parameter to estimate. And since the factor is 100% missing data, this means we end up with 4000 rows of data x 6000 samples x 4 chains = 96,000,000 MCMC samples for the factor scores alone, resulting in a pretty big model file. This is too big for Github, so I’ll only push the draws we need to make sure the model recovered the true parameter estimates.\nWe can put the draws we need into a tidy format and do some cleaning, then we can plot them to visualize the approximated posteriors:\n\n\nCode\nbfit.1.samples <- bfit.1 |>\n\n  # Get the raw MCMC samples\n  gather_draws(\n    bsp_m2_mif1,\n    bsp_m3_mif1,\n    bsp_m4_mif1,\n    bsp_m5_mif1,\n    bsp_m6_mif1\n  ) |>\n\n  # Do some renaming for clarity\n  mutate(.variable = case_when(\n    .variable == \"bsp_m2_mif1\" ~ \"Loading for m2\",\n    .variable == \"bsp_m3_mif1\" ~ \"Loading for m3\",\n    .variable == \"bsp_m4_mif1\" ~ \"Loading for m4\",\n    .variable == \"bsp_m5_mif1\" ~ \"Loading for m5\",\n    .variable == \"bsp_m6_mif1\" ~ \"Loading for m6\"\n  )) |> \n\n  # Add the true factor loadings from above\n  mutate(true_loading = case_when(\n    .variable == \"Loading for m2\" ~ .1,\n    .variable == \"Loading for m3\" ~ .6,\n    .variable == \"Loading for m4\" ~ .2,\n    .variable == \"Loading for m5\" ~ .9,\n    .variable == \"Loading for m6\" ~ -.4\n  )) \n\n# Save the tidy draws for reproducibility\nsaveRDS(bfit.1.samples, \"fits/b07.01.samples.rds\")\n\n\n\n\nCode\n# Load the tidy samples\nbfit.1.samples <- readRDS(\"fits/b07.01.samples.rds\")\n\n# Plot\nbfit.1.samples|>\n\n  ggplot(aes(x = .value, fill = .variable)) +\n    stat_halfeye(fill = clrs$purple) +\n    geom_vline(aes(xintercept = true_loading), linetype = 2) + \n    scale_x_continuous(expand = c(0, 0.015)) +\n    scale_y_continuous(expand = c(0, 0.015)) +\n    guides(fill = \"none\") +\n    theme_nice() +\n    theme_posterior_densities() +\n    facet_wrap(~.variable, scales = \"free_x\") \n\n\n\n\n\nEstimated posterior densities of factor loadings for CFA model fit with brms. The loading for m1 was not estimated because we used the marker variable approach for identifiability.\n\n\n\n\nThe model didn’t do a great job, with the true parameter values falling outside the 95% interval for all loadings. But the results are not totally off-base, and the model has successfully captured the overall magnitude and sign of each loading. This illustrates the general proof of concept that we can do a decent CFA in brms.\nIn practice I would probably not choose brms for a simple CFA of this kind, where we have no prior information to inform the parameter estimates and no need to incorporate uncertainty in factor score estimates into models of other variables. If I were committed to doing this sort of model Bayesianly then I would use the blavaan package, which works with lavaan syntax, allows you to specify custom priors, and is lightening-fast compared to the brms version implemented above. But blavaan is limited in the types of models it can handle, and cannot fit the type of time-to-event model we are heading towards here. Stan offers greater flexibility."
  },
  {
    "objectID": "regression-with-latent-predictors.html#bayesian-cfa-with-correlated-factors",
    "href": "regression-with-latent-predictors.html#bayesian-cfa-with-correlated-factors",
    "title": "5  Regression With Latent Predictors",
    "section": "5.4 Bayesian CFA with Correlated Factors",
    "text": "5.4 Bayesian CFA with Correlated Factors\nNow let’s introduce our two factors “adaptability” and “collaboration”, and expand the model to assume these two latent factors may be correlated. People traditionally use this type of model to assess the discriminant validity of their factors, as we saw in Chapter 1. The only change from the model in the previous section is that now we imagine the two factors to be drawn from a shared bivariate normal distribution in which the diagonals are the factor-specific residual variances and the off-diagonals are the between-factor covariance, given as the product of their estimated correlation \\(\\rho\\) and the two factor-specific variances. Here’s the updated model definition, with the covariance terms highlighted:\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\nm_{1} \\\\\nm_{2} \\\\\nm_{3} \\\\\nm_{4} \\\\\nm_{5} \\\\\nm_{6}\n\\end{bmatrix}\n&\\sim \\text{MVNormal}\n\\left(\n\\begin{bmatrix}\n\\mu_{m1} \\\\\n\\mu_{m2} \\\\\n\\mu_{m3} \\\\\n\\mu_{m4} \\\\\n\\mu_{m5} \\\\\n\\mu_{m6}\n\\end{bmatrix},\n\\Sigma_m\n\\right) \\\\\n\\\\\n\\mu_{m1} &= \\lambda_1 \\text{adapt}, \\quad \\mu_{m2} = \\lambda_2 \\text{adapt}, \\quad \\mu_{m3} = \\lambda_3 \\text{adapt} \\\\\n\\mu_{m4} &= \\lambda_4 \\text{collab}, \\quad \\mu_{m5} = \\lambda_5 \\text{collab}, \\quad \\mu_{m6} = \\lambda_6 \\text{collab} \\\\\n\\\\\n\\begin{bmatrix}\n\\text{adapt}_{i} \\\\\n\\text{collab}_{i}\n\\end{bmatrix}\n&\\sim \\text{MVNormal}\n\\left(\n\\begin{bmatrix}\n\\mu_\\text{adapt} \\\\\n\\mu_\\text{collab}\n\\end{bmatrix},\n\\Sigma_f\n\\right) \\\\\n\\\\\n\\Sigma_f &=\n\\begin{bmatrix}\n\\sigma_\\text{adapt}^2 & \\color{#c93f55}{\\rho \\sigma_\\text{adapt} \\sigma_\\text{collab}} \\\\\n\\color{#c93f55}{\\rho \\sigma_\\text{adapt} \\sigma_\\text{collab}} & \\sigma_\\text{adapt}^2\n\\end{bmatrix}\n\\\\\n\\Sigma_m &=\n\\begin{bmatrix}\n\\sigma_{m1}^2 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & \\sigma_{m2}^2 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & \\sigma_{m3}^2 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & \\sigma_{m4}^2 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & \\sigma_{m5}^2 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & \\sigma_{m6}^2\n\\end{bmatrix}\n\\end{aligned}\n\\]\nWe can update the lavaan model structure from the previous section to simulate new data with correlated factors. Note that in the lavaan syntax f1 ~~ .4*f2 we are specifying the covariance between the factors, not the correlation. But as we’ll see later, the distinction doesn’t matter in this case.\n\n\nCode\n# Declare the new model with correlated factors\ncorrelated.factors.model <- ' \n  f1 =~ .8*m1 + .6*m2 + .9*m3\n  f2 =~ .1*m4 + .2*m5 + -.4*m6\n  f1 ~~ .4*f2\n'\n\n# Simulate data from the model\nfake_dat <- lavaan::simulateData(model = correlated.factors.model, sample.nobs = 4000)\n\n\nNow to fit a model to the simulated data. We’ll use raw Stan from now on, because brms doesn’t currently support setting constraints on individual correlation terms in the model, even in the form of constant priors. There seems to be a plan to include more SEM-like flexibility of covariance matrixes in general in brms 3.0, but we do better to dive into raw Stan anyway for added flexibility.\nI tried a few different model definitions in Stan, but they all had convergence issues and failed to recover the true parameter estimates. Convergence can be tricky in factor analysis, and things get even more complicated in a Bayesian context where we’re using MCMC to approximate the posterior. So I took to the Stan Forums, where I found a few people who had faced similar issues in Bayesian factor analysis. I have worked closely from the strategy provided in this comment by Mauricio Garnier-Villarre. Instead of the lavaan-default ‘marker variable approach’ to identifiability we used above in our brms model, Mauricio fixes the factor means and variances at 0 and 1, respectively, which has the benefit of letting the model freely estimate all of the factor loadings. He also adds some code to sign-correct the estimated loadings in the generated quantities{} block, which apparently helps align estimates across chains. I just make a few minor adjustments such as removing intercepts from the linear predictors of the factors to better align the model with the lavaan defaults. Here is the Stan code:\ndata {\n  int N; // sample size\n  int P; // number of variables\n  int D; // number of factors\n  array[N] vector[P] X; // data matrix of order [N,P]\n  int n_lam; // how many factor loadings are estimated\n}\nparameters {\n  matrix[N, D] FS_UNC; // factor scores, matrix of order [N,D]\n  cholesky_factor_corr[D] L_corr_d; // cholesky correlation between factors\n  vector<lower=0>[P] sd_p; // residual sd for each variable\n  vector<lower=-30, upper=30>[n_lam] lam_UNC; // A bunch of lightly-constrained factor loadings\n}\ntransformed parameters {\n  // a vector to hold the factor means, which will be a bunch of 0s\n  vector[D] M;\n  M = rep_vector(0, D);\n  \n  // a vector to hold the factor SDs, which will be a bunch of 1s. \n  vector<lower=0>[D] Sd_d;\n  Sd_d = rep_vector(1, D);\n  \n  // A vector to hold the linear predictors for the manifest variables, which are each a deterministic function of loading*factor_score\n  array[N] vector[P] mu_UNC;\n  for (i in 1 : N) {\n    mu_UNC[i, 1] = lam_UNC[1] * FS_UNC[i, 1];\n    mu_UNC[i, 2] = lam_UNC[2] * FS_UNC[i, 1];\n    mu_UNC[i, 3] = lam_UNC[3] * FS_UNC[i, 1];\n    mu_UNC[i, 4] = lam_UNC[4] * FS_UNC[i, 2];\n    mu_UNC[i, 5] = lam_UNC[5] * FS_UNC[i, 2];\n    mu_UNC[i, 6] = lam_UNC[6] * FS_UNC[i, 2];\n  }\n  \n  // the var-covar matrix for the factors, a deterministic function of the deterministic SDs and the estimated rhos defined in parameters{}\n  cholesky_factor_cov[D] L_Sigma;\n  L_Sigma = diag_pre_multiply(Sd_d, L_corr_d);\n}\nmodel {\n  // Declare some priors\n  L_corr_d ~ lkj_corr_cholesky(1); // Prior on factor corrs\n  lam_UNC ~ normal(0, 10); // Prior on loadings\n  sd_p ~ cauchy(0, 2.5); // Prior on residual SDs of manifest variables\n  \n  // Set up the likelihoods of the manifest and latent factors\n  for (i in 1 : N) {\n    for (j in 1 : P) {\n      // Manifest variables\n      X[i, j] ~ normal(mu_UNC[i, j], sd_p[j]);\n    }\n    // Latent factors\n    FS_UNC[i] ~ multi_normal_cholesky(M, L_Sigma);\n  }\n}\ngenerated quantities {\n  corr_matrix[D] Rho_UNC; /// correlation matrix\n  corr_matrix[D] Rho; /// correlation matrix\n  matrix[P, D] lam; // factor loadings\n  matrix[N, D] FS; // factor scores, matrix of order [N,D]\n  \n  // Do some fancy things to sign-correct the parameter estimates.\n  // The idea seems to be that when we estimate the loadings with unconstrained signs\n  // It can lead to identification issues. So we take these steps to correct the signs.\n  // See this Stan forum comment for more: https://discourse.mc-stan.org/t/non-convergence-of-latent-variable-model/12450/10?\n  Rho_UNC = multiply_lower_tri_self_transpose(L_corr_d);\n  Rho = Rho_UNC;\n  FS = FS_UNC;\n  lam = rep_matrix(0, P, D);\n  lam[1 : 3, 1] = to_vector(lam_UNC[1 : 3]);\n  lam[4 : 6, 2] = to_vector(lam_UNC[4 : 6]);\n  \n  // factor 1\n  if (lam_UNC[1] < 0) {\n    lam[1 : 3, 1] = to_vector(-1 * lam_UNC[1 : 3]);\n    FS[ : , 1] = to_vector(-1 * FS_UNC[ : , 1]);\n    \n    if (lam_UNC[4] > 0) {\n      Rho[1, 2] = -1 * Rho_UNC[1, 2];\n      Rho[2, 1] = -1 * Rho_UNC[2, 1];\n    }\n  }\n  // factor 2\n  if (lam_UNC[4] < 0) {\n    lam[4 : 6, 2] = to_vector(-1 * lam_UNC[4 : 6]);\n    FS[ : , 2] = to_vector(-1 * FS_UNC[ : , 2]);\n    \n    if (lam_UNC[1] > 0) {\n      Rho[2, 1] = -1 * Rho_UNC[2, 1];\n      Rho[1, 2] = -1 * Rho_UNC[1, 2];\n    }\n  }\n}\nNow we can compile and run the model:\n\n\nCode\n# Write the raw Stan code from an R string to a Stan file\nstan_file <- cmdstanr::write_stan_file(stan_model_code)\n\n# Autoformat the model syntax to preempt any warnings at compile time\nmodel <- cmdstanr::cmdstan_model(stan_file, compile = FALSE)\nmodel$format(canonicalize = TRUE)\n\n# Compile the model from the Stan file\nmodel <- cmdstanr::cmdstan_model(stan_file)\n\n# Put the data into a list format Stan can understand\nX <- as.matrix(fake_dat)\nP <- ncol(fake_dat) \nN <- nrow(fake_dat)\nD <- 2\n\ndata_list <- list(N=N,P=P,D=D,X=X, n_lam=6)\n\n# Fit the model\nfit <- model$sample(\n  data = data_list,\n  seed = 199,\n  chains = 4,\n  parallel_chains = 4,\n  iter_warmup = 1000,\n  iter_sampling = 6000,\n  adapt_delta=0.99, \n  max_treedepth = 12,\n  refresh = 100 \n)\n\n# Save the resulting model fit\nsaveRDS(fit, \"fits/b07.02.rds\")\n\n\nThe model took about 5 hours to sample and the resulting .csv files containing the MCMC samples are large – on the order of 2.5GB each. Fortunately we are able to selectively load only the draws from variables we’re interested in, so we have no problem working in memory.\n\n\nCode\n# Load the cmdstanr model object\nfit <- readRDS(\"fits/b07.02.rds\")\n\n# Load the MCMC draws from the variables we care about\ndraws <- fit$draws(\n  variables = c(\n  \"lam[1,1]\",\n  \"lam[2,1]\",\n  \"lam[3,1]\",\n  \"lam[4,2]\",\n  \"lam[5,2]\",\n  \"lam[6,2]\",\n  \"Rho[2,1]\"\n  )\n)\n\n# Save the result\nsaveRDS(draws, \"fits/b07.02.samples.rds\")\n\n\nNow we can explore the results. One important point is that we can interpret the Stan model’s estimates of the correlation coefficient between the two factors directly as covariances (IE analogous to the true parameter simulated with lavaan), because we fixed the factor standard deviations to be equal to 1. Put differently, because our sigmas are both equal to 1, this equation:\n\\[\\begin{aligned}\n\\rho(X, Y) = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\n\\end{aligned}\\]\nJust becomes this:\n\\[\\begin{aligned}\n\\rho(X, Y) = \\frac{\\text{Cov}(X, Y)}{1 * 1} = \\text{Cov}(X, Y)\n\\end{aligned}\\]\nWe’re interested in the loadings and the between-factor covariance. So we can wrangle the MCMC samples for those variables and plot them to look at the estimated posteriers. First let’s look at the loadings. Based on this plot it looks like the model did pretty well – the posterior peak is close to the true value for all of the loadings:\n\n\nCode\n# Load the draws for the loadings and correlation \ndraws <- readRDS(\"fits/b07.02.samples.rds\")\n\n# Plot\ndplt <- draws |>\n\n  # Get the raw draws for the factor loadings and the beween-factor correlation coefficient\n  gather_draws(\n    `lam[1,1]`,\n    `lam[2,1]`,\n    `lam[3,1]`,\n    `lam[4,2]`,\n    `lam[5,2]`,\n    `lam[6,2]`,\n    `Rho[2,1]`\n  ) |>\n\n  # Do some renaming for clarity\n  mutate(.variable = case_when(\n    .variable == \"lam[1,1]\" ~ \"Loading for m1\",\n    .variable == \"lam[2,1]\" ~ \"Loading for m2\",\n    .variable == \"lam[3,1]\" ~ \"Loading for m3\",\n    .variable == \"lam[4,2]\" ~ \"Loading for m4\",\n    .variable == \"lam[5,2]\" ~ \"Loading for m5\",\n    .variable == \"lam[6,2]\" ~ \"Loading for m6\",\n    .variable == \"Rho[2,1]\" ~ \"Covariance for f1 ~ f2\",\n  )) |>\n\n  # Add the true factor loadings for plotting\n  mutate(true_loading = case_when(\n    .variable == \"Loading for m1\" ~ .8,\n    .variable == \"Loading for m2\" ~ .6,\n    .variable == \"Loading for m3\" ~ .9,\n    .variable == \"Loading for m4\" ~ .1,\n    .variable == \"Loading for m5\" ~ .2,\n    .variable == \"Loading for m6\" ~ -.4,\n    .variable == \"Covariance for f1 ~ f2\" ~ .4\n  )) \n\ndplt |>\n\n  filter(.variable != \"Covariance for f1 ~ f2\") |>\n\n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(fill = clrs$purple) +\n  geom_vline(aes(xintercept = true_loading), linetype = 2) + \n  scale_x_continuous(expand = c(0, 0.015)) +\n  scale_y_continuous(expand = c(0, 0.015)) +\n  theme_nice() +\n  theme_posterior_densities() +\n  labs(\n    x = \"Estimate\",\n    y = \"Density\"\n  ) +\n # theme_posterior_densities() +\n  facet_wrap(~.variable, scales = \"free_x\") \n\n\n\n\n\nBayesian posterior intervals of factor loadings for CFA model fit with Stan. All loadings are estimated because we do not use the ‘marker variable approach’ for identifiability,\n\n\n\n\nThe model also seems to have accurately recovered the true between-factor covariance, albeit with some unwholesome weirdness in the tail of its approximated posterior. This weirdness appears to be pulling the mean estimate upwards away from the true parameter estimate:\n\n\nCode\ndplt |>\n\n  filter(\n    .variable == \"Covariance for f1 ~ f2\",\n    .value >= 0,\n  ) |>\n\n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(fill = clrs$red) +\n  geom_vline(aes(xintercept = true_loading), linetype = 2) + \n  scale_x_continuous(expand = c(0, 0.015)) +\n  scale_y_continuous(expand = c(0, 0.015)) +\n  guides(fill = \"none\") +\n  theme_nice() +\n  theme_posterior_densities() +\n  labs(\n    x = \"Estimate\",\n    y = \"Density\"\n  ) +\n  facet_wrap(~.variable, scales = \"free_x\") \n\n\n\n\n\nPosterior densitity of between-factor covariance.\n\n\n\n\nWe can also look at the MCMC convergence diagnostics such as rhat and effective number of samples. Here we see that we have rhats slightly greater than 1 for a few of the loadings and for rho. We also see that loading 4, loading 6, and rho have a pretty low number of effective samples. This is cause for concern, and in a real application I would probably increase the number of MCMC iterations to see if we can improve that effective sample size, and if that failed then I would think of ways to reparameterize the model.\n\n\nCode\ndraws |> \n\n  summary() |>\n\n  select(variable, rhat, ess_bulk, ess_tail) |>\n\n  mutate(across(where(is.numeric), ~round(.x, 2))) |>\n\n  knitr::kable()\n\n\n\n\n\nvariable\nrhat\ness_bulk\ness_tail\n\n\n\n\nlam[1,1]\n1.00\n8935.85\n15354.16\n\n\nlam[2,1]\n1.00\n16440.47\n18512.91\n\n\nlam[3,1]\n1.00\n6788.10\n11132.78\n\n\nlam[4,2]\n1.01\n806.68\n450.49\n\n\nlam[5,2]\n1.00\n1929.60\n2266.63\n\n\nlam[6,2]\n1.02\n345.23\n122.68\n\n\nRho[2,1]\n1.02\n294.12\n124.70\n\n\n\n\n\nThe trace plots seem healthy overall, albeit with some occasional spikes and some parts where one chain goes berzerk relative to the others. Chain 4 for the between-factor correlation seems especially problematic, with it following a strange trajectory during its final iterations. Perhaps this explains why that variable has the highest rhat and displays some irregularity in the tail of its approximated posterior. Still, even for that parameter the chains seem generally healthy overall.\n\n\nCode\n# Trace plots\ndraws |>\n\n  bayesplot::mcmc_trace() + \n  scale_color_manual(values = c(\"blue\", \"red\", \"orange\", clrs$purple)) + \n  theme_minimal() +\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  )\n\n\nWe can feel ok about the model’s performance here because it recovered the true parameter values fairly well, but the MCMC convergence checks suggest our chains were not totally happy.\nOne last thing before moving on: as mentioned above, we could have simply used the blavaan package to fit this model instead of fussing with custom Stan code. The blavaan version fits way faster and recovers the true parameter estimates. But blavaan is less flexible than Stan, and crucially for the present analysis, there is no way to do survival analysis with blavaan. But for posterity, below is the syntax we would use to fit the model with blavaan. If we specify mcmcfile = TRUE then the resulting Stan file gets written to a new directory called lavExport, or we can supply a filepath as the argument in which case the Stan file will get written there. The file is very large (over 1000 lines) and confusing, I think because it is precompiled to handle any possible model you could specify in R. The Stan documentation includes an example for specifying custom priors in a blavaan model.\n\n\nCode\n# Fit the model using the same lavaan-syntax model use used above to simulate the dataset\nblavfit.1 <- bcfa(correlated.factors.model, data=fake_dat, mcmcfile = T)\n\n# Gaze at parameter estimates\nblavfit.1 |> summary()\n\n# Make sure the MCMC diagnostics are ok\nblavInspect(blavfit.1, \"mcobj\")"
  },
  {
    "objectID": "regression-with-latent-predictors.html#bayesian-cfa-with-custom-covariance-structure",
    "href": "regression-with-latent-predictors.html#bayesian-cfa-with-custom-covariance-structure",
    "title": "5  Regression With Latent Predictors",
    "section": "5.5 Bayesian CFA with Custom Covariance Structure",
    "text": "5.5 Bayesian CFA with Custom Covariance Structure\nOften in latent variable modelling we want to account for the possibility that certain measured variables are confounded by some shared unmeasured influences. One way to address this is to add another factor to the model to account for these unmeasured influences, but it can be simpler to just estimate a covariance term for those measured variables. This is what Brown (2006) calls a ‘correlated uniqueness model’. I have more detailed notes on these ideas in Chapter 3.\nAs an example of this scenario, we can imagine that the measured variables m1 and m4 are confounded because they were both collected by participant survey, while the other four measures were each collected by different means. To reflect this change in the model, we need to change the specification of the variance-covariance matrix of the shared multivariable likelihood of the measured variables: where before all of the off-diagonal elements were fixed at 0, now we need the off-diagonals representing the covariance between m1 and m4 to be freely estimated.\nHere’s the updated model, with the new parameter highlighted in yellow:\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\nm_{1} \\\\\nm_{2} \\\\\nm_{3} \\\\\nm_{4} \\\\\nm_{5} \\\\\nm_{6}\n\\end{bmatrix}\n&\\sim \\text{MVNormal}\n\\left(\n\\begin{bmatrix}\n\\mu_{m1} \\\\\n\\mu_{m2} \\\\\n\\mu_{m3} \\\\\n\\mu_{m4} \\\\\n\\mu_{m5} \\\\\n\\mu_{m6}\n\\end{bmatrix},\n\\Sigma_m\n\\right) \\\\\n\\\\\n\\mu_{m1} &= \\lambda_1 \\text{adapt}, \\quad \\mu_{m2} = \\lambda_2 \\text{adapt}, \\quad \\mu_{m3} = \\lambda_3 \\text{adapt} \\\\\n\\mu_{m4} &= \\lambda_4 \\text{collab}, \\quad \\mu_{m5} = \\lambda_5 \\text{collab}, \\quad \\mu_{m6} = \\lambda_6 \\text{collab} \\\\\n\\\\\n\\begin{bmatrix}\n\\text{adapt}_{i} \\\\\n\\text{collab}_{i}\n\\end{bmatrix}\n&\\sim \\text{MVNormal}\n\\left(\n\\begin{bmatrix}\n\\mu_\\text{adapt} \\\\\n\\mu_\\text{collab}\n\\end{bmatrix},\n\\Sigma_f\n\\right) \\\\\n\\\\\n\\Sigma_f &=\n\\begin{bmatrix}\n\\sigma_\\text{adapt}^2 & \\color{#c93f55}{\\rho \\sigma_\\text{adapt} \\sigma_\\text{collab}} \\\\\n\\color{#c93f55}{\\rho \\sigma_\\text{adapt} \\sigma_\\text{collab}} & \\sigma_\\text{adapt}^2\n\\end{bmatrix}\n\\\\\n\\Sigma_m &=\n\\begin{bmatrix}\n\\sigma_{m1}^2 & 0 & 0 & \\color{#eacc62}{\\rho\\ \\sigma_{m1} \\sigma_{m4}} & 0 & 0 \\\\\n0 & \\sigma_{m2}^2 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & \\sigma_{m3}^2 & 0 & 0 & 0 \\\\\n\\color{#eacc62}{\\rho \\sigma_{m1} \\sigma_{m4}} & 0 & 0 & \\sigma_{m4}^2 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & \\sigma_{m5}^2 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & \\sigma_{m6}^2\n\\end{bmatrix}\n\\end{aligned}\n\\]\nWe can simulate data from this DAG using lavaan like we did for the two models above, updating the syntax to give m1 and m4 a covariance of .4:\n\n\nCode\n# Declare the new lavaan model where m1 and m4 covary\nmtmm.model <- ' \n  f1 =~ .8*m1 + .6*m2 + .9*m3\n  f2 =~ .1*m4 + .2*m5 + -.4*m6\n  f1 ~~ .4*f2\n  m1 ~~ .4*m4\n'\n\n# Simulate data from the lavaan model\nfake_dat <- lavaan::simulateData(model = mtmm.model, sample.nobs = 4000)\n\n\nWe’ll also need to add a new parameter to our Stan model to account for the possibility that m1 and m4 covary. This only involves a few changes, namely:\n\nParameters Block: declare a matrix L_corr_m1_m4 to represent the Cholesky factorization of a correlation matrix between m1 and m4;\nTransformed Parameters Block: declare the Cholesky factorization of a covariance matrix L_Sigma_m1_m4 and populate it by matrix-multiplying the standard deviations of m1 and m4 with the Cholesky factorization of the correlation matrix declared above;\nModel Block:, declare m1 and m4 as drawn from a shared bivariate normal distribution separate from the other measured variables. This is equivalent to declaring all measured variales as drawn from a shared distribution like in the mathematical rendering of the model shown above, but it is simpler to implement. We can use the Cholesky factorization of a multivariate normal distribution, with the vector of the mu estimates of m1 and m4 as the mean vector, and the Cholesky factorization of the covariance matrix L_Sigma_m1_m4 we declared above as the covariance matrix.\nGenerated Quantities Block: pull out the correlation parameter for m1 and m4 from the estimated variance covariance matrix so we can analyze it.\n\ndata {\n  int N; // sample size\n  int P; // number of variables\n  int D; // number of factors\n  array[N] vector[P] X; // data matrix of order [N,P]\n  int n_lam; // how many factor loadings are estimated\n}\nparameters {\n  matrix[N, D] FS_UNC; // factor scores, matrix of order [N,D]\n  cholesky_factor_corr[D] L_corr_d; // cholesky correlation between factors\n  cholesky_factor_corr[2] L_corr_m1_m4; // cholesky correlation between the m1 and m4\n  vector<lower=0>[P] sd_p; // residual sd for each variable\n  vector<lower=-30, upper=30>[n_lam] lam_UNC; // A bunch of lightly-constrained factor loadings\n}\ntransformed parameters {\n  // a vector to hold the factor means, which will be a bunch of 0s\n  vector[D] M;\n  M = rep_vector(0, D);\n  \n  // a vector to hold the factor SDs, which will be a bunch of 1s. \n  vector<lower=0>[D] Sd_d;\n  Sd_d = rep_vector(1, D);\n  \n  // A vector to hold the linear predictors for the manifest variables, which are each a deterministic function of loading*factor_score\n  array[N] vector[P] mu_UNC;\n  for (i in 1 : N) {\n    mu_UNC[i, 1] = lam_UNC[1] * FS_UNC[i, 1];\n    mu_UNC[i, 2] = lam_UNC[2] * FS_UNC[i, 1];\n    mu_UNC[i, 3] = lam_UNC[3] * FS_UNC[i, 1];\n    mu_UNC[i, 4] = lam_UNC[4] * FS_UNC[i, 2];\n    mu_UNC[i, 5] = lam_UNC[5] * FS_UNC[i, 2];\n    mu_UNC[i, 6] = lam_UNC[6] * FS_UNC[i, 2];\n  }\n  \n  // the var-covar matrix for the factors, a deterministic function of the deterministic SDs and the estimated rhos defined in the parameters block\n  cholesky_factor_cov[D] L_Sigma;\n  L_Sigma = diag_pre_multiply(Sd_d, L_corr_d);\n\n  // Same, but for the var-covar matrix of m1 and m4\n  cholesky_factor_cov[D] L_Sigma_m1_m4;\n  L_Sigma_m1_m4 = diag_pre_multiply(sd_p[{1, 4}], L_corr_m1_m4);\n}\nmodel {\n  // Declare some priors\n  L_corr_d ~ lkj_corr_cholesky(1); // Prior on factor corrs\n  L_corr_m1_m4 ~ lkj_corr_cholesky(1); // Prior on corr between m1 and m4\n  lam_UNC ~ normal(0, 10); // Prior on loadings\n  sd_p ~ cauchy(0, 2.5); // Prior on residual SDs of manifest variables\n\n  // Set up the joint log likelihood function\n  for (i in 1 : N) {\n    // The uncorrelated measured variables\n    for (j in {2,3,5,6}) {\n      // Manifest variables\n      X[i, j] ~ normal(mu_UNC[i, j], sd_p[j]);\n    }\n\n    // m1 and m4, which get special treatment for being correlated\n    to_vector({X[i, 1], X[i, 4]}) ~ multi_normal_cholesky([mu_UNC[i, 1], mu_UNC[i, 4]]', L_Sigma_m1_m4);\n\n    // Latent factors\n    FS_UNC[i] ~ multi_normal_cholesky(M, L_Sigma);\n  }\n}\ngenerated quantities {\n  corr_matrix[D] Rho_UNC; /// correlation matrix for factors\n  corr_matrix[D] Rho_factors; /// correlation matrix for factors\n  corr_matrix[D] Rho_m1_m4; /// correlation matrix for m1 and m4\n  matrix[P, D] lam; // factor loadings\n  matrix[N, D] FS; // factor scores, matrix of order [N,D]\n  \n  // Do some fancy things to sign-correct the parameter estimates.\n  // The idea seems to be that when we estimate the loadings with unconstrained signs\n  // It can lead to identification issues. So we take these steps to correct the signs.\n  // See this Stan forum comment for more: https://discourse.mc-stan.org/t/non-convergence-of-latent-variable-model/12450/10\n  Rho_UNC = multiply_lower_tri_self_transpose(L_corr_d);\n  Rho_factors = Rho_UNC;\n  Rho_m1_m4 = multiply_lower_tri_self_transpose(L_corr_m1_m4);\n  FS = FS_UNC;\n  lam = rep_matrix(0, P, D);\n  lam[1 : 3, 1] = to_vector(lam_UNC[1 : 3]);\n  lam[4 : 6, 2] = to_vector(lam_UNC[4 : 6]);\n  \n  // factor 1\n  if (lam_UNC[1] < 0) {\n    lam[1 : 3, 1] = to_vector(-1 * lam_UNC[1 : 3]);\n    FS[ : , 1] = to_vector(-1 * FS_UNC[ : , 1]);\n    \n    if (lam_UNC[4] > 0) {\n      Rho_factors[1, 2] = -1 * Rho_UNC[1, 2];\n      Rho_factors[2, 1] = -1 * Rho_UNC[2, 1];\n    }\n  }\n  // factor 2\n  if (lam_UNC[4] < 0) {\n    lam[4 : 6, 2] = to_vector(-1 * lam_UNC[4 : 6]);\n    FS[ : , 2] = to_vector(-1 * FS_UNC[ : , 2]);\n    \n    if (lam_UNC[1] > 0) {\n      Rho_factors[2, 1] = -1 * Rho_UNC[2, 1];\n      Rho_factors[1, 2] = -1 * Rho_UNC[1, 2];\n    }\n  }\n}\nNow we can compile and run the model:\n\n\nCode\n# Write the raw Stan code from an R string to a Stan file\nstan_file <- cmdstanr::write_stan_file(stan_model_code)\n\n# Autoformat the model syntax to preempt any warnings at compile time\nmodel <- cmdstanr::cmdstan_model(stan_file, compile = FALSE)\nmodel$format(canonicalize = TRUE)\n\n# Compile the model from the Stan file\nmodel <- cmdstanr::cmdstan_model(stan_file)\n\n# Put the data into a list format Stan can understand\nX <- as.matrix(fake_dat)\nP <- ncol(fake_dat) \nN <- nrow(fake_dat)\nD <- 2\n\ndata_list <- list(N=N,P=P,D=D,X=X, n_lam=6)\n\n# Fit the model\nfit <- model$sample(\n  data = data_list,\n  seed = 199,ftimes\n  chains = 4,\n  parallel_chains = 4,\n  iter_warmup = 1000,\n  iter_sampling = 6000,\n  refresh = 100 \n)\n\n# Save the resulting model fit\nsaveRDS(fit, \"fits/b07.03.rds\")\n\n\nOnce again, the resulting object is enormous. So let’s pare it down to only contain the draws of substantive interest.\n\n\nCode\n# Load the cmdstanr model object\nfit <- readRDS(\"fits/b07.03.rds\")\n\n# Load the MCMC draws from the variables we care about\ndraws <- fit$draws(\n  variables = c(\n  \"lam[1,1]\",\n  \"lam[2,1]\",\n  \"lam[3,1]\",\n  \"lam[4,2]\",\n  \"lam[5,2]\",\n  \"lam[6,2]\",\n  \"Rho_factors[2,1]\", \n  \"Rho_m1_m4[2,1]\"\n  )\n)\n\n# Save the result\nsaveRDS(draws, \"fits/b07.03.samples.rds\")\n\n\nHow did that go? It looks like the model did a good job recovering the true loadings:\n\n\nCode\n# Load the draws saved in the previous block\ndraws <- readRDS(\"fits/b07.03.samples.rds\")\n\n# Prepare the data for plotting\ndplt <- draws |>\n\n  # Get the raw draws for the factor loadings and the beween-factor correlation coefficient\n  gather_draws(\n   `lam[1,1]`,\n   `lam[2,1]`,\n   `lam[3,1]`,\n   `lam[4,2]`,\n   `lam[5,2]`,\n   `lam[6,2]`,\n   `Rho_factors[2,1]`,\n   `Rho_m1_m4[2,1]`,\n  ) |>\n\n  # Do some renaming for clarity\n  mutate(.variable = case_when(\n    .variable == \"lam[1,1]\" ~ \"Loading for m1\",\n    .variable == \"lam[2,1]\" ~ \"Loading for m2\",\n    .variable == \"lam[3,1]\" ~ \"Loading for m3\",\n    .variable == \"lam[4,2]\" ~ \"Loading for m4\",\n    .variable == \"lam[5,2]\" ~ \"Loading for m5\",\n    .variable == \"lam[6,2]\" ~ \"Loading for m6\",\n    .variable == \"Rho_factors[2,1]\" ~ \"Covariance for f1 ~ f2\",\n    .variable == \"Rho_m1_m4[2,1]\" ~ \"Covariance for m1 ~ m4\"\n  )) |>\n\n  # Add the true factor loadings for plotting\n  mutate(true_loading = case_when(\n    .variable == \"Loading for m1\" ~ .8,\n    .variable == \"Loading for m2\" ~ .6,\n    .variable == \"Loading for m3\" ~ .9,\n    .variable == \"Loading for m4\" ~ .1,\n    .variable == \"Loading for m5\" ~ .2,\n    .variable == \"Loading for m6\" ~ -.4,\n    .variable == \"Covariance for f1 ~ f2\" ~ .4,\n    .variable == \"Covariance for m1 ~ m4\" ~ .4\n  )) \n\n# Plot the loadings\ndplt |>\n\n  filter(!.variable %in% c(\"Covariance for f1 ~ f2\", \"Covariance for m1 ~ m4\")) |>\n\n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(fill = clrs$purple) +\n  geom_vline(aes(xintercept = true_loading), linetype = 2) + \n  scale_x_continuous(expand = c(0, 0.015)) +\n  scale_y_continuous(expand = c(0, 0.015)) +\n  guides(fill = \"none\") +\n  theme_nice() +\n  theme_posterior_densities() +\n  labs(\n    x = \"Estimate\",\n    y = \"Density\"\n  ) +\n  facet_wrap(~.variable, scales = \"free_x\") \n\n\n\n\n\nAnd things are looking good for the covariance terms as well:\n\n\nCode\n# Plot the covariance stuff\ndplt |>\n\n  filter(.variable %in% c(\"Covariance for f1 ~ f2\", \"Covariance for m1 ~ m4\")) |>\n\n  # For visual clarity, to get rid of some weird draws\n  filter(.value > 0) |>\n\n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(aes(fill = .variable)) +\n  geom_vline(aes(xintercept = true_loading), linetype = 2) + \n  scale_fill_manual(values = c(clrs$red, clrs$yellow)) +\n  scale_x_continuous(expand = c(0, 0.015)) +\n  scale_y_continuous(expand = c(0, 0.015)) +\n  guides(fill = \"none\") +\n  labs(x = \"lab\",\n     y = NULL)  +\n  facet_wrap(~.variable, scales = \"free_x\") +\n  theme_nice() +\n  theme_posterior_densities() +\n  labs(\n    x = \"Estimate\",\n    y = \"Density\"\n  ) +\n  facet_wrap(~.variable, scales = \"free_x\") \n\n\n\n\n\nOut of curiosity we can also check the MCMC diagnostics. The Rhats and effective sample sizes are looking a bit healthier than the previous model, which is interesting considering we’ve made the model more complex by adding a new parameter.\n\n\nCode\ndraws |>\n\n  summary() |>\n\n  select(variable, rhat, ess_bulk, ess_tail) |>\n\n  mutate(across(where(is.numeric), ~round(.x, 2))) |>\n\n  knitr::kable()\n\n\n\n\n\nvariable\nrhat\ness_bulk\ness_tail\n\n\n\n\nlam[1,1]\n1.00\n4652.90\n9876.69\n\n\nlam[2,1]\n1.00\n13721.92\n17472.99\n\n\nlam[3,1]\n1.00\n3707.37\n6725.97\n\n\nlam[4,2]\n1.00\n4983.17\n10820.66\n\n\nlam[5,2]\n1.00\n4287.63\n8968.60\n\n\nlam[6,2]\n1.01\n537.83\n956.49\n\n\nRho_factors[2,1]\n1.01\n400.12\n569.74\n\n\nRho_m1_m4[2,1]\n1.00\n8851.63\n14678.11\n\n\n\n\n\nThe trace plots also look generally healthy, albeit exhibiting a few of the same big spikes we saw in the previous example.\n\n\nCode\ndraws |>\n\n  bayesplot::mcmc_trace() + \n  scale_color_manual(values = c(\"blue\", \"red\", \"orange\", clrs$purple)) + \n  theme_minimal() +\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  )"
  },
  {
    "objectID": "regression-with-latent-predictors.html#bayesian-survival-analysis-with-latent-predictors",
    "href": "regression-with-latent-predictors.html#bayesian-survival-analysis-with-latent-predictors",
    "title": "5  Regression With Latent Predictors",
    "section": "5.6 Bayesian Survival Analysis with Latent Predictors",
    "text": "5.6 Bayesian Survival Analysis with Latent Predictors\nNow that we’re confident our CFA MTMM Stan model is well-specified, we can incorporate the substantive regression model for the outcome of interest: time to employment.\nThe updated model is shown below. All we’ve done is add two new lines at the top, shown in blue. These lines say we imagine time-to-event \\(T\\) for each person \\(i\\) to be drawn from a Weibull distribution, whose location parameter is a linear model of the latent factors \\(\\text{adapt}\\) and \\(\\text{collab}\\), as well as other measured variables in the matrix \\(\\mathbf{X}\\), each with its own coefficient in the vector \\(\\boldsymbol{\\beta}\\).\n\\[\\begin{aligned}\n\\color{#3c4b99}T_i &\\color{#3c4b99}\\sim \\text{Weibull}(\\theta_i, k) \\\\\n\\color{#3c4b99}\\theta_i &\\color{#3c4b99}= \\alpha + \\beta_1 \\text{adapt}_{i} + \\beta_2 \\text{collab}_{i} + \\mathbf{X}_i \\boldsymbol{\\beta} \\\\\n\\\\\n\\begin{bmatrix}\nm_{1} \\\\\nm_{2} \\\\\nm_{3} \\\\\nm_{4} \\\\\nm_{5} \\\\\nm_{6}\n\\end{bmatrix}\n&\\sim \\text{MVNormal}\n\\left(\n\\begin{bmatrix}\n\\mu_{m1} \\\\\n\\mu_{m2} \\\\\n\\mu_{m3} \\\\\n\\mu_{m4} \\\\\n\\mu_{m5} \\\\\n\\mu_{m6}\n\\end{bmatrix},\n\\Sigma_m\n\\right) \\\\\n\\\\\n\\mu_{m1} &= \\lambda_1 \\text{adapt}, \\quad \\mu_{m2} = \\lambda_2 \\text{adapt}, \\quad \\mu_{m3} = \\lambda_3 \\text{adapt} \\\\\n\\mu_{m4} &= \\lambda_4 \\text{collab}, \\quad \\mu_{m5} = \\lambda_5 \\text{collab}, \\quad \\mu_{m6} = \\lambda_6 \\text{collab} \\\\\n\\\\\n\\begin{bmatrix}\n\\text{adapt}_{i} \\\\\n\\text{collab}_{i}\n\\end{bmatrix}\n&\\sim \\text{MVNormal}\n\\left(\n\\begin{bmatrix}\n\\mu_\\text{adapt} \\\\\n\\mu_\\text{collab}\n\\end{bmatrix},\n\\Sigma_f\n\\right) \\\\\n\\\\\n\\Sigma_f &=\n\\begin{bmatrix}\n\\sigma_\\text{adapt}^2 & \\color{#c93f55}{\\rho \\sigma_\\text{adapt} \\sigma_\\text{collab}} \\\\\n\\color{#c93f55}{\\rho \\sigma_\\text{adapt} \\sigma_\\text{collab}} & \\sigma_\\text{adapt}^2\n\\end{bmatrix}\n\\\\\n\\Sigma_m &=\n\\begin{bmatrix}\n\\sigma_{m1}^2 & 0 & 0 & \\color{#eacc62}{\\rho\\ \\sigma_{m1} \\sigma_{m4}} & 0 & 0 \\\\\n0 & \\sigma_{m2}^2 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & \\sigma_{m3}^2 & 0 & 0 & 0 \\\\\n\\color{#eacc62}{\\rho \\sigma_{m1} \\sigma_{m4}} & 0 & 0 & \\sigma_{m4}^2 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & \\sigma_{m5}^2 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & \\sigma_{m6}^2\n\\end{bmatrix}\n\\end{aligned}\\]\nWhy a Weibull likelihood? According to Collett (2003), this is a common choice for parameteric survival analysis because it corresponds to a flexible monotonic baseline hazard function, which often feels like a safe assumption that is neither too rigid (like an exponential likelihood and corresponding flat baseline hazard function) nor too flexible (like a spline with many knots). It also has the advantage of being interpretable as both a Proportional Hazards regression or as an Accelerated Failure Time regression, which gives us flexibility in how we communicate about the parameter estimates, and allows us to not worry too much about the proportional hazards assumption.\n\n5.6.1 Data Simulation and Validation\nUnlike in previous sections, here we won’t be able to rely on lavaan::simulateData() to create our simulated data, because it can’t generate time-to-event data. So we’ll need to generate the data ourselves. Mark Lai has shared some nice code to do this, which I adapt slightly to allow for covariance between m1 and m4 as in the MTMM model above.\n\n\nCode\nset.seed(199)\n\n# Define sample size\nN <- 4000\n\n# Define the Fixed Parameters\nalpha <- c(0, 0)  # latent means\n\n# latent variances/covariances\nPhi <- matrix(c(1, 0.4, \n                0.4, 1), nrow = 2)  \n\n# factor loadings\nLambda <- cbind(c(.6, .6, .5, 0, 0, 0), \n                c(0, 0, 0, .1, .2, -.4))  \n\n# Error structure of measured variables\nTheta <- diag(c(c(1, 1, 1, 1, 1, 1)))\nTheta[4, 1] <- .4\nTheta[1, 4] <- .4\n\n# Generate latent factor scores\neta <- MASS::mvrnorm(N, mu = alpha, Sigma = Phi)\n\n# Generate residuals\ne <- MASS::mvrnorm(N, mu = rep(0, 6), Sigma = Theta)\n\n# Compute outcome scores: m_i = t(Lambda %*% eta_i) + e\nm <- tcrossprod(eta, Lambda) + e\n\n# Pack the measured variables and factor scores into a dataset\nfake_dat_measurement <- tibble(\n  m1 = m[,1],\n  m2 = m[,2],\n  m3 = m[,3],\n  m4 = m[,4],\n  m5 = m[,5],\n  m6 = m[,6],\n  adapt = eta[,1],\n  collab = eta[,2]\n)\n\n\nNow we have our ‘measured’ variables, simulated according to our chosen factor loadings and error structure. Just to be safe, let’s make sure lavaan can recover the true parameter estimates. We need to specify std.lv = TRUE to override lavaan’s default behaviour of using the marker variable approach for identifiability, and instead fix the variances of the latent variables to 1. If we don’t do this then the model doesn’t return the true simulated parameter estimates because the scales of the latent variables are transformed. Looks like lavaan can recover the true parameter estimates, which means our data simulation code does what we think it does:\n\n\nCode\nsurv.measurement.model <- ' \n  f1 =~ m1 + m2 + m3\n  f2 =~ m4 + m5 + m6\n\n  f1 ~~ f1\n  f2 ~~ f2\n\n  m1 ~~ m4\n'\n\n# Fit the model\nfit_test <- cfa(surv.measurement.model, data = fake_dat_measurement |> select(starts_with(\"m\")), std.lv = TRUE)\n\n# Make sure it works\nfit_test |> \n\n  broom::tidy() |>\n\n  select(term, estimate) |>\n\n  knitr::kable()\n\n\n\n\n\nterm\nestimate\n\n\n\n\nf1 =~ m1\n0.6678483\n\n\nf1 =~ m2\n0.5770462\n\n\nf1 =~ m3\n0.5188564\n\n\nf2 =~ m4\n0.1762007\n\n\nf2 =~ m5\n0.2140015\n\n\nf2 =~ m6\n-0.3767890\n\n\nf1 ~~ f1\n1.0000000\n\n\nf2 ~~ f2\n1.0000000\n\n\nm1 ~~ m4\n0.3992284\n\n\nm1 ~~ m1\n0.9306335\n\n\nm2 ~~ m2\n0.9550032\n\n\nm3 ~~ m3\n0.9701072\n\n\nm4 ~~ m4\n0.9710964\n\n\nm5 ~~ m5\n0.9948669\n\n\nm6 ~~ m6\n0.9899081\n\n\nf1 ~~ f2\n0.4361973\n\n\n\n\n\nNow we need to simulate the demographic variables and time-to-event data. We can draw the covariates straightforwardly from the relevent distributions, but the times-to-event will require a bit more thought. There are existing packages for simulating survival data in R (such as simsurv by Sam Brilleman of rstanarm fame) but since we took an explicit approach to simulating the latent variables this time, it’s nice to continue in that fashion for the entire dataset. I found this Stack Exchange answer by user Ryan SY Kwan helpful for learning an explicit workflow for simulating survival data from a Weibull distribution, which I implement here.\nFirstly, it’s convenient to express the Weibull distribution’s shape parameter with reference to the linear model, so that we can use R’s built-in rweibull() function to sample our event times. We can start from the PDF of the Weibull distribution:\n\\[\\begin{equation}\nf(x; k, \\lambda) = \\begin{cases}\n\\frac{k}{\\lambda} \\left( \\frac{x}{\\lambda} \\right)^{k-1} e^{-(x/\\lambda)^{k}} & x > 0, \\\\\n0 & x \\leq 0.\n\\end{cases}\n\\end{equation}\\]\nAnd its cumulative density function given by:\n\\[\\begin{equation}\nF(x; k, \\lambda) = 1 - \\exp\\left(-\\left(\\frac{x}{\\lambda}\\right)^{k}\\right)\n\\end{equation}\\]\nSo the survival curve, given by 1 - CDF, is:\n\\[\\begin{equation}\nS(x; k, \\lambda) = \\exp\\left(-\\left(\\frac{x}{\\lambda}\\right)^{k}\\right)\n\\end{equation}\\]\nAnd using the classic negative-log relationship between the survival function and the cumulative hazard function, we can say:\n\\[\\begin{equation}\nH(t) = -\\log\\left(\\exp\\left(-\\left(\\frac{t}{\\lambda}\\right)^{\\rho}\\right)\\right)\n\\end{equation}\\]\n\\[\\begin{equation}\nH(t) = \\left(\\frac{t}{\\lambda}\\right)^{\\rho}\n\\end{equation}\\]\nNow we have everything we need to implement the the classic form of a Cox Proportional Hazards regression, given by:=\n\\[\\begin{equation}\nH(t | \\mathbf{X}) = H_0(t) \\exp(\\mathbf{\\beta}' \\mathbf{X})\n\\end{equation}\\]\nSince this is directly equivalent to the model for the actual function of interest, the baseline hazard. As Singer and Willett (2003) put it, “This direct equivalence may not be intuitive, but it certainly is invaluable.”\n\\[\\begin{equation}\nh(t | \\mathbf{X}) = h_0(t) \\exp(\\mathbf{\\beta}' \\mathbf{X})\n\\end{equation}\\]\nSo we can take the definition of the Weibull cumulative hazard function we found above and substitute it into this equation, then take its negative exp to get it back in terms of the survival function:\n\\[\\begin{equation}\nS(t \\mid \\mathbf{x}) = \\exp\\left(-\\left(\\frac{t}{\\lambda}\\right)^{\\rho} \\exp(\\mathbf{\\beta}' \\mathbf{x})\\right)\n\\end{equation}\\]\nNow we can rearrange things to make the scale parameter \\(\\lambda\\) itself a function of the linear model. This allows us to use R’s built-in rweibull() function to generate the event times. All we need is some fancy algebra:\n\\[\\begin{align*}\nS(t \\mid x, \\lambda) &= \\exp\\left(-\\left(\\frac{t}{\\lambda}\\right)^{\\rho} \\exp(x' \\beta)\\right) \\\\\n&= \\exp\\left(-\\left(\\frac{t}{\\lambda}\\right)^{\\rho} \\exp\\left(\\frac{x' \\beta}{\\rho}\\right)\\rho\\right) \\\\\n&= \\exp\\left(-\\left(\\frac{t}{\\frac{\\lambda}{\\exp\\left(\\frac{x' \\beta}{\\rho}\\right)}}\\right)^{\\rho}\\right) \\\\\n\\end{align*}\\]\nNow the denominator incorporates both \\(\\lambda\\) and the linear model. So we can just take the whole denominator and call it its own new thing \\(\\lambda'\\):\n\\[\\begin{align*}\n\\lambda' = \\frac{\\lambda}{\\exp\\left(\\frac{x' \\beta}{\\rho}\\right)}\n\\end{align*}\\]\nThus we’ve snuck our linear model into the classic no-model definition of the Weibull survival function we saw above, just with \\(\\lambda'\\) instead of \\(\\lambda\\):\n\\[\\begin{equation}\nS(x; k, \\lambda) = \\exp\\left(-\\left(\\frac{x}{\\lambda'}\\right)^{k}\\right)\n\\end{equation}\\]\nThis is the parameterization we can implement when we go to create our event times. We’ll also imagine censoring times to be drawn from an unconditional exponential distribution, which creates non-informative censoring. We’ll generate an event time and a censoring time for each person, and a person’s status will be the shorter of those two times. We can start by choosing values for the Weibull distribution’s rho and lambda parameters that generate a realistic-seeming distribution of times-to-employment in the absence of a linear model. This is simpler conceptually if we define rho = 1. After some trial and error I settled on lambda = .02, which results in a mean time-to-employment of 22 days in the simulated dataset.\nNow we can choose the simulated parameter values for the linear model. It’s a bit tricky to understand what these parameter values mean on the outcome scale, because as mentioned above, Weibull regression parameters can be interpreted either as hazard ratios or acceleration factors. Brilleman et al. (2020) provide a clear overview:\n\n\\(\\exp(-\\beta_p(t))\\) is an acceleration factor: Sub-zero values of the transformed parameter estimates correspond to slower event times.\n\\(\\exp(\\beta_p(t))\\) is a survival time ratio: Sub-zero values of the transformed parameter estimates correspond to faster event times.\n\nSo we can declare them as survival time ratios for simplicity when implementing the simulation, but the survival package and Stan will return acceleration factors by default. This nice markdown document provides a walkthrough of the transformations required to move between these two interpretations in R. For example, we can imagine that being on employment insurance is associated with twice-as-fast time-to-employment because it means you have recent work experience. We simulate this by setting the coefficient beta_ie <- log(2), which corresponds to a survival time ratio of 2 and an acceleration factor of .5. Likewise, we can imagine that a one-unit increase in the latent trait ‘Collaboration Skills’ is associated with 20% faster job attainment, so we set beta_collab <- log(1.2), which corresponds to an acceleration factor of exp(-log(1.2)) ~ 0.833. We can proceed in the same way for the other parameter estimates.\n\n\nCode\n# Declare some parameter values for baseline hazard\nlambda <- .02\nrho <- 1 # keep rho = 1 to keep the conversions simple / make it easier to specify lambda\nlambda_wiki <- lambda^(-1 / rho) \n\n# Declare some true linear model parameters\nbeta_adapt <- log(1) # adaptability skills have no effect on time-to-employment\nbeta_collab <- log(1.2) # collaboration skills decrease time-to-employment by 20% for each standard deviation increase.\nbeta_age <- log(.9) # older people find employment a bit slower -- each increase in age of 1 standard deviation slows time-to-employment to 90% of what is was previously.\nbeta_ei <- log(2) # people on EI find employment twice as fast \n\n# Declare the rate for the exponential distribution from which we will sample censoring times\ncensor_rate = .005\n\n# Simulate the covariates and event times\nfake_dat <- fake_dat_measurement |>\n\n  rowid_to_column() |>\n\n  mutate(\n\n    # Simulate the covariates\n    age = rnorm(N), \n    ei_status = rbinom(N, 1, .4)\n\n  ) |>\n\n  group_by(rowid) |>\n\n  mutate(\n\n    # Simulate the event times and censoring times\n    lambda_prime = lambda_wiki / exp((beta_adapt*adapt + beta_collab*collab + beta_age*age + beta_ei*ei_status) / rho),\n    time_event = rweibull(1, shape=rho, scale=lambda_prime),\n    time_censor = rexp(1, censor_rate),\n    time = min(time_event, time_censor),\n    censored = ifelse(time_censor <= time_event, 1, 0),\n    status = ifelse(time_censor > time_event, 1, 0)\n\n  ) |>\n\n  ungroup() |>\n\n  # Remove the intermediary variables we don't need\n  select(-c(\n    lambda_prime,\n    time_event,\n    time_censor\n  ))\n\n\nIf we’ve done everything correctly then a basic survival analysis should be able to recover the true parameter estimates using the true model specification. Indeed, this is what we see:\n\n\nCode\n# Create the surv object\nsurv_object <- Surv(fake_dat$time, fake_dat$status)\n\n# Fit the model using the surv object\ntest <- survreg(surv_object ~ adapt + collab + age + ei_status, data=fake_dat, dist='weibull')\n\n# Wrangle and plot\ntest |> \n  \n  broom::tidy(conf.int = TRUE) |>\n\n  mutate(\n    estimate = exp(estimate),\n    conf.low = exp(conf.low),\n    conf.high = exp(conf.high)\n  ) |>\n\n  filter(!term %in% c(\"(Intercept)\", \"Log(scale)\")) |>\n\n  mutate(true_loading = case_when(\n    term == \"age\" ~ 1.11,\n    term == \"collab\" ~ 0.833,\n    term == \"adapt\" ~ 1,\n    term == \"ei_status\" ~ 0.5\n  )) |>\n\n  ggplot(aes(x = reorder(term, -estimate), y = estimate)) +\n    geom_point() +\n    geom_linerange(aes(ymin = conf.low, ymax = conf.high), size = 2, colour = \"#767676\") +\n    geom_point(size = 5, colour = \"#767676\") +\n    geom_point(size = 3, color = \"white\") +\n    geom_point(aes(x = term, y = true_loading), colour = \"red4\", size = 5, shape =  13, stroke = 1) +\n    coord_flip() + \n    labs(\n      y = \"Estimate\",\n      x = \"Variable\"\n    ) +\n    theme_minimal() \n\n\n\n\n\nA basic survival model recovers the true parameter values. Estimates and 95% CIs shown in grey; true parameter values shown as red targets.\n\n\n\n\nNow that we’ve simulated our full dataset, we can do some basic exploratory analysis of the data to make sure the event times make sense:\n\n\nCode\nfake_dat |>\n\n  mutate(\n    censored = as.factor(censored)\n  )|>\n\n  ggplot() + \n  geom_histogram(aes(x = time, fill = censored), colour = \"black\", position = \"identity\") +\n  scale_x_continuous(limits = c(3, 600), n.breaks = 10, expand=c(0, 0)) +\n  scale_y_continuous(expand=c(0, 0)) +\n  scale_fill_manual(\n    values = c(\"0\" = alpha(clrs$blue, 1), \"1\" = alpha(clrs$red, 1)),\n    name = NULL,  \n    labels = c(\"Employed\", \"Censored\")  \n  ) +\n  labs(\n    x = \"Days to Employment\",\n    y = \"N Graduates\"\n  ) +\n  theme_nice() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nHistogram of times-to-event, including employment outcomes and censorings.\n\n\n\n\nIt’s also a good idea to plot some stratified Kaplan-Meier curves to make sure our simulated parameter estimates mean what we think they mean. For example, we see that when we stratify by ei_status those with ei_status == TRUE see faster estimated event times, which is what we intended:\n\n\nCode\nkm.object <- survfit(surv_object ~ 1 + ei_status, data = fake_dat)\n\nplt <- survminer::ggsurvplot(\n  fit = km.object,\n  conf.int = TRUE,\n  risk.table = FALSE,\n  legend = \"none\",\n  palette = c(clrs$purple, \"#767676\"),\n  title = \"\",\n  xlab = \"Days Since Graduation\", \n  ylab = \"Proportion Unemployed\",\n  ggtheme = theme_bw()\n)  \n\nplt$plot +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  theme_nice() +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\nKaplan-Meier estimated survival curves, stratified by EI Status. Crosses are censorings.\n\n\n\n\n\n\n5.6.2 Prior Predictive Simulation\nUnlike the factor analysis models we fit above, this model has substantive predictor variables, namely age, ei_status, and the two latent variables adapt and collab. Based on previous research, we have a sense of the types of parameter estimates that seem realistic for these predictors. So can use prior predictive simulation to specify informative priors. Here’s what we’ll do: 1. Write some simple code to simulate from the prior distributions of interest; 2. Transform the simulated values to check whether the priors make sense on the scale of interest; 3. Once we arrive at reasonable-seeming priors for all of the parameters, we can use brms to simulate event times from the prior predictive distribution and make sure everything is making sense.\nThe brms parameterization of the Weibull distribution is slightly different than the one used in the survival package, with brms specifying a mean parameter as a function of the scale and shape parameters of the more traditional parameterization:\n\\[\\begin{align*}\ns = \\frac{\\mu}{\\Gamma\\left(1 + \\frac{1}{\\alpha}\\right)}\n\\end{align*}\\]\nSo when we specify a prior for the intercept term in our Bayesian model, we’re also specifying a prior for this transformation of the scale and shape parameters under the traditional parameterization:\n\\[\\begin{align*}\n\\mu = s \\times \\Gamma\\left(1 + \\frac{1}{\\alpha}\\right)\n\\end{align*}\\]\nI don’t think this has any big implications: setting a prior for the intercept is more intuitive than setting a prior for the scale and shape parameters indivdually anyway. We can copy this approach when we go to fit our Stan model.\nLet’s start with the prior for the intercept, specifying a prior on the raw ‘days-to-employment’ scale because that helps keep things interpretable.\nBased on previous studies I’ve done at my work, an average time-to-employment of 30 days seems reasonable, and an average higher than 60 days seems very unlikely. After much trial and error with the code below it seems like a gaussian with mean = 3.3, sd = .4 gives a distribution with appropriate characteristics to represent our uncertainty about average time-to-employment:\n\n\nCode\n# Simulate some data\nsim <- rnorm(10000, 3.3, .4) |>\n\n  as_tibble() |>\n\n  mutate(value = exp(value)) \n\n# Plot the distribution\nsim |>\n\n    ggplot(aes(x = value)) +\n    stat_halfeye(fill = clrs$pink) +\n    scale_x_continuous(n.breaks = 10, expand = c(0, 0)) +\n    scale_y_continuous(expand = c(0, 0)) +\n    labs(\n      x = \"rnorm draw\",\n      y = \"Density\"\n    ) +\n    theme_nice() +\n    theme_posterior_densities()\n\n\n\n\n\nPrior predictive check for average time-to-employment: draws from a gaussian distribution with mean = 3.3, and sd = .4\n\n\n\n\nNow for the beta coefficients. We can think of ourselves as settings priors as on log acceleration factors, so the exponentiated draws from the prior should reflect our uncertainty about how a one-unit change in that covariate might be associated with a faster or slower time-to-employment, keeping all other measured covariates equal. For any of the covariates, an acceleration factor of 10 or more in either direction (I.E. 10 or 0.10) seems pretty unlikely. As with the intercept, we fiddle with the code until we get a distribution that has the properties we want. much trial and error it seems like a gaussian with mean = 0, sd = 1.45 captures this ‘factor of 5’ intuition, with a factor of 10 or greater being associated with a probability of only about 5% in either direction.\n\n\nCode\n# Simulate some data\nsim <- rnorm(10000, 0, 1.45) |>\n\n  as_tibble() |>\n\n  mutate(value = exp(value)) \n\n# Plot the distribution\nsim |>\n\n    ggplot(aes(x = value)) +\n    stat_halfeye(fill = clrs$pink) +\n    scale_x_continuous(limits = c(0, 20), n.breaks = 10, expand = c(0, 0)) +\n    scale_y_continuous(expand = c(0, 0)) +\n    labs(\n      x = \"rnorm draw\",\n      y = \"Density\"\n    ) +\n    theme_nice() +\n    theme_posterior_densities() \n\n\n\n\n\nPrior predictive check for Weibull regression parameter estimates: exponentiated draws from a gaussian distribution with mean = 0, and sd = 1.45\n\n\n\n\nNow that we have some reasonable priors, we can sample from them to simulate from the prior predictive distribution. I don’t have any background knowledge to inform the estimate of the shape parameter, so we can set a conventional cauchy(0, 2.5) prior to keep it vague. The resulting densities of bounded 1-year times-to-event look reasonable to me, and there is plenty of uncertainty leftover for the model to refine its predictions in response to the data.\n\n\nCode\n# Declare the formula\nbf <- formula(time | cens(censored) ~ 0 + Intercept + adapt + collab + age + ei_status)\n\n# See how brms names the parameters, so we know how to specify priors form them\n# get_prior(\n#  formula = bf,\n#  data = fake_dat,\n#  family = weibull()\n#)\n\n# Declare the priors we want\npriors <- c(\n    prior(normal(3.3, .4), class = \"b\", coef = \"Intercept\"), # The average time-to-employment. Use gamma because times are positive and continuous\n    prior(cauchy(0, 2.5), class = \"shape\"), # Constrain the shape parameter, or else it thinks event times can go on basically forever.\n    prior(normal(0, 1.45), class = \"b\", coef = \"adapt\"),\n    prior(normal(0, 1.45), class = \"b\", coef = \"age\"),\n    prior(normal(0, 1.45), class = \"b\", coef = \"collab\"),\n    prior(normal(0, 1.45), class = \"b\", coef = \"ei_status\")\n  )\n\n# Fit the model\nfit.prior <- brm(\n  data = fake_dat,\n  formula = bf,\n  family = weibull(),\n  prior = priors,\n  sample_prior = \"only\",\n  iter = 2000,\n  file = \"fits/b07.04.priors.rds\"\n)\n\n# Figure out what brms has named everything so we can wrangle the draws\n# get_variables(fit.prior)\n    \n# Check the prior predictive distribution\nprior_preds <- fake_dat |>\n\n  add_predicted_draws(fit.prior, ndraws = 500) |>\n\n  group_by(rowid) |>\n\n  mutate(prior_pred_mean = mean(.prediction)) \n\nprior_preds |>\n\n  ggplot() + \n  geom_density(aes(x = .prediction, group = .draw), colour=alpha(clrs$pink, .25)) +\n  scale_y_continuous(expand=c(0, 0)) +\n  xlim(c(1, 100)) +\n  ylim(c(0, .08)) +\n  theme_nice() +\n  theme_posterior_densities() +\n  xlab(\"Days to employment\") +\n  ylab(\"Density\")\n\n\n\n\n\nDensities for 500 draws from the prior predictive distribution of event times. X-axis truncated for visual clarity.\n\n\n\n\nWe can also check the distribution of summary statistics under the prior predictive distribution, like the mean time to event within each draw. This is something Gelman et al. (2020) advocate in their Bayesian Workflow paper. This reveals an area where our priors are lacking: while the mode of the means is well within our desired range of values, there is a very long tail. This is because the Weibull distribution preserves a long tail of event times, and we haven’t told it that event times like 400,000 days are strictly out of the question. The model assigns very low probability to these types of numbers, but they still pull up the mean event time. We will address this in the next iteration of the model. For now let’s stick with the plan and see if the model is able to recover the true parameter estimates and a reasonable posterior predictive distribution despite these imperfect priors.\n\n\nCode\nfake_dat |>\n \n  add_predicted_draws(fit.prior, ndraws = 500) |>\n\n  group_by(.draw) |>\n\n  mutate(draw_mean = mean(.prediction)) |>\n\n  distinct(.draw, .keep_all = TRUE) |>\n\n  ggplot() + \n  geom_histogram(aes(x = draw_mean), colour = \"white\", fill = clrs$pink) +\n  labs(x = \"Mean days-to-employment\", y = \"n draws\") +\n  scale_x_continuous(expand=c(0, 0), limits = c(0, 10000)) +\n  scale_y_continuous(expand=c(0, 0)) +\n  theme_minimal()\n\n\n\n\n\nMean event times of 500 draws from the prior predictive distribution of event times. Many of the prior draws suggest mean event times that we know to be absurd.\n\n\n\n\n\n\n5.6.3 Fitting the Stan Model\nNow we’re ready to fit the survival model using the latent factors as predictors.\ndata {\n  // Factor Analysis\n  int N; // sample size for both models\n  int P; // number of variables in the factor analysis\n  int D; // number of factors\n  array[N] vector[P] X_factor; // data matrix for factor analysis\n  int n_lam; // how many factor loadings are estimated\n  \n  // Weibull Model\n  vector[N] Y; // response variable for Weibull model\n  array[N] int<lower=-1, upper=2> cens; // indicates censoring for Weibull model\n  int K; // number of predictors in Weibull model, now it should be D + 2 + intercept\n  matrix[N, K] X_weibull; // population-level design matrix for Weibull model\n}\nparameters {\n  matrix[N, D] FS_UNC; // factor scores\n  cholesky_factor_corr[D] L_corr_d; // cholesky correlation between factors\n  cholesky_factor_corr[2] L_corr_m1_m4; // cholesky correlation between the m1 and m4\n  vector<lower=0>[P] sd_p; // residual sd for each variable in factor analysis\n  vector<lower=-30, upper=30>[n_lam] lam_UNC; // A bunch of lightly-constrained factor loadings\n  \n  real<lower=0> shape; // shape parameter for Weibull model\n  vector[K] b; // regression coefficients for Weibull model\n}\ntransformed parameters {\n  // Factor Analysis transformed parameters\n  vector[D] M = rep_vector(0, D); // factor means\n  vector<lower=0>[D] Sd_d = rep_vector(1, D); // factor SDs\n  array[N] vector[P] mu_UNC; // A vector to hold the linear predictors for the manifest variables\n\n  for (i in 1 : N) {\n    mu_UNC[i, 1] = lam_UNC[1] * FS_UNC[i, 1];\n    mu_UNC[i, 2] = lam_UNC[2] * FS_UNC[i, 1];\n    mu_UNC[i, 3] = lam_UNC[3] * FS_UNC[i, 1];\n    mu_UNC[i, 4] = lam_UNC[4] * FS_UNC[i, 2];\n    mu_UNC[i, 5] = lam_UNC[5] * FS_UNC[i, 2];\n    mu_UNC[i, 6] = lam_UNC[6] * FS_UNC[i, 2];\n  }\n  \n  cholesky_factor_cov[D] L_Sigma = diag_pre_multiply(Sd_d, L_corr_d); // var-covar matrix for the factors\n  cholesky_factor_cov[D] L_Sigma_m1_m4 = diag_pre_multiply(sd_p[{1, 4}], L_corr_m1_m4); // var-covar matrix of m1 and m4\n  \n  // Weibull transformed parameters\n  vector[N] mu_weibull = rep_vector(0.0, N); // initialize linear predictor term for Weibull model\n  mu_weibull += X_weibull * b; // use both factor scores and observed predictors as predictors\n}\nmodel {\n  // Factor Analysis\n  L_corr_d ~ lkj_corr_cholesky(1);\n  L_corr_m1_m4 ~ lkj_corr_cholesky(1);\n  lam_UNC ~ normal(0, 10);\n  sd_p ~ cauchy(0, 2.5);\n  \n  for (i in 1 : N) {\n    for (j in {2,3,5,6}) {\n      X_factor[i, j] ~ normal(mu_UNC[i, j], sd_p[j]);\n    }\n    to_vector({X_factor[i, 1], X_factor[i, 4]}) ~ multi_normal_cholesky([mu_UNC[i, 1], mu_UNC[i, 4]]', L_Sigma_m1_m4);\n    FS_UNC[i] ~ multi_normal_cholesky(M, L_Sigma);\n  }\n\n  // Weibull Survival Analysis\n  shape ~ cauchy(0, 2.5);\n  b[1] ~ normal(3.3, 0.4);\n  for (k in 2:K) {\n    b[k] ~ normal(0, 1.45);\n  }\n  for (n in 1 : N) {\n    if (cens[n] == 0) {\n      target += weibull_lpdf(Y[n] | shape, exp(mu_weibull[n]) / tgamma(1 + 1 / shape));\n    } else if (cens[n] == 1) {\n      target += weibull_lccdf(Y[n] | shape, exp(mu_weibull[n]) / tgamma(1 + 1 / shape));\n    } \n  }\n}\ngenerated quantities {\n  corr_matrix[D] Rho_UNC; // correlation matrix for factors\n  corr_matrix[D] Rho_factors; // correlation matrix for factors\n  corr_matrix[D] Rho_m1_m4; // correlation matrix for m1 and m4\n  matrix[P, D] lam; // factor loadings\n  matrix[N, D] FS; // factor scores\n  \n  Rho_UNC = multiply_lower_tri_self_transpose(L_corr_d);\n  Rho_factors = Rho_UNC;\n  Rho_m1_m4 = multiply_lower_tri_self_transpose(L_corr_m1_m4);\n  FS = FS_UNC;\n  lam = rep_matrix(0, P, D);\n  lam[1 : 3, 1] = to_vector(lam_UNC[1 : 3]);\n  lam[4 : 6, 2] = to_vector(lam_UNC[4 : 6]);\n  \n  if (lam_UNC[1] < 0) {\n    lam[1 : 3, 1] = to_vector(-1 * lam_UNC[1 : 3]);\n    FS[ : , 1] = to_vector(-1 * FS_UNC[ : , 1]);\n    if (lam_UNC[4] > 0) {\n      Rho_factors[1, 2] = -1 * Rho_UNC[1, 2];\n      Rho_factors[2, 1] = -1 * Rho_UNC[2, 1];\n    }\n  }\n  if (lam_UNC[4] < 0) {\n    lam[4 : 6, 2] = to_vector(-1 * lam_UNC[4 : 6]);\n    FS[ : , 2] = to_vector(-1 * FS_UNC[ : , 2]);\n    if (lam_UNC[1] > 0) {\n      Rho_factors[2, 1] = -1 * Rho_UNC[2, 1];\n      Rho_factors[1, 2] = -1 * Rho_UNC[1, 2];\n    }\n  }\n\n  // Draws from posterior predictive distribution\n  vector[N] y_pred;\n  for (n in 1:N) {\n    y_pred[n] = weibull_rng(shape, exp(mu_weibull[n]) / tgamma(1 + 1 / shape));\n  }\n}\nNow compile the model and run. I’ve worked closely from the Stan code for a Weibull model generated by brms, essentially just grafting it onto our MTMM factor analysis Stan code from before. This only involved a few steps:\n\nData block: add new vectors for the response variable Y and predictor variables K of the Weibull model, and for censoring status cens. Also declare a design matrix X_weibull of the necessary dimensions;\nParameters block: add a vector of coefficients b for the beta coefficients of the Weibull model;\nTransformed Parameters block: declare the constant shape parameter for the Weibull model, as well as the linear predictor mu for each observation, then exponentiate it;\nModel block: add the priors we simulated above, as well as the likelihood function conditional on censoring status;\nGenerated Quantities block: generate draws from the posterior predictive distribution, to help with model checking.\n\nNote that we need to feed an extra column of 1s into the design matrix X_weibull to accound for the intercept.\n\n\nCode\n# Write the raw Stan code from an R string to a Stan file\nstan_file <- cmdstanr::write_stan_file(stan_model_code)\n\n# Autoformat the model syntax to preempt any warnings at compile time\nmodel <- cmdstanr::cmdstan_model(stan_file, compile = FALSE)\nmodel$format(canonicalize = TRUE)\n\n# Compile the model from the Stan file\nmodel <- cmdstanr::cmdstan_model(stan_file)\n\n# Put the data into a list format Stan can understand\nP <- 8 # Because 6 measured variables + 2 factors \nN <- nrow(fake_dat)\nD <- 2\nX_factor <- as.matrix(fake_dat |> select(matches(\"^m[1-9]$|^adapt$|^collab$\")))\n\nY <- fake_dat$time\ncens <- fake_dat$censored\nX_weibull <- as.matrix(\n  fake_dat |> \n    mutate(intercept = 1) |>\n    select(\n      intercept,\n      adapt,\n      collab,\n      age,\n      ei_status\n    ) \n)\n\ndata_list <- list(\n  N=N,\n  P=P,\n  D=D,\n  X_factor=X_factor, \n  n_lam=6,\n  Y=Y,\n  K=5, # four predictors plus a dummy column of 1s for the intercept\n  cens=cens,\n  X_weibull=X_weibull\n)\n\n# Fit the model\nfit <- model$sample(\n  data = data_list,\n  seed = 199,\n  chains = 4,\n  parallel_chains = 4,\n  iter_warmup = 1000,\n  iter_sampling = 6000,\n  output_dir = \"fits\",\n  refresh = 1 # print update at each iter\n)\n\n# Save the resulting model fit\nsaveRDS(fit, \"fits/b07.04.rds\")\n\n\n\n\n5.6.4 Posterior Checks\n\n\nCode\nfit <- readRDS(\"fits/b07.04.rds\")\n\n# If you need to check variable names, call this and inspect the resulting object\n#summary <- fit$summary()\n\n# Load the MCMC draws from the variables we care about\ndraws <- fit$draws(\n  variables = c(\n  \"lam[1,1]\",\n  \"lam[2,1]\",\n  \"lam[3,1]\",\n  \"lam[4,2]\",\n  \"lam[5,2]\",\n  \"lam[6,2]\",\n  \"Rho_factors[2,1]\", \n  \"Rho_m1_m4[2,1]\",\n  \"b\",\n  \"shape\"\n  )\n)\n\n# Save the result\nsaveRDS(draws, \"fits/b07.04.samples.rds\")\n\n\nIt looks like the model did a good job capturing the true time-to-event parameter estimates in the form of acceleration factors, as well as the CFA model parameters from before.\n\n\nCode\n# Load the draws saved in the previous block\ndraws <- readRDS(\"fits/b07.04.samples.rds\")\n\n# Plot\ncleaned_draws <- draws |>\n\n  # Get the raw draws for the factor loadings and the beween-factor correlation coefficient\n  gather_draws(\n    `b[1]`,\n    `b[2]`,\n    `b[3]`,\n    `b[4]`,\n    `b[5]`,\n    `lam[1,1]`,\n    `lam[2,1]`,\n    `lam[3,1]`,\n    `lam[4,2]`,\n    `lam[5,2]`,\n    `lam[6,2]`,\n    `Rho_factors[2,1]`,\n    `Rho_m1_m4[2,1]`\n  ) |>\n\n  mutate(.value = ifelse(.variable %in% c(\"b[1]\", \"b[2]\", \"b[3]\", \"b[4]\", \"b[5]\"), exp(.value), .value)) |>\n  \n  # Do some renaming for clarity\n  mutate(.variable = case_when(\n   .variable == \"b[1]\" ~ \"Intercept\",\n   .variable == \"b[2]\" ~ \"b_Adapt\",\n   .variable == \"b[3]\" ~ \"b_Collab\",\n   .variable == \"b[4]\" ~ \"b_Age\",\n   .variable == \"b[5]\" ~ \"b_EI Status\",\n   .variable == \"lam[1,1]\" ~ \"Loading for m1\",\n   .variable == \"lam[2,1]\" ~ \"Loading for m2\",\n   .variable == \"lam[3,1]\" ~ \"Loading for m3\",\n   .variable == \"lam[4,2]\" ~ \"Loading for m4\",\n   .variable == \"lam[5,2]\" ~ \"Loading for m5\",\n   .variable == \"lam[6,2]\" ~ \"Loading for m6\",\n   .variable == \"Rho_factors[2,1]\" ~ \"Covariance for f1 ~ f2\",\n   .variable == \"Rho_m1_m4[2,1]\" ~ \"Covariance for m1 ~ m4\"\n )) |>\n\n  filter(.variable != \"Intercept\") |>\n\n  # Add the true factor loadings for plotting\n  mutate(true_loading = case_when(\n    .variable == \"b_Age\" ~ 1.11,\n    .variable == \"b_Collab\" ~ 0.833,\n    .variable == \"b_Adapt\" ~ 1,\n    .variable == \"b_EI Status\" ~ 0.5,\n    .variable == \"Loading for m1\" ~ .8,\n    .variable == \"Loading for m2\" ~ .6,\n    .variable == \"Loading for m3\" ~ .9,\n    .variable == \"Loading for m4\" ~ .1,\n    .variable == \"Loading for m5\" ~ .2,\n    .variable == \"Loading for m6\" ~ -.4,\n    .variable == \"Covariance for f1 ~ f2\" ~ .4,\n    .variable == \"Covariance for m1 ~ m4\" ~ .4\n  )) \n\n# Factor loadings\ncleaned_draws |>\n\n  filter(str_detect(.variable, \"Loading\")) |>\n\n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(fill = clrs$purple) +\n  geom_vline(aes(xintercept = true_loading), linetype = 2) + \n  scale_x_continuous(expand = c(0, 0.015)) +\n  scale_y_continuous(expand = c(0, 0.015)) +\n  guides(fill = \"none\") +\n  theme_nice() +\n  theme_posterior_densities() +\n  labs(\n    x = \"Estimate\",\n    y = \"Density\"\n  )  +\n  facet_wrap(~.variable, scales = \"free_x\") \n\n\n\n\n\nCode\n# Latent covariance terms\ncleaned_draws |>\n\n  filter(str_detect(.variable, \"Covariance\")) |>\n\n  ggplot(aes(x = .value)) +\n  stat_halfeye(aes(fill = .variable)) +\n  geom_vline(aes(xintercept = true_loading), linetype = 2) + \n  scale_fill_manual(values = c(clrs$red, clrs$yellow)) +\n  scale_x_continuous(expand = c(0, 0.015)) +\n  scale_y_continuous(expand = c(0, 0.015)) +\n  guides(fill = \"none\") +\n  theme_nice() +\n  theme_posterior_densities() +\n  labs(\n    x = \"Estimate\",\n    y = \"Density\"\n  )  +\n  facet_wrap(~.variable, scales = \"free_x\") \n\n\n\n\n\nCode\n# Latent covariance terms\ncleaned_draws |>\n\n  filter(str_detect(.variable, \"b_\")) |>\n\n  ggplot(aes(x = .value)) +\n  stat_halfeye(fill = clrs$blue) +\n  geom_vline(aes(xintercept = true_loading), linetype = 2) + \n  scale_x_continuous(expand = c(0, 0.015)) +\n  scale_y_continuous(expand = c(0, 0.015)) +\n  guides(fill = \"none\") +\n  theme_nice() +\n  theme_posterior_densities() +\n  labs(\n    x = \"Estimate\",\n    y = \"Density\"\n  )  +\n  facet_wrap(~.variable, scales = \"free_x\") \n\n\n\n\n\nThe MCMC diagnostics suggest the sampling went smoothly:\n\n\nCode\n# Precompute the summary, to save time at quarto render\nsummary_draws <- draws |> summary() \nsaveRDS(summary_draws, \"fits/b07.04.summary.rds\")\n\n\n\n\nCode\nsummary_draws <- readRDS(\"fits/b07.04.summary.rds\")\n\nsummary_draws |> \n\n  select(variable, rhat, ess_bulk, ess_tail) |>\n\n  mutate(across(where(is.numeric), ~round(.x, 2))) |>\n\n  knitr::kable()\n\n\n\n\n\nvariable\nrhat\ness_bulk\ness_tail\n\n\n\n\nlam[1,1]\n1.00\n4916.86\n10233.60\n\n\nlam[2,1]\n1.00\n11105.72\n15670.87\n\n\nlam[3,1]\n1.00\n5311.07\n9951.95\n\n\nlam[4,2]\n1.00\n2905.07\n7002.21\n\n\nlam[5,2]\n1.00\n2870.14\n5853.40\n\n\nlam[6,2]\n1.00\n640.57\n685.89\n\n\nRho_factors[2,1]\n1.01\n523.47\n801.13\n\n\nRho_m1_m4[2,1]\n1.00\n9496.22\n15220.41\n\n\nb[1]\n1.00\n19221.24\n18004.81\n\n\nb[2]\n1.00\n31222.92\n18877.40\n\n\nb[3]\n1.00\n27175.92\n18515.76\n\n\nb[4]\n1.00\n33494.37\n17809.97\n\n\nb[5]\n1.00\n22349.27\n18851.47\n\n\nshape\n1.00\n28256.96\n17710.42\n\n\n\n\n\nHow does this compare to the traditional two-stage approach of fitting the time-to-event model to factor score point estimates? Both models do well, which is not surprising because we know them to both be specified according to the true data generating process. But the advantage of the Bayesian approach is shown in the coefficient for collab below. This is the factor with looser loadings and poor convergent valiity, which has resulted in greater uncertainty in the frequentist lavaan model’s estimated factor loadings and the ensuing factor score point estimates. However, the Bayesian model was able to incorporate this uncertainty into an estimated posterior distribution for each factor score, allowing it to take that uncertainty into consideration when estimating the regression parameter estimates. This gives the Bayesian model more precision in its estimate of b_collab.\n\n\nCode\n# get the factor scores from the lavaan model we fit above\nfactor_scores <- predict(fit_test)\n\n# add the factor scores to fake_dat as predictors\ndat_factor_scores <- fake_dat |>\n\n  cbind(factor_scores) |>\n\n  rename(\"adapt_score\" = f1, \"collab_score\" = f2) \n\n# Fit the 2-stage Weibull AFT regression\nfactor.score.model <- survreg(surv_object ~ adapt_score + collab_score + age + ei_status, data=dat_factor_scores, dist='weibull')\n\n# Rename some parts of the Stan model output to align with the frequentist output.\ncleaned_draws_dplt <- cleaned_draws |>\n\n  mutate(.variable = case_when(\n    .variable == \"b_Collab\" ~ \"collab_score\",\n    .variable == \"b_Adapt\" ~ \"adapt_score\",\n    .variable == \"b_Age\" ~ \"age\",\n    .variable == \"b_EI Status\" ~ \"ei_status\"\n  ))\n\n# Wrangle and plot\nfactor.score.model |> \n  \n  broom::tidy(conf.int = TRUE) |>\n\n  mutate(\n    estimate = exp(estimate),\n    conf.low = exp(conf.low),\n    conf.high = exp(conf.high)\n  ) |>\n\n  filter(!term %in% c(\"(Intercept)\", \"Log(scale)\")) |>\n\n  mutate(true_loading = case_when(\n    term == \"age\" ~ 1.11,\n    term == \"collab_score\" ~ 0.833,\n    term == \"adapt_score\" ~ 1,\n    term == \"ei_status\" ~ 0.5\n  )) |>\n\n  ggplot() +\n    geom_point(aes(x = reorder(term, -estimate), y = estimate)) +\n    geom_linerange(aes(x = reorder(term, -estimate), y = estimate, ymin = conf.low, ymax = conf.high), size = 4, colour = \"#878787\") +\n    geom_point(aes(x = reorder(term, -estimate), y = estimate), size = 7, colour = \"#878787\") +\n    geom_point(aes(x = reorder(term, -estimate), y = estimate), size = 5, color = \"white\") +\n    geom_point(aes(x = term, y = true_loading), colour = \"red4\", size = 7, shape =  13, stroke = 1) +\n    stat_halfeye(data = cleaned_draws_dplt , aes(x = .variable, y = .value), fill = clrs$blue, alpha = .8) +\n    coord_flip() + \n    labs(\n      y = \"Estimated survival time ratio\",\n      x = \"\"\n    ) +\n    theme_minimal() \n\n\n\n\n\nA Bayesian win: posterior estimates for regression coefficients of latent predictors (blue densities) are more precise than estimates from the traditional two-stage approach using factor score point estimates (grey point-ranges).\n\n\n\n\nWe can also do more conventional checks of parametric survival models recommended by Collett (2003), such as plotting the estimated baseline hazard function for various covariate combinations, and overlaying the estimated survival function against the Kaplan-Meier estimates. Let’s start by visualizing posterior draws of the survival lfunction, and comparing it to the true simulation survival function.\nThe parameter estimates all look good individually, but we’ll need to sample from the posterior predictive distribution to get a sense of whether our model is really useful. When we sampled from the prior predictive distribution we saw that the model thought unrealistic long-tail event times like 400,000 days were possible. If the model still believes those things we’ll need to refine our code to give our model more background information, either in the form of priors or by changes to its overall structure.\nStart by retreiving the samples from the posterior predictive distribution we defined at the bottom of the Stan model’s generated quantities{} block:\n\n\nCode\n# Get the draws from the posterior predictive distribution\npp_draws <- fit$draws(variables = c(\"y_pred\"))\n\n# Save the result\nsaveRDS(pp_draws, \"fits/b07.04.post_pred_samples.rds\")\n\n\nLoad the samples from the cache:\n\n\nCode\n# Load the posterior preds from the cache\npp_draws <- readRDS(\"fits/b07.04.post_pred_samples.rds\")\n\n# Summarize the draws into points\npp_summaries <- pp_draws |> summary()\n\n# Store the summary object again to save time at quarto render\nsaveRDS(pp_summaries, \"fits/b07.04.post_pred_summaries.rds\")\n\n\nThe mean posterior predictions don’t suffer from the same degree of long tail pathology the prior predictions did:\n\n\nCode\npp_summaries <- readRDS(\"fits/b07.04.post_pred_summaries.rds\")\n\n# Calculate the true mean time in the dataset\nmean_time <- mean(fake_dat$time)\n\n# Plot the posterior predicted means\npp_summaries |> select(\"time_pred\" = mean)|>\n\n  pivot_longer(cols = everything(), names_to = \"name\", values_to = \"time\") |>\n\n  mutate(name = ifelse(name == \"time_dat\", \"Data\", \"Mean posterior predictions\")) |>\n\n  ggplot() +\n  geom_histogram(aes(x = time, fill = name, alpha = name), binwidth = 5, colour = \"white\", fill = clrs$blue, alpha = 1, position = \"identity\") +\n  geom_vline(aes(xintercept = mean_time), linetype = 2, size = 1.5) +\n  scale_alpha_manual(values=c(1, .7)) +\n  scale_x_continuous(limits = c(0, 200), expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  theme_nice()  +\n  labs(\n    x = 'Days since graduation', \n    y = \"N Posterior Draws\"\n  )\n\n\n\n\n\nMeans of draws from the posterior predictive distribution. These are more realistic than the means from the prior predictive distribution we plotted above.\n\n\n\n\nWe can get a more detailed look at the model’s uncertainty by plotting the densitities of draws from the posterior distribution against the distribution of the original data. This shows that the posterior predictive distribution of event times skews longer the distribution in the dataset:\n\n\nCode\ndplt <- pp_draws |>\n  \n  gather_draws(\n    y_pred[i],\n    ndraws = 10\n  ) |>\n\n  ungroup() |>\n  \n  select(i, .draw, .value) \n\nsaveRDS(dplt, \"fits/b07.04.post_pred_20_draws.rds\")\n\n\n\n\nCode\ndplt <- readRDS(\"fits/b07.04.post_pred_20_draws.rds\")\n\ndplt |>\n  \n  ggplot() +\n  geom_density(aes(x = .value, group = .draw, color = 'Posterior draws')) +\n  geom_density(data = fake_dat |> filter(censored == 0), aes(x = time, color = 'Data')) +\n  scale_color_manual(\n    name='',\n    breaks=c('Posterior draws', 'Data'),\n    values=c('Posterior draws' = alpha(clrs$blue, .5), 'Data' = 'black')\n  ) +\n  scale_x_continuous(limits = c(0, 365), expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  labs(x = \"Days to employment\", y = \"Density\") +\n  theme_nice() +\n  theme_posterior_densities() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nDensities of 20 draws from the posterior predictive distribution (blue) against the distribution of the data (black). The model expects slower times-to-employment overall than the distribution in the data.\n\n\n\n\nTo a certain extent this is expected: we have lots of censored data, and that creates uncertainty about how long the longest event times could be. A quick investigation shows that the longest time-to-employment in the 20 posterior draws plotted above is 850 days. This seems long compared to the observed data, but it is not impossible given the domain of research – sometimes people leaving a skills training program do hold off on getting a new job for a few years for various reasons. So the posterior expectations are not outlandish and impossible, like the prior expectations were.Still, if we think the tail of the posterior predictive distribution is too long then we have several ways of giving that information to the model. We’ll implement one of those ways in the next section when we expand to a multilevel model."
  },
  {
    "objectID": "regression-with-latent-predictors.html#bayesian-multilevel-survival-analysis-with-latent-predictors",
    "href": "regression-with-latent-predictors.html#bayesian-multilevel-survival-analysis-with-latent-predictors",
    "title": "5  Regression With Latent Predictors",
    "section": "5.7 Bayesian Multilevel Survival Analysis with Latent Predictors",
    "text": "5.7 Bayesian Multilevel Survival Analysis with Latent Predictors\nThe Stan model in the previous section is still missing some key information: our program was delivered to ~70 cohorts of various sizes, each administered by a different non-profit or community college. This means our data has nested structure that we should tell the model about via a multilevel model. In short, we just tell the model that cohort-specific regression coefficients are drawn from a shared multivariate normal distribution, and we ask the model to estimate the mean vector and covariance matrix of that distribution. This allows the model to partially pool information across clusters insofar as it deems such pooling appropriate given its estimates of the between-cohort variance an covariance parameters: if the model estimates little variance between clusters then it will pool more, but if it estimates high variance then it will pool less. This biases the cluster-specific estimates by shrinking them towards the estimated ‘grand mean’, which is often a good thing because more bias means less variance. We only vary the substantive predictors, not the parameters of the CFA portion of the model, IE the factor loadings. Here’s the new model definition:\n\\[\n\\begin{aligned}\n\\color{#3c4b99}T_i &\\color{#3c4b99}\\sim \\text{Weibull}(\\theta_{i,j}, k) \\\\\n\\color{#3c4b99}\\theta_{i,j} &\\color{#3c4b99}= \\alpha_j + \\beta_{\\text{age}_j} \\text{age}_i + \\beta_{\\text{ei}_j} \\text{ei\\_status}_i + \\beta_{\\text{adapt}_j} \\text{adapt}_i + \\beta_{\\text{collab}_j} \\text{collab}_i \\\\\n\\\\\n\\begin{bmatrix}\n\\alpha \\\\\n\\beta_{\\text{age}_j} \\\\\n\\beta_{\\text{ei}_j} \\\\\n\\beta_{\\text{adapt}_j} \\\\\n\\beta_{\\text{collab}_j}\n\\end{bmatrix}\n&\\sim \\text{MVNormal}\n\\left(\n\\begin{bmatrix}\n\\mu_{\\alpha} \\\\\n\\mu_{\\text{age}} \\\\\n\\mu_{\\text{ei}} \\\\\n\\mu_{\\text{adapt}} \\\\\n\\mu_{\\text{collab}}\n\\end{bmatrix},\n\\Sigma_{\\beta}\n\\right) \\\\\n\\\\\n\\begin{bmatrix}\nm_{1} \\\\\nm_{2} \\\\\nm_{3} \\\\\nm_{4} \\\\\nm_{5} \\\\\nm_{6}\n\\end{bmatrix}\n&\\sim \\text{MVNormal}\n\\left(\n\\begin{bmatrix}\n\\mu_{m1} \\\\\n\\mu_{m2} \\\\\n\\mu_{m3} \\\\\n\\mu_{m4} \\\\\n\\mu_{m5} \\\\\n\\mu_{m6}\n\\end{bmatrix},\n\\Sigma_m\n\\right) \\\\\n\\\\\n\\mu_{m1} &= \\lambda_1 \\text{adapt}, \\quad \\mu_{m2} = \\lambda_2 \\text{adapt}, \\quad \\mu_{m3} = \\lambda_3 \\text{adapt} \\\\\n\\mu_{m4} &= \\lambda_4 \\text{collab}, \\quad \\mu_{m5} = \\lambda_5 \\text{collab}, \\quad \\mu_{m6} = \\lambda_6 \\text{collab} \\\\\n\\\\\n\\begin{bmatrix}\n\\text{adapt}_{i} \\\\\n\\text{collab}_{i}\n\\end{bmatrix}\n&\\sim \\text{MVNormal}\n\\left(\n\\begin{bmatrix}\n\\mu_\\text{adapt} \\\\\n\\mu_\\text{collab}\n\\end{bmatrix},\n\\Sigma_f\n\\right) \\\\\n\\\\\n\\Sigma_f &=\n\\begin{bmatrix}\n\\sigma_\\text{adapt}^2 & \\color{#c93f55}{\\rho \\sigma_\\text{adapt} \\sigma_\\text{collab}} \\\\\n\\color{#c93f55}{\\rho \\sigma_\\text{adapt} \\sigma_\\text{collab}} & \\sigma_\\text{adapt}^2\n\\end{bmatrix}\n\\\\\n\\\\\n\\Sigma_m &=\n\\begin{bmatrix}\n\\sigma_{m1}^2 & 0 & 0 & \\color{#eacc62}{\\rho\\ \\sigma_{m1} \\sigma_{m4}} & 0 & 0 \\\\\n0 & \\sigma_{m2}^2 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & \\sigma_{m3}^2 & 0 & 0 & 0 \\\\\n\\color{#eacc62}{\\rho \\sigma_{m1} \\sigma_{m4}} & 0 & 0 & \\sigma_{m4}^2 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & \\sigma_{m5}^2 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & \\sigma_{m6}^2\n\\end{bmatrix}\n\\\\\n\\\\\n\\color{#469d76}{\\Sigma_{\\beta}} &= \\color{#469d76}{\n\\begin{bmatrix}\n\\sigma_{\\alpha}^2 & \\rho_{\\alpha,\\text{age}} \\sigma_{\\alpha} \\sigma_{\\text{age}} & \\rho_{\\alpha,\\text{ei}} \\sigma_{\\alpha} \\sigma_{\\text{ei}} & \\rho_{\\alpha,\\text{adapt}} \\sigma_{\\alpha} \\sigma_{\\text{adapt}} & \\rho_{\\alpha,\\text{collab}} \\sigma_{\\alpha} \\sigma_{\\text{collab}} \\\\\n\\rho_{\\alpha,\\text{age}} \\sigma_{\\alpha} \\sigma_{\\text{age}} & \\sigma_{\\text{age}}^2 & \\rho_{\\text{age},\\text{ei}} \\sigma_{\\text{age}} \\sigma_{\\text{ei}} & \\rho_{\\text{age},\\text{adapt}} \\sigma_{\\text{age}} \\sigma_{\\text{adapt}} & \\rho_{\\text{age},\\text{collab}} \\sigma_{\\text{age}} \\sigma_{\\text{collab}} \\\\\n\\rho_{\\alpha,\\text{ei}} \\sigma_{\\alpha} \\sigma_{\\text{ei}} & \\rho_{\\text{age},\\text{ei}} \\sigma_{\\text{age}} \\sigma_{\\text{ei}} & \\sigma_{\\text{ei}}^2 & \\rho_{\\text{ei},\\text{adapt}} \\sigma_{\\text{ei}} \\sigma_{\\text{adapt}} & \\rho_{\\text{ei},\\text{collab}} \\sigma_{\\text{ei}} \\sigma_{\\text{collab}} \\\\\n\\rho_{\\alpha,\\text{adapt}} \\sigma_{\\alpha} \\sigma_{\\text{adapt}} & \\rho_{\\text{age},\\text{adapt}} \\sigma_{\\text{age}} \\sigma_{\\text{adapt}} & \\rho_{\\text{ei},\\text{adapt}} \\sigma_{\\text{ei}} \\sigma_{\\text{adapt}} & \\sigma_{\\text{adapt}}^2 & \\rho_{\\text{adapt},\\text{collab}} \\sigma_{\\text{adapt}} \\sigma_{\\text{collab}} \\\\\n\\rho_{\\alpha,\\text{collab}} \\sigma_{\\alpha} \\sigma_{\\text{collab}} & \\rho_{\\text{age},\\text{collab}} \\sigma_{\\text{age}} \\sigma_{\\text{collab}} & \\rho_{\\text{ei},\\text{collab}} \\sigma_{\\text{ei}} \\sigma_{\\text{collab}} & \\rho_{\\text{adapt},\\text{collab}} \\sigma_{\\text{adapt}} \\sigma_{\\text{collab}} & \\sigma_{\\text{collab}}^2 \\\\\n\\end{bmatrix}\n} \\\\\n\\end{aligned}\n\\]\n\n5.7.1 Data simulation and validation\nWe’ll need to simulate a new time-to-event dataset to reflect the fact that each cluster has its own parameter estimates drawn from a shared distribution. No changes are needed to the latent variables, so we can reuse our fake_dat_measurement from the previous example. But we’ll need to change the way we simulated the event times such that each cluster has its own true parameter value for each of the regression coefficients of interest. We can use a slightly-modified version of the simulation approach provided by Kurz (2023) in replicating the robot cafés example from McElreath (2020).\nWe can reuse the parameter estimates from our previous model as the means of the shared distribution from which the cluster-specific parameter values are drawn. When setting the rhos, it seems reasonable to imagine that perhaps clusters with high baseline hazard (given by high values of lambda) are located in areas with hotter labour markets in which employers are less choosy in their hiring, making it so that the other predictors will matter less in determining time-to-employment. So we can set those rhos to negative correlation between lambda and the substantive predictors.\n\n\nCode\nset.seed(3)\n\n# Average lambda parameter of Weibull baseline hazard\navg_lambda <- .02\nrho <- 1 # keep rho = 1 to keep the conversions simple / make it easier to specify lambda\navg_lambda_wiki <- avg_lambda^(-1 / rho) \n\n# Average effects for the substantive predictors\navg_beta_adapt <- log(1) # adaptability skills have no effect on time-to-employment\navg_beta_collab <- log(1.2) # collaboration skills decrease time-to-employment by 20% for each standard deviation increase.\navg_beta_age <- log(.9) # older people find employment a bit slower -- each increase in age of 1 standard deviation slows time-to-employment to 90% of what is was previously.\navg_beta_ei <- log(2) # people on EI find employment twice as fast \n\n# Standard deviations of lamba and the substantive predictors\nsd_lambda <- log(500)\nsd_beta_adapt <- log(1.1) \nsd_beta_collab <- log(1.3) \nsd_beta_age <- log(2) # the effect of age varies a lot by region\nsd_beta_ei <- log(1.2) # the effect of EI is very consistent\n\n# Pearson correlation coefficients for each pair of regression parameters\nrho_lambda_adapt <- -.3\nrho_lambda_collab <- -.1\nrho_lambda_age <- -.2\nrho_lambda_ei <- -.1\nrho_adapt_collab <- .1\nrho_adapt_age <- .2\nrho_adapt_ei <- .1\nrho_collab_age <- .3\nrho_collab_ei <- 0\nrho_age_ei <- .1\n\n# Calculate the covariances\ncov_lambda_adapt <- sd_lambda * sd_beta_adapt * rho_lambda_adapt\ncov_lambda_collab <- sd_lambda * sd_beta_collab * rho_lambda_collab\ncov_lambda_age <- sd_lambda * sd_beta_age * rho_lambda_age\ncov_lambda_ei <- sd_lambda * sd_beta_ei * rho_lambda_ei\ncov_adapt_collab <- sd_beta_adapt * sd_beta_collab * rho_adapt_collab\ncov_adapt_age <- sd_beta_adapt * sd_beta_age * rho_adapt_age\ncov_adapt_ei <- sd_beta_adapt * sd_beta_ei * rho_adapt_ei\ncov_collab_age <- sd_beta_collab * sd_beta_age * rho_collab_age\ncov_collab_ei <- sd_beta_collab * sd_beta_ei * rho_collab_ei\ncov_age_ei <- sd_beta_age * sd_beta_ei * rho_age_ei\n\n# Bring it all together as a covariance matrix\nsigma <- matrix(c(\n    sd_lambda^2, cov_lambda_adapt, cov_lambda_collab, cov_lambda_age, cov_lambda_ei,\n    cov_lambda_adapt, sd_beta_adapt^2, cov_adapt_collab, cov_adapt_age, cov_adapt_ei,\n    cov_lambda_collab, cov_adapt_collab, sd_beta_collab^2, cov_collab_age, cov_collab_ei,\n    cov_lambda_age, cov_adapt_age, cov_collab_age, sd_beta_age^2, cov_age_ei,\n    cov_lambda_ei, cov_adapt_ei, cov_collab_ei, cov_age_ei, sd_beta_ei^2\n), ncol = 5)\n\n# Declare the rate for the exponential distribution from which we will sample censoring times\ncensor_rate = .005\n\n\nNow to define the cluster-specific true parameter values using the means and covariance matrix we defined above:\n\n\nCode\n# Assign people to clusters\nmax_cluster_size <- N / 50\nmin_cluster_size <- N / 100\n\n# Create a sequence of cluster sizes\ncluster_sizes <- sample(min_cluster_size:max_cluster_size, N, replace = TRUE)\n\nfake_dat_with_clusters <- fake_dat_measurement |>\n\n  mutate(cluster_id = rep(1:length(cluster_sizes), times=cluster_sizes)[1:N])\n\n# Generate the true cluster-specific parameter estimates\nn_clusters <- fake_dat_with_clusters |> distinct(cluster_id) |> nrow()\n\ncluster_params = MASS::mvrnorm(n_clusters, c(avg_lambda_wiki, avg_beta_adapt, avg_beta_collab, avg_beta_age, avg_beta_ei), sigma) |>\n\n  as.data.frame() |>\n\n  rowid_to_column() |>\n\n  rename(\n    \"cluster_id\" = 1,\n    \"lambda_wiki\" = 2,\n    \"beta_adapt\" = 3,\n    \"beta_collab\" = 4,\n    \"beta_age\" = 5,\n    \"beta_ei_status\" = 6\n  )\n\n\nLastly, generate the measured covariates, event times, and censoring statuses:\n\n\nCode\n# Simulate the cluster covariates and event times\nfake_dat <- fake_dat_with_clusters |>\n\n  # Bring in the cluster-specific \n  left_join(cluster_params, by = \"cluster_id\") |>\n\n  mutate(\n    \n    # Simulate the covariates\n    age = rnorm(N), \n    ei_status = rbinom(N, 1, .4)\n    \n  ) |>\n\n  rowid_to_column() |>\n\n  group_by(rowid) |>\n\n  mutate(\n    \n    # Simulate the event times and censoring times using cluster-specific parameter values\n    lambda_prime = lambda_wiki / exp((beta_adapt*adapt + beta_collab*collab + beta_age*age + beta_ei*ei_status) / rho),\n    time_event = rweibull(1, shape=rho, scale=lambda_prime),\n    time_censor = rexp(1, censor_rate),\n    time = min(time_event, time_censor),\n    censored = ifelse(time_censor <= time_event, 1, 0),\n    status = ifelse(time_censor > time_event, 1, 0)\n    \n  ) |>\n\n  ungroup() |>\n\n  # Remove the intermediary variables we don't need\n  dplyr::select(-c(\n    time_event,\n    time_censor,\n    lambda_wiki,\n    lambda_prime\n  ))\n\n\nNow that we have our clustered simulated dataset, we can explore it to make sure it has realistic properties as a whole and within each individual cluster. We can start by checking whether the full distribution of event times across clusters seems realistic:\n\n\nCode\nfake_dat |>\n\n  mutate(censored = as.factor(censored))|>\n\n  ggplot() + \n  geom_histogram(aes(x = time, fill = censored), colour = \"black\", position = \"identity\") +\n  scale_x_continuous(limits = c(3, 600), n.breaks = 10, expand=c(0, 0)) +\n  scale_y_continuous(expand=c(0, 0)) +\n  scale_fill_manual(\n    values = c(\"0\" = alpha(clrs$blue, 1), \"1\" = alpha(clrs$red, 1)),\n    name = NULL,  # Removes legend title\n    labels = c(\"Employed\", \"Censored\")  # Changes legend category labels\n  ) +\n  labs(\n    x = \"Days to Employment\",\n    y = \"N Graduates\"\n  ) +\n  theme_nice() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nHistogram of times-to-event, including employment outcomes and censorings.\n\n\n\n\nThe full distribution looks realistic. Now let’s look at the distributions within each individual cluster. Again, everything seems fine:\n\n\nCode\n# Visualize the selected clusters\nfake_dat |>\n\n  group_by(cluster_id) |>\n  mutate(max_time = max(time)) |>\n  arrange(max_time) |>\n  ungroup() |>\n  mutate(cluster_id = factor(cluster_id, levels = unique(cluster_id))) |>\n\n  ggplot() +\n  geom_jitter(aes(x = time, y = cluster_id, colour = as.character(censored)), height = .1, alpha = .5) +\n  scale_colour_manual(\n    values = c(clrs$blue, clrs$red),\n    name = \"\", \n    labels = c(\"Employed\", \"Censored\")\n  ) +\n  scale_x_discrete(expand = c(0, 0)) +\n  theme_minimal(base_family = \"Lato\") +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    axis.title = element_text(face = \"bold\"),\n    strip.text = element_text(face = \"bold\"),\n    legend.title = element_text(face = \"bold\"),\n    axis.text.y = element_blank(),\n    legend.position = \"top\"\n  ) +\n  labs(\n    x = \"Days to employment or censoring\",\n    y = \"Clusters\",\n    legend.title = \"haha\"\n  )\n\n\n\n\n\nEmployment and censoring times for all simulated clusters. Each row is a cluster. Each dot is a program graduate.\n\n\n\n\nTo look at the cluster-specific time-to-event properties in greater detail we can even estimate and plot cluster-specific Kaplan-Meier curves. These show realistic variation between clusters:\n\n\nCode\nsurv_object <- Surv(fake_dat$time, fake_dat$status)\n\n# Get the KM curve for reference:\nkm.object <- survfit(surv_object ~ 1 + cluster_id, data = fake_dat)\n\nplt <- survminer::ggsurvplot(\n  fit = km.object,\n  conf.int = FALSE,\n  risk.table = FALSE,\n  legend = \"none\",\n  title = \"\",\n  xlab = \"Days Since Graduation\", \n  ylab = \"Proportion Unemployed\",\n  ggtheme = theme_bw()\n)  \n\nplt$data.survplot |> \n  \n  select(time, surv, cluster_id) |> \n  \n  ggplot() +\n  geom_line(aes(x = time, y = surv, group = cluster_id), alpha = .5, colour = clrs$blue) +\n  theme_nice() +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  labs(\n    x = \"Time\",\n    y = \"Kaplan-Meier Estimate\"\n  )\n\n\n\n\n\nCluster-specific Kaplan-Meier curves for simulated data. There is a lot of variation between clusters.\n\n\n\n\nAs a final check before fitting our updated Stan model, we can get a sense of between-cluster variance in specific parameter estimates by fitting a time-to-event model to each cluster in isolation and plotting the cluster-specific parameter estimates in a single plot. Andrew Gelman (2007) call this plot the ‘Secret Weapon’ “because it is so easy and powerful but yet is rarely used as a data-analytic tool.” We’ll fit these cluster-specific no-pooling models using the conventional two-stage factor scores approach, and we’ll focus on the two variables of interest to the client, I.E. the latent variables adapt and collab:\n\n\nCode\n# Declare a lavaan factor model\nsurv.measurement.model <- ' \n  f1 =~ m1 + m2 + m3\n  f2 =~ m4 + m5 + m6\n\n  f1 ~~ f1\n  f2 ~~ f2\n\n  m1 ~~ m4\n'\n\n# Fit the model\nfit_test <- cfa(surv.measurement.model, data = fake_dat_measurement |> dplyr::select(starts_with(\"m\")), std.lv = TRUE)\n\n# get the factor scores from the lavaan model we fit above\nfactor_scores <- lavaan::predict(fit_test)\n\n# add the factor scores to fake_dat as predictors\nfake_dat <- fake_dat |>\n\n  cbind(factor_scores) |>\n\n  rename(\"adapt_score\" = f1, \"collab_score\" = f2) \n\n# Split data by cluster\nsplit_data <- fake_dat |>\n  group_by(cluster_id) |>\n  group_split()\n\n# Fit the model to each subset\nmodel_list <- map(split_data, function(data) {\n  surv_obj <- Surv(data$time, data$status)\n  survreg(surv_obj ~ 1 + adapt_score + collab_score + age + ei_status, data = data, dist = 'weibull')\n})\n\n# Extract and clean the cluster-specific parameter estimates\nmodel_summaries <- map(model_list, broom::tidy, conf.int = TRUE, exp = FALSE)\n\n# Add the cluster IDs, exponentiate the results\ntidy_results <- map2_df(model_summaries, unique(fake_dat$cluster_id), \n                       function(model, cluster) {\n                         model <- model |>\n                           mutate(\n                            cluster_id = cluster,\n                            estimate = exp(estimate),\n                            std.error = exp(std.error),\n                            conf.low = exp(conf.low),\n                            conf.high = exp(conf.high)\n                           )\n                         return(model)\n                       })\n\ndplt_secret_weapon <- tidy_results |>\n  \n  left_join(\n    \n    fake_dat |> \n      \n      group_by(cluster_id) |> \n      \n      count()\n    \n  ) |>\n\n  # Only keep the variables we need\n  filter(term %in% c(\"adapt_score\", \"collab_score\")) |>\n\n  # add true mean effects\n  mutate(\n    true_mean_effect = case_when(\n      term == \"age\" ~ 1.11,\n      term == \"collab_score\" ~ 0.833,\n      term == \"adapt_score\" ~ 1,\n      term == \"ei_status\" ~ 0.5\n    )) |> \n  \n  mutate(cluster_id = factor(cluster_id, levels = unique(cluster_id[order(n)])))\n\ndplt_secret_weapon |> \n\n  mutate(term = case_when(\n    term == \"collab_score\" ~ \"Collab\",\n    term == \"adapt_score\" ~ \"Adapt\"\n  )) |>\n\n  ggplot(aes(x = factor(cluster_id), y = estimate)) +\n  geom_hline(aes(yintercept = true_mean_effect), linetype = 2) +\n  geom_point(position = position_dodge(width = 0.5), size = 3, colour = \"#767676\") +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high),\n                position = position_dodge(width = 0.5), width = 0.2, colour = \"#767676\") +\n  facet_wrap(~ term, scales = c(\"free\"), ncol = 1) +\n  scale_x_discrete(\n    breaks = NULL, \n    labels = NULL, \n    name = \"<---- Smaller Clusters                                           Bigger Clusters ---->\"  # Custom x-axis title\n  ) +\n  ylab(\"Survival Time Ratio\") +\n  theme_nice() +\n  theme(\n    legend.position = \"bottom\",\n    panel.border = element_blank()\n   # panel.grid = element_blank(),\n  ) +\n  labs(y = \"Survival Time Ratio\") +\n  facet_wrap(~ term, scales = c(\"free\"), ncol = 1) \n\n\n\n\n\nAndrew Gelman’s ‘Secret Weapon’ plot showing variation between cluster-specific parameter estimates for regression coefficients. The dashed line shows the true simulation mean effect.\n\n\n\n\n\n\n5.7.2 Fitting the Stan Model\nNow we can update our model, working closely from a brms-generated Stan model with the desired multilevel structure:\n\nFunctions block: At the top of the model create a new functions{} block, and copy the scale_r_cor function from the brms-generated Stan code;\nData block: add new variables to hold the cluster levels and cluster-specific parameters;\nParameters block: add a matrix z_1 for the standardized cluster-specific parameter estimates (we’ll calculate the unstandardized ones in the transformed parameters block), as well as the elements to construct the covariance matrix of the varying effects;\nTransformed parameters block: add a matrix r_1 for the unstandardized cluster-specific parameter estimates, and then calculate them using our custom function from the functions block. Also define some of the priors. We need to fix up the positive-constrained prior distributions for the Weibull shape parameter and the standard deviations of the varying effects, which is why for each we subtract the log of the complement of the cumulative density function from itself. This has the effect of standardizing the area under the positivity-constrained portion of the curve so that it integrates to 1. I found this explanation very clear and helpful for understanding this;\nModel block: expand the linear predictor for the Weibull likelihood to inclulde the cluster-specific varying effects defined in the previous blocks;\nGenerated Quantities block:: De-Cholesky the estimated correlation matrix, and extract one of the diagonals. Also add a new approach to sampling from the posterior predictive distribution, using rejection sampling to deal with the overdispersion we encountered in the previous moddel, as documented in Eren M. Elçi’s vignette on Bayesian survival analysis in Stan). This isn’t really rejection sampling in the traditional sense of sampling from a distribution \\(c*Q(x)\\) known to be greater than \\(P(x)\\) at all values of x. Rather, we’re just sampling from a conditional, truncated version of the posterior predictive distribution.\n\nNo need to do prior predictive checking here: we can reuse our priors from the previous model for the means of the varying effects distributions, and use default non-informative priors for the varying effects standard deviations and correlation terms.\nfunctions {\n  /* compute correlated group-level effects\n   * Args:\n   *   z: matrix of unscaled group-level effects\n   *   SD: vector of standard deviation parameters\n   *   L: cholesky factor correlation matrix\n   * Returns:\n   *   matrix of scaled group-level effects\n   */\n  matrix scale_r_cor(matrix z, vector SD, matrix L) {\n    // r is stored in another dimension order than z\n    return transpose(diag_pre_multiply(SD, L) * z);\n  }\n}\ndata {\n  // Factor Analysis\n  int N; // sample size for both models\n  int P; // number of variables in the factor analysis\n  int D; // number of factors\n  array[N] vector[P] X_factor; // data matrix for factor analysis\n  int n_lam; // how many factor loadings are estimated\n  \n  // Weibull Model\n  vector[N] Y; // response variable for Weibull model\n  array[N] int<lower=-1, upper=2> cens; // indicates censoring for Weibull model\n  int K; // number of predictors in Weibull model, now it should be D + 2 + intercept\n  matrix[N, K] X_weibull; // population-level design matrix for Weibull model\n\n  // Multilevel Structure \n  int<lower=1> N_1; // number of clusters\n  int<lower=1> M_1; // number of cluster-varying predictors\n  array[N] int<lower=1> J_1; // Row-specific cluster-indicator. \n  vector[N] Z_1_1; // Row-specific values of the cluster-level predictors:\n  vector[N] Z_1_2;\n  vector[N] Z_1_3;\n  vector[N] Z_1_4;\n  vector[N] Z_1_5;\n  int<lower=1> NC_1; // number of group-level correlations\n}\nparameters {\n  // Factor Analysis\n  matrix[N, D] FS_UNC; // factor scores\n  cholesky_factor_corr[D] L_corr_d; // cholesky correlation between factors\n  cholesky_factor_corr[2] L_corr_m1_m4; // cholesky correlation between the m1 and m4\n  vector<lower=0>[P] sd_p; // residual sd for each variable in factor analysis\n  vector<lower=-30, upper=30>[n_lam] lam_UNC; // A bunch of lightly-constrained factor loadings\n  \n  // Weibull Model\n  real<lower=0> shape; // shape parameter for Weibull model\n  vector[K] b; // regression coefficients for Weibull model\n\n  // Multilevel Structure\n  vector<lower=0>[M_1] sd_1; // standard deviations of the underlying parameter distributions, to be estimated\n  matrix[M_1, N_1] z_1; // standardized cluster-specific estimates for each regression predictor\n  cholesky_factor_corr[M_1] L_1; // cholesky factor of correlation matrix for varying effects\n}\ntransformed parameters {\n  // Factor Analysis transformed parameters\n  vector[D] M = rep_vector(0, D); // factor means\n  vector<lower=0>[D] Sd_d = rep_vector(1, D); // factor SDs\n  array[N] vector[P] mu_UNC; // A vector to hold the linear predictors for the manifest variables\n\n  for (i in 1 : N) {\n    mu_UNC[i, 1] = lam_UNC[1] * FS_UNC[i, 1];\n    mu_UNC[i, 2] = lam_UNC[2] * FS_UNC[i, 1];\n    mu_UNC[i, 3] = lam_UNC[3] * FS_UNC[i, 1];\n    mu_UNC[i, 4] = lam_UNC[4] * FS_UNC[i, 2];\n    mu_UNC[i, 5] = lam_UNC[5] * FS_UNC[i, 2];\n    mu_UNC[i, 6] = lam_UNC[6] * FS_UNC[i, 2];\n  }\n  \n  cholesky_factor_cov[D] L_Sigma = diag_pre_multiply(Sd_d, L_corr_d); // var-covar matrix for the factors\n  cholesky_factor_cov[D] L_Sigma_m1_m4 = diag_pre_multiply(sd_p[{1, 4}], L_corr_m1_m4); // var-covar matrix of m1 and m4\n\n  // Multilevel structure\n  matrix[N_1, M_1] r_1; // declare some variables to hold the actual cluster-specific effects, where in the parameter block we just declared the standardized ones z_1.\n  vector[N_1] r_1_1; // using vectors speeds up indexing in loops...\n  vector[N_1] r_1_2;\n  vector[N_1] r_1_3;\n  vector[N_1] r_1_4;\n  vector[N_1] r_1_5;\n  r_1 = scale_r_cor(z_1, sd_1, L_1); // use our custom function from the functions block to compute the unstandardized cluster-specific effects\n  r_1_1 = r_1[ : , 1];\n  r_1_2 = r_1[ : , 2];\n  r_1_3 = r_1[ : , 3];\n  r_1_4 = r_1[ : , 4];\n  r_1_5 = r_1[ : , 5];\n  real lprior = 0; // create a variable to contain the priors, to which we'll add some of the actual priors in the next few lines...\n  lprior += normal_lpdf(b[1] | 3.3, 0.4);\n  lprior += normal_lpdf(b[2] | 0, 1.45);\n  lprior += normal_lpdf(b[3] | 0, 1.45);\n  lprior += normal_lpdf(b[4] | 0, 1.45);\n  lprior += normal_lpdf(b[5] | 0, 1.45);\n  lprior += cauchy_lpdf(shape | 0, 2.5) - 1 * cauchy_lccdf(0 | 0, 2.5);\n  lprior += student_t_lpdf(sd_1 | 3, 0, 2.5) - 5 * student_t_lccdf(0 | 3, 0, 2.5);\n  lprior += lkj_corr_cholesky_lpdf(L_1 | 1);\n\n  // Weibull intercepts\n  vector[N] mu_weibull = rep_vector(0.0, N); // initialize linear predictor term for Weibull model\n  mu_weibull += X_weibull * b; // use both factor scores and observed predictors as predictors\n  for (n in 1 : N) { // Add all the cluster-specific varying effects to the linpred\n    mu_weibull[n] += r_1_1[J_1[n]] * Z_1_1[n] + r_1_2[J_1[n]] * Z_1_2[n]\n              + r_1_3[J_1[n]] * Z_1_3[n] + r_1_4[J_1[n]] * Z_1_4[n]\n              + r_1_5[J_1[n]] * Z_1_5[n];\n  }\n}\nmodel {\n  // Factor analysis priors\n  L_corr_d ~ lkj_corr_cholesky(1);\n  L_corr_m1_m4 ~ lkj_corr_cholesky(1);\n  lam_UNC ~ normal(0, 10);\n  sd_p ~ cauchy(0, 2.5);\n\n  // Factor analysis likelihood\n  for (i in 1 : N) {\n    for (j in {2,3,5,6}) {\n      X_factor[i, j] ~ normal(mu_UNC[i, j], sd_p[j]);\n    }\n    to_vector({X_factor[i, 1], X_factor[i, 4]}) ~ multi_normal_cholesky([mu_UNC[i, 1], mu_UNC[i, 4]]', L_Sigma_m1_m4);\n    FS_UNC[i] ~ multi_normal_cholesky(M, L_Sigma);\n  }\n\n  // Weibull Survival Analysis priors\n  shape ~ cauchy(0, 2.5);\n  b[1] ~ normal(3.3, 0.4);\n  for (k in 2:K) {\n    b[k] ~ normal(0, 1.45);\n  }\n  \n  // Weibull Survival Analysis priors\n  for (n in 1 : N) {\n    if (cens[n] == 0) {\n      target += weibull_lpdf(Y[n] | shape, exp(mu_weibull[n]) / tgamma(1 + 1 / shape));\n    } else if (cens[n] == 1) {\n      target += weibull_lccdf(Y[n] | shape, exp(mu_weibull[n]) / tgamma(1 + 1 / shape));\n    } \n  }\n\n  // priors including constants\n  target += lprior;\n  target += std_normal_lpdf(to_vector(z_1));\n}\ngenerated quantities {\n  // Factor analysis\n  corr_matrix[D] Rho_UNC; // correlation matrix for factors\n  corr_matrix[D] Rho_factors; // correlation matrix for factors\n  corr_matrix[D] Rho_m1_m4; // correlation matrix for m1 and m4\n  matrix[P, D] lam; // factor loadings\n  matrix[N, D] FS; // factor scores\n  \n  Rho_UNC = multiply_lower_tri_self_transpose(L_corr_d);\n  Rho_factors = Rho_UNC;\n  Rho_m1_m4 = multiply_lower_tri_self_transpose(L_corr_m1_m4);\n  FS = FS_UNC;\n  lam = rep_matrix(0, P, D);\n  lam[1 : 3, 1] = to_vector(lam_UNC[1 : 3]);\n  lam[4 : 6, 2] = to_vector(lam_UNC[4 : 6]);\n  \n  if (lam_UNC[1] < 0) {\n    lam[1 : 3, 1] = to_vector(-1 * lam_UNC[1 : 3]);\n    FS[ : , 1] = to_vector(-1 * FS_UNC[ : , 1]);\n    if (lam_UNC[4] > 0) {\n      Rho_factors[1, 2] = -1 * Rho_UNC[1, 2];\n      Rho_factors[2, 1] = -1 * Rho_UNC[2, 1];\n    }\n  }\n  if (lam_UNC[4] < 0) {\n    lam[4 : 6, 2] = to_vector(-1 * lam_UNC[4 : 6]);\n    FS[ : , 2] = to_vector(-1 * FS_UNC[ : , 2]);\n    if (lam_UNC[1] > 0) {\n      Rho_factors[2, 1] = -1 * Rho_UNC[2, 1];\n      Rho_factors[1, 2] = -1 * Rho_UNC[1, 2];\n    }\n  }\n\n  // Draws from posterior predictive distribution\n  vector[N] y_pred;\n  for (n in 1:N) {\n    y_pred[n] = weibull_rng(shape, exp(mu_weibull[n]) / tgamma(1 + 1 / shape));\n  }\n\n  // Draws from the _truncated_ posterior predictive distribution\n  vector[N] y_pred_trunc;\n  {\n    real tmp;\n    real max_time;\n    max_time = max(Y); // Assuming Y holds both censored and uncensored times\n    \n    for (n in 1:N) {\n      tmp = max_time + 1; // Initialize tmp to a value just greater than max_time\n      while (tmp > max_time) {\n        tmp = weibull_rng(shape, exp(mu_weibull[n]) / tgamma(1 + 1 / shape));\n      }\n      y_pred_trunc[n] = tmp;\n    }\n  }\n\n  // De-Choleskify the the estimated correlation matrix of the varying effects\n  corr_matrix[M_1] Cor_1 = multiply_lower_tri_self_transpose(L_1); // compute group-level correlations\n  vector<lower=-1, upper=1>[NC_1] cor_1;\n  for (k in 1 : M_1) {    // extract upper diagonal of correlation matrix\n    for (j in 1 : (k - 1)) {\n      cor_1[choose(k - 1, 2) + j] = Cor_1[j, k];\n    }\n  }\n}\n\n\nCode\n# Write the raw Stan code from an R string to a Stan file\nstan_file <- cmdstanr::write_stan_file(stan_model_code)\n\n# Autoformat the model syntax to preempt any warnings at compile time\nmodel <- cmdstanr::cmdstan_model(stan_file, compile = FALSE)\nmodel$format(canonicalize = TRUE)\n\n# Compile the model from the Stan file\nmodel <- cmdstanr::cmdstan_model(stan_file)\n\n# Put the data into a list format Stan can understand\nP <- 8 # Because 6 measured variables + 2 factors \nN <- nrow(fake_dat)\nX_factor <- as.matrix(fake_dat |> select(matches(\"^m[1-9]$|^adapt$|^collab$\")))\n\nY <- fake_dat$time\ncens <- fake_dat$censored\nX_weibull <- as.matrix(\n  fake_dat |> \n    mutate(intercept = 1) |>\n    select(\n      intercept,\n      adapt,\n      collab,\n      age,\n      ei_status\n    ) \n)\n\nN_clusters <- fake_dat |> distinct(cluster_id) |> nrow()\n\ndata_list <- list(\n  ### Factor analysis\n  N=N, # Sample size\n  P=P, # Parameters in the factor analysis\n  D=2, # Number of factors\n  X_factor=X_factor, # Data matrix for factor analysis\n  n_lam=6, # N factor loadings\n\n  ### Time-to-event analysis\n  Y=Y, # Event times\n  K=5, # Four predictors plus a dummy column of 1s for the intercept\n  cens=cens, # Censoring status\n  X_weibull=X_weibull, # Data matrix for Weibull regression\n\n  ### Multilevel structure\n  N_1 = N_clusters,\n  M_1 = 5, # Number of varying effects\n  J_1  = as.integer(fake_dat$cluster_id), # Row-specific cluster indicators\n  Z_1_1 = rep(1, N), # Cluster-level predictors...\n  Z_1_2 = fake_dat$adapt,\n  Z_1_3 = fake_dat$collab,\n  Z_1_4 = fake_dat$age,\n  Z_1_5 = fake_dat$ei_status,\n  NC_1 = 10 # Number of correlation terms for varying effect covariance matrix, (N * N-1 / 2)\n)\n\n# Fit the model\nfit <- model$sample(\n  data = data_list,\n  seed = 199,\n  chains = 4,\n  parallel_chains = 4,\n  iter_warmup = 1000,\n  iter_sampling = 6000,\n  output_dir = \"fits\",\n  refresh = 1 # print update at each iter\n)\n\n# Save the resulting model fit\nsaveRDS(fit, \"fits/b07.05.rds\")\n\n\n\n\n5.7.3 Posterior Checks\n\n\nCode\nfit <- readRDS(\"fits/b07.05.rds\")\n\n# Call this if you need to check variable names\n# fit$metadata() \n\n# Load the MCMC draws from the variables we care about\ndraws <- fit$draws(\n  variables = c(\n  \"lam[1,1]\",\n  \"lam[2,1]\",\n  \"lam[3,1]\",\n  \"lam[4,2]\",\n  \"lam[5,2]\",\n  \"lam[6,2]\",\n  \"Rho_factors[2,1]\", \n  \"Rho_m1_m4[2,1]\",\n  \"b\",\n  \"shape\"\n  )\n)\n\n# Save the result\nsaveRDS(draws, \"fits/b07.05.samples.rds\")\n\n\nNow we can visualize all the same parameter estimates as before, but also the new estimates of the standard deviations of the varying effects, shown below in green. The model’s 95% intervals include the true parameter values for al\n\n\nCode\n# Load the draws saved in the previous block\ndraws <- readRDS(\"fits/b07.05.samples.rds\")\n\n# Plot\ncleaned_draws <- draws |>\n\n  # Get the raw draws for the factor loadings and the beween-factor correlation coefficient\n  gather_draws(\n    `b[1]`,\n    `b[2]`,\n    `b[3]`,\n    `b[4]`,\n    `b[5]`,\n    `lam[1,1]`,\n    `lam[2,1]`,\n    `lam[3,1]`,\n    `lam[4,2]`,\n    `lam[5,2]`,\n    `lam[6,2]`,\n    `Rho_factors[2,1]`,\n    `Rho_m1_m4[2,1]`\n  ) |>\n\n  mutate(.value = ifelse(.variable %in% c(\"b[1]\", \"b[2]\", \"b[3]\", \"b[4]\", \"b[5]\"), exp(.value), .value)) |>\n  \n  # Do some renaming for clarity\n  mutate(.variable = case_when(\n   .variable == \"b[1]\" ~ \"Intercept\",\n   .variable == \"b[2]\" ~ \"b_Adapt\",\n   .variable == \"b[3]\" ~ \"b_Collab\",\n   .variable == \"b[4]\" ~ \"b_Age\",\n   .variable == \"b[5]\" ~ \"b_EI Status\",\n   .variable == \"lam[1,1]\" ~ \"Loading for m1\",\n   .variable == \"lam[2,1]\" ~ \"Loading for m2\",\n   .variable == \"lam[3,1]\" ~ \"Loading for m3\",\n   .variable == \"lam[4,2]\" ~ \"Loading for m4\",\n   .variable == \"lam[5,2]\" ~ \"Loading for m5\",\n   .variable == \"lam[6,2]\" ~ \"Loading for m6\",\n   .variable == \"Rho_factors[2,1]\" ~ \"Covariance for f1 ~ f2\",\n   .variable == \"Rho_m1_m4[2,1]\" ~ \"Covariance for m1 ~ m4\"\n )) |>\n\n  filter(.variable != \"Intercept\") |>\n\n  # Add the true factor loadings for plotting\n  mutate(true_loading = case_when(\n    .variable == \"b_Age\" ~ 1.11,\n    .variable == \"b_Collab\" ~ 0.833,\n    .variable == \"b_Adapt\" ~ 1,\n    .variable == \"b_EI Status\" ~ 0.5,\n    .variable == \"Loading for m1\" ~ .8,\n    .variable == \"Loading for m2\" ~ .6,\n    .variable == \"Loading for m3\" ~ .9,\n    .variable == \"Loading for m4\" ~ .1,\n    .variable == \"Loading for m5\" ~ .2,\n    .variable == \"Loading for m6\" ~ -.4,\n    .variable == \"Covariance for f1 ~ f2\" ~ .4,\n    .variable == \"Covariance for m1 ~ m4\" ~ .4\n  )) \n\n# Factor loadings\ncleaned_draws |>\n\n  filter(str_detect(.variable, \"Loading\")) |>\n\n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(fill = clrs$purple) +\n  geom_vline(aes(xintercept = true_loading), linetype = 2) + \n  scale_x_continuous(expand = c(0, 0.015)) +\n  scale_y_continuous(expand = c(0, 0.015)) +\n  guides(fill = \"none\") +\n  theme_nice() +\n  theme_posterior_densities() +\n  labs(\n    x = \"Estimate\",\n    y = \"Density\"\n  )  +\n  facet_wrap(~.variable, scales = \"free_x\") \n\n\n\n\n\nCode\n# Latent covariance terms\ncleaned_draws |>\n\n  filter(str_detect(.variable, \"Covariance\")) |>\n\n  ggplot(aes(x = .value)) +\n  stat_halfeye(aes(fill = .variable)) +\n  geom_vline(aes(xintercept = true_loading), linetype = 2) + \n  scale_fill_manual(values = c(clrs$red, clrs$yellow)) +\n  scale_x_continuous(expand = c(0, 0.015)) +\n  scale_y_continuous(expand = c(0, 0.015)) +\n  guides(fill = \"none\") +\n  theme_nice() +\n  theme_posterior_densities() +\n  labs(\n    x = \"Estimate\",\n    y = \"Density\"\n  )  +\n  facet_wrap(~.variable, scales = \"free_x\") \n\n\n\n\n\nCode\n# Latent covariance terms\ncleaned_draws |>\n\n  filter(str_detect(.variable, \"b_\")) |>\n\n  ggplot(aes(x = .value)) +\n  stat_halfeye(fill = clrs$blue) +\n  geom_vline(aes(xintercept = true_loading), linetype = 2) + \n  scale_x_continuous(expand = c(0, 0.015)) +\n  scale_y_continuous(expand = c(0, 0.015)) +\n  guides(fill = \"none\") +\n  theme_nice() +\n  theme_posterior_densities() +\n  labs(\n    x = \"Estimate\",\n    y = \"Density\"\n  )  +\n  facet_wrap(~.variable, scales = \"free_x\") \n\n\n\n\n\n\n\nCode\nfit <- readRDS(\"fits/b07.05.rds\")\n\n# Call this if you need to check variable names\nfit$metadata() \n\n# Load the MCMC draws from the variables we care about\ncovar_draws <- fit$draws(\n  variables = c(\n    \"sd_1\",\n    \"cor_1\"\n  )\n)\n\ncovar_draws_summary <- covar_draws |> summary()\n\n# Save the result\nsaveRDS(covar_draws, \"fits/b07.05.covar.samples.rds\")\nsaveRDS(covar_draws_summary, \"fits/b07.05.covar.samples.summary.rds\")\n\n\n\n\nCode\ncovar_draws <- readRDS(\"fits/b07.05.covar.samples.rds\") \n\ncovar_draws |> \n\n# Get the raw draws\n  gather_draws(\n    sd_1[i]\n  #  cor_1[i]\n  ) |> \n  \n  \n  # Do some renaming for clarity\n  mutate(.variable = case_when(\n    .variable == \"sd_1\" & i == 1 ~ \"sd_intercept\",\n    .variable == \"sd_1\" & i == 2 ~ \"sd_Adapt\",\n    .variable == \"sd_1\" & i == 3 ~ \"sd_Collab\",\n    .variable == \"sd_1\" & i == 4 ~ \"sd_Age\",\n    .variable == \"sd_1\" & i == 5 ~ \"sd_EI Status\"\n  )) |> \n    \n  filter(.variable != \"sd_intercept\") |> \n  \n  mutate(.value = exp(.value)) |> \n\n  # Add the true simulation parameter values\n  mutate(true_loading = case_when(\n    .variable == \"sd_adapt\" ~ 1.1,\n    .variable == \"sd_collab\" ~ 1.3,\n    .variable == \"sd_age\" ~ 2,\n    .variable == \"sd_ei_status\" ~ 1.2,\n  )) |> \n\n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(fill = clrs$green) +\n  geom_vline(aes(xintercept = true_loading), linetype = 2) + \n  scale_x_continuous(expand = c(0, 0.015)) +\n  scale_y_continuous(expand = c(0, 0.015)) +\n  guides(fill = \"none\") +\n  theme_nice() +\n  theme_posterior_densities() +\n  labs(\n    x = \"Estimate\",\n    y = \"Density\"\n  )  +\n  facet_wrap(~.variable, scales = \"free_x\") \n\n\nWarning: Removed 96000 rows containing missing values (`geom_vline()`).\n\n\n\n\n\nThe MCMCM diagnostics all look good:\n\n\nCode\n# Precompute the summary, to save time at quarto render\nsummary_draws <- draws |> summary() \nsaveRDS(summary_draws, \"fits/b07.05.summary.rds\")\n\n\n\n\nCode\nsummary_draws <- readRDS(\"fits/b07.05.summary.rds\")\n\nsummary_draws |> \n\n  select(variable, rhat, ess_bulk, ess_tail) |>\n\n  mutate(across(where(is.numeric), ~round(.x, 2))) |> \n\n  knitr::kable()\n\n\n\n\n\nvariable\nrhat\ness_bulk\ness_tail\n\n\n\n\nlam[1,1]\n1.00\n4254.24\n8094.98\n\n\nlam[2,1]\n1.00\n14928.70\n17408.69\n\n\nlam[3,1]\n1.00\n5861.26\n10756.91\n\n\nlam[4,2]\n1.00\n4181.75\n8500.97\n\n\nlam[5,2]\n1.00\n4504.81\n7613.87\n\n\nlam[6,2]\n1.00\n1374.40\n1660.91\n\n\nRho_factors[2,1]\n1.01\n1143.16\n2052.58\n\n\nRho_m1_m4[2,1]\n1.00\n9559.78\n16264.81\n\n\nb[1]\n1.00\n18085.54\n17786.14\n\n\nb[2]\n1.00\n25852.35\n17825.27\n\n\nb[3]\n1.00\n13912.43\n16561.26\n\n\nb[4]\n1.00\n7926.43\n12265.94\n\n\nb[5]\n1.00\n24393.42\n19452.43\n\n\nshape\n1.00\n25227.36\n17644.14\n\n\n\n\n\nSince this is a multilevel model, we should expect to see shrinkage in the cluster-specific parameter estimates: the estimates should be pulled towards the mean of the distribution of estimates, especially for smaller clusters and clusters with more extreme unpooled parameter estimates. We can visualize this shrinkage by taking our secret-weapon plot from before and adding in our Stan model’s cluster-specific posterior estimates. Here we see shrinkage on full display for the main predictors of interest, with the Stan model showing more stable and accurate estimates in general, especially for clusters where the unpooled estimate is extreme:\n\n\nCode\nfit <- readRDS(\"fits/b07.05.rds\")\n\n# Call this if you need to check variable names\n# fit$metadata() \n\n# Load the MCMC draws from the variables we care about\nranef_draws <- fit$draws(\n  variables = c(\n    \"r_1_1\",\n    \"r_1_2\",\n    \"r_1_3\",\n    \"r_1_4\",\n    \"r_1_5\"\n  )\n)\n\nranef_summary <- ranef_draws |> summary()\n\n# Save the result\nsaveRDS(draws, \"fits/b07.05.ranef.samples.rds\")\nsaveRDS(ranef_summary, \"fits/b07.05.ranef.samples.summary.rds\")\n\n\n\n\nCode\nranef_summary <- readRDS(\"fits/b07.05.ranef.samples.summary.rds\")\n\n# Tidy up the posterior estimates \nranef_summary_cleaned <- ranef_summary |>\n  \n  mutate(\n    across(where(is.numeric), exp),\n    cluster_id = as.factor(str_extract(variable, \"(?<=\\\\[)\\\\d+(?=\\\\])\"))\n  ) |> \n  \n  # get the precalculated cluster sizes\n  left_join(dplt_secret_weapon |> select(cluster_id, n) |> distinct(cluster_id, .keep_all = TRUE)) |> \n  \n  # reorder the cluster_ids by cluster size and clean the `term` variable\n  mutate(\n    cluster_id = factor(cluster_id, levels = unique(cluster_id[order(n)])),\n    term = case_when(\n      str_detect(variable, \"r_1_1\") ~ \"intercept\",\n      str_detect(variable, \"r_1_2\") ~ \"adapt\",\n      str_detect(variable, \"r_1_3\") ~ \"collab\",\n      str_detect(variable, \"r_1_4\") ~ \"age\",\n      str_detect(variable, \"r_1_5\") ~ \"ei_status\",\n      TRUE ~ variable\n    )\n  ) |> \n  \n  rename(\n    \"posterior_mean\" = mean, \n    \"posterior_sd\" = sd, \n    \"posterior_q5\" = q5, \n    \"posterior_q95\" = q95\n  ) |> \n  \n  select(\n    cluster_id, \n    term, \n    posterior_mean, \n    posterior_sd, \n    posterior_q5, \n    posterior_q95\n  ) \n  \n# Tidy the true cluster-specific parameter estimates:\ntrue_cluster_effects <- fake_dat |> \n      \n    dplyr::select(cluster_id, starts_with(\"beta\")) |> \n      \n    distinct(cluster_id, .keep_all = TRUE) |>\n      \n    mutate(cluster_id = factor(cluster_id)) |> \n  \n      # get the precalculated cluster sizes\n    left_join(dplt_secret_weapon |> dplyr::select(cluster_id, n) |> distinct(cluster_id, .keep_all = TRUE)) |> \n        \n    mutate(cluster_id = factor(cluster_id, levels = unique(cluster_id[order(n)]))) |> \n\n      \n    pivot_longer(starts_with(\"beta\"), names_to = \"term\", values_to = \"true_cluster_effect\") |> \n      \n    mutate(\n      # Clean the labels\n      term = str_remove(term, \"beta_\"),\n      # Convert the true simulated effects into survival time ratios\n      true_cluster_effect = exp(-true_cluster_effect)\n    )\n  \n# Start with the data from the secret weapon plot we made before fitting the Stan model:\ndplt_secret_weapon |> \n\n  # We fit the secret weapon estimates using factor scores. Rename them to align with the other elements of the present chart.\n  mutate(term = case_when(\n    term == \"adapt_score\" ~ \"adapt\",\n    term == \"collab_score\" ~ \"collab\",\n    TRUE ~ term\n  )) |>\n  \n  # Add the cluster-specific true parameter values\n  left_join(true_cluster_effects) |> \n    \n  # Add the Stan model's posterior summaries\n  left_join(ranef_summary_cleaned) |> \n  \n  filter(term %in% c(\"adapt\", \"collab\")) |>\n\n  mutate(term = str_to_title(term)) |>\n  \n  # Plot\n  ggplot(aes(x = factor(cluster_id), y = estimate)) +\n  geom_hline(aes(yintercept = true_mean_effect), linetype = 2) +\n  geom_point(aes(y = true_cluster_effect), position = position_dodge(width = 0.5), size = 3, colour = \"#D02090\", shape = 1, stroke = 1) +\n  geom_point(position = position_dodge(width = 0.5), size = 3, colour = \"#767676\", alpha = .7) +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high),\n                position = position_dodge(width = 0.5), width = 0.2, colour = \"#767676\", alpha = .7) +\n  geom_point(aes(y = posterior_mean), position = position_dodge(width = 0.5), size = 3, colour = clrs$purple, alpha = .7) +\n  geom_errorbar(aes(ymin = posterior_q5, ymax = posterior_q95),\n                position = position_dodge(width = 0.5), width = 0.2, colour = clrs$purple, alpha = .7) +         scale_x_discrete(\n    breaks = NULL,  \n    labels = NULL,  \n    name = \"<---- Smaller Clusters                              Bigger Clusters ---->\"  \n  ) +\n  ylab(\"Survival Time Ratio\") +\n  theme_nice() +\n  theme(legend.position = \"bottom\") +\n  labs(y = \"Survival Time Ratio\") +\n  facet_wrap(~ term, scales = c(\"free\"), ncol = 1) \n\n\n\n\n\nShrinkage on display: posterior means and compatible intervals (purple) are more stable and accurate than unpooled estimates and asymptotic compatible intervals (grey) when compared to true simulated effects (violet rings). The dashed line shows the true simulation mean effect.\n\n\n\n\nHow about the posterior predictions? Did sampling from the constrained posterior predictive distribution improve the model’s predictions? Let’s retrieving the samples from both the constrained and unconstrained posterior predictive distributions. We’ll chunk the cleaning into steps so this document doesn’t take ages to render.\n\n\nCode\n# Get the draws from the posterior predictive distribution\npp_draws <- fit$draws(variables = c(\"y_pred\"))\npp_draws_trunc <- fit$draws(variables = c(\"y_pred_trunc\"))\n\n# Save the results\nsaveRDS(pp_draws, \"fits/b07.05.post_pred_samples.rds\")\nsaveRDS(pp_draws_trunc, \"fits/b07.05.post_pred_samples_trunc.rds\")\n\n\nLoad the samples from the cache:\n\n\nCode\n# Load the posterior preds from the cache\npp_draws <- readRDS(\"fits/b07.05.post_pred_samples.rds\")\npp_draws_trunc <- readRDS(\"fits/b07.05.post_pred_samples_trunc.rds\")\n\n# Summarize the draws into points\npp_summaries <- pp_draws |> summary()\npp_summaries_trunc <- pp_draws_trunc |> summary()\n\n# Store the summary objects again to save time at quarto render\nsaveRDS(pp_summaries, \"fits/b07.05.post_pred_summaries.rds\")\nsaveRDS(pp_summaries_trunc, \"fits/b07.05.post_pred_summaries_trunc.rds\")\n\n\nThe mean posterior predictions:\nWe can get a more detailed look at the model’s uncertainty by plotting the densitities of draws from the posterior distribution against the distribution of the original data. Again we pre-process the draws so that rendering the Quarto document isn’t slow:\n\n\nCode\ndraws_untrunc <- pp_draws |>\n  \n  gather_draws(\n    y_pred[i],\n    ndraws = 10\n  ) |>\n\n  ungroup() |>\n  \n  select(i, .draw, .value) |> \n  \n  mutate(group = \"non-truncated\")\n\ndraws_trunc <- pp_draws_trunc |> \n  \n  gather_draws(\n    y_pred_trunc[i],\n    ndraws = 10\n  ) |>\n\n  ungroup() |>\n  \n  select(i, .draw, .value) |> \n  \n  mutate(group = \"truncated\")\n\ndplt <- \n  rbind(\n    draws_trunc,\n    draws_untrunc\n  )\n\nsaveRDS(dplt, \"fits/b07.05.post_pred_20_draws.rds\")\n\n\nBoth the truncated and untruncated posterior predictive distributions are a better fit to the data here than in the previous model with no multilevel structure. But the posterior predictive distribution of event times still is less peaked and has a wider tail than the observed distribution. The overall density isn’t very different between the posterior predictions for the truncated vs untruncated sampling approaches, which reflects the fact that the absurdly long event times in the untruncated approach were only a tiny minority of events. Still, the truncated approach is better because it doesn’t allow for these absurdities, making it more useful in a predictive context, for example.\n\n\nCode\ndplt <- readRDS(\"fits/b07.05.post_pred_20_draws.rds\")\n\nggplot() +\n  geom_density(data = dplt |> filter(group == \"truncated\"), \n               aes(x = .value, group = .draw, colour = \"Truncated posterior draws\")) +\n  geom_density(data = dplt |> filter(group == \"non-truncated\"), \n               aes(x = .value, group = .draw, colour = \"Non-truncated posterior draws\")) + \n  geom_density(data = fake_dat |> filter(censored == 0), \n               aes(x = time), color = \"black\") +\n  scale_colour_manual(\n    name='',\n    breaks=c('Non-truncated posterior draws', 'Truncated posterior draws', 'Data'),\n    values=c('Non-truncated posterior draws' = alpha(clrs$blue, .5), \n             'Truncated posterior draws' = alpha(clrs$red, .5), \n             'Data' = alpha('black', 0))\n  ) +\n  scale_x_continuous(limits = c(0, 365), expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  labs(x = \"Days to employment\") +\n  theme_nice() +\n  theme_posterior_densities() + \n  theme(legend.position = \"bottom\")\n\n\n\n\n\nDensities of 20 draws from the posterior predictive distribution (blue) against the distribution of the data (black). The model expects slower times-to-employment overall than the distribution in the data.\n\n\n\n\n\n\n\n\nAndrew Gelman, Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press.\n\n\nBrilleman, Samuel L., Eren M. Elci, Jacqueline Buros Novik, and Rory Wolfe. 2020. “Bayesian Survival Analysis Using the Rstanarm r Package.” https://arxiv.org/abs/2002.09633.\n\n\nBrown, Timothy A. 2006. Confirmatory Factor Analysis for Applied Research.\n\n\nCollett, David. 2003. Modelling Survival Data in Medical Research. 2nd ed. Chapman; Hall/CRC.\n\n\nGelman, Andrew, Aki Vehtari, Daniel Simpson, Charles C. Margossian, Bob Carpenter, Yuling Yao, Lauren Kennedy, Jonah Gabry, Paul-Christian Bürkner, and Martin Modrák. 2020. “Bayesian Workflow.” https://arxiv.org/abs/2011.01808.\n\n\nKankaraš, Miloš, Eva Feron, and Rachel Renbarger. 2019. “Assessing Students’ Social and Emotional Skills Through Triangulation of Assessment Methods,” no. 208. https://doi.org/https://doi.org/https://doi.org/10.1787/717ad7f2-en.\n\n\nKurz, A. Solomon. 2023. Statistical Rethinking with Brms, Ggplot2, and the Tidyverse: Second Edition. Version 0.4.0. https://bookdown.org/content/4857/.\n\n\nMcElreath, Richard; 2020. Statistical Rethinking: A Bayesian Course with Examples in r and STAN. 2nd ed. CRC Press LLC.\n\n\nSinger, Judith D., and John B. Willett. 2003. Applied Longitudinal Data Analysis - Modeling Change and Event Occurrence. Oxford University Press."
  },
  {
    "objectID": "sem-intro.html",
    "href": "sem-intro.html",
    "title": "Structural Equation Modelling",
    "section": "",
    "text": "Someone on the stan forums recommended this book for a bayesian example: https://www.guilford.com/books/Bayesian-Structural-Equation-Modeling/Sarah-Depaoli/9781462547746"
  }
]