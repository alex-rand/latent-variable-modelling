---
title: "Bayesian CFA"
format:
  html:
    theme: default
---

```{r}
#| label: setup
#| message: false

library(tidyverse)
library(lavaan)
library(blavaan)
library(brms)
library(tidybayes)
library(survival)

# MCMC specifications
options(brms.backend = "cmdstanr")
options(mc.cores = parallel::detectCores())
```

Can brms replicate results from lavaan?

{{brms}} doesn't officially have SEM capabilities [(but they do seem to be coming soon!)](https://github.com/paul-buerkner/brms/issues/304). But [STAN forum contributer Jack Bailey](https://discourse.mc-stan.org/t/confirmatory-factor-analysis-using-brms/23139) has 'patant√©' a solution using artful prior specifications.

To demonstrate, we can simulate some data using {{lavaan}} and show that `brms::brm()` is able to recover the 'true' factor loadings.

## First Example: Simple Factor Structure

First we can simulate some data with a simple factor structure using lavaan's handy `simulateData()` function:
```{r}
#| label: sim-1-fac
#| message: false
#| warning: false

pop.model <- ' 
  f1 =~ .8*m1 + .1*m2 + .6*m3 + .2*m4 + .9*m5 + -.4*m6
'

# Simulate data from the model
fake_dat <- lavaan::simulateData(model = pop.model, sample.nobs = 4000)

# Visualize the measured dat
fake_dat |>

  select(m1, m2, m3, m4, m5, m6) |>
  
  pivot_longer(everything(), names_to = "var", values_to = "measurement") |>

  ggplot() +
  geom_histogram(aes(x = measurement)) +
  facet_wrap(~var) +
  theme_bw()

```

Unsurprisingly, the lavaan `cfa()` function is able to recover the true parameters for the data it generated.

```{r}
#| label: lavaan-fit-1-fac
#| message: false
#| warning: false

# Fit the model
fit_1 <- cfa(pop.model, data = fake_dat)

# Make sure it works
fit_1 |> broom::tidy()

```

But can brms recover the true parameter values? I like this approach because it gives a nice conceptual perspective on what we're actually doing when we're doing latant variable modelling: we're just doing a linear regression where the measured variables are all drawn from a sharedd multivariate normal distribution, and where the linear model component of their location parameters include a covariate for which we only have missing data. It's a pretty cool and weird thing.

```{r}
#| label: brms-fit-1-fac
#| eval: false

# Add a latent variable to the dataset
fake_dat$f1 <- NA_real_

bfit.1 <- brm(
    formula =
      bf(m1 ~ 0 + mi(f1)) +
      bf(m2 ~ 0 + mi(f1)) +
      bf(m3 ~ 0 + mi(f1)) +
      bf(m4 ~ 0 + mi(f1)) +
      bf(m5 ~ 0 + mi(f1)) +
      bf(m6 ~ 0 + mi(f1)) +
      bf(f1| mi() ~ 1) + 
      set_rescor(rescor = FALSE),
    family = gaussian(),
    prior =
      prior(constant(1), class = "b", resp = "m1") +
      prior(constant(1), class = "sigma", resp = "m1") +
      prior(normal(0, 10), class = "b", resp = "m2") +
      prior(constant(1), class = "sigma", resp = "m2") +
      prior(normal(0, 10), class = "b", resp = "m3") +
      prior(constant(1), class = "sigma", resp = "m3") +
      prior(normal(0, 10), class = "b", resp = "m4") +
      prior(constant(1), class = "sigma", resp = "m4") +
      prior(normal(0, 10), class = "b", resp = "m5") +
      prior(constant(1), class = "sigma", resp = "m5") +
      prior(normal(0, 10), class = "b", resp = "m6") +
      prior(constant(1), class = "sigma", resp = "m6") +
      prior(normal(0, 10), class = "Intercept", resp = "f1") +
      prior(cauchy(0, 1), class = "sigma", resp = "f1"),
    data = fake_dat,
    warmup = 1000,
    iter = 6000,
    file = "fits/b07.01.rds"
  )

```

A Bayesian missing data model treats each missing observation as a parameter to estimate. And since the factor is missing in each row of the dataset, this means we end up with 6000 samples for each row of data we have, resulting in a pretty big model file. This is too big for Github, so I'll only push the draws we need to replicate the figure below.

First we can put the draws we need into a tidy format and do some cleaning:

```{r}
#| label: wrangle-draws-1-fac
#| eval: false

bfit.1.samples <- bfit.1 |>

  # Get the raw MCMC samples
  gather_draws(
    bsp_m2_mif1,
    bsp_m3_mif1,
    bsp_m4_mif1,
    bsp_m5_mif1,
    bsp_m6_mif1
  ) |>

  # Do some renaming for clarity
  mutate(.variable = case_when(
    .variable == "bsp_m2_mif1" ~ "Loading for m2",
    .variable == "bsp_m3_mif1" ~ "Loading for m3",
    .variable == "bsp_m4_mif1" ~ "Loading for m4",
    .variable == "bsp_m5_mif1" ~ "Loading for m5",
    .variable == "bsp_m6_mif1" ~ "Loading for m6"
  )) |> 

  # Add the true factor loadings from above
  mutate(true_loading = case_when(
    .variable == "Loading for m2" ~ .1,
    .variable == "Loading for m3" ~ .6,
    .variable == "Loading for m4" ~ .2,
    .variable == "Loading for m5" ~ .9,
    .variable == "Loading for m6" ~ -.4
  )) 

# Save the tidy draws for reproducibility
saveRDS(bfit.1.samples, "fits/b07.01.samples.rds")
```

Now we can plot. Looks like STAN does an OK job at recovering the true factor loadings (we exclude the first loading since it was held constant at 1 to provide the scale for the latent variable):
```{r}
#| label: viz-draws-1-fac
#| message: false
#| warning: false
#| fig-cap: "Posterior distributions of estimated factor loadings for measured variables 2-6. The dashed line is the 'true' simulated loading for each variable."
#| fig-alt: "Posterior distributions of estimated factor loadings for measured variables 2-6. The dashed line is the 'true' simulated loading for each variable."


# Load the tidy samples
bfit.1.samples <- readRDS("fits/b07.01.samples.rds")

# Plot
bfit.1.samples|>

  ggplot(aes(x = .value, fill = .variable)) +
    stat_halfeye(fill = "mediumorchid") +
    geom_vline(aes(xintercept = true_loading), linetype = 2) + 
    scale_x_continuous(expand = c(0, 0.015)) +
    scale_y_continuous(expand = c(0, 0.015)) +
    guides(fill = "none") +
    labs(x = "lab",
       y = NULL)  +
    facet_wrap(~.variable) +
    theme_minimal() + 
    theme(panel.grid.major = element_blank())
  
```

In all cases the model is __pretty close__ to the true parameter value. It's a bit concerning when the standard errors are so small they fail to capture the true values, but this illustrates the general proof of concept: we can do a pretty good CFA in {{brms}}.

## Second Example: Correlated factors

Now an example where we assume the latent vactors are themselves dependent, so that discriminant validity is questionable. So we can include a term for their correlation structure in the model. We'll need to amend the lavaan model structure to implement these changes.

```{r}
#| label: sim-2-fac
#| message: false
#| warning: false

# Declare the new model with correlated factors
correlated.factors.model <- ' 
  f1 =~ .8*m1 + .6*m2 + .9*m3
  f2 =~ .1*m4 + .2*m5 + -.4*m6
  f1 ~~ .4*f2
'

# Simulate data from the model
fake_dat <- lavaan::simulateData(model = correlated.factors.model, sample.nobs = 4000)

```

The complication here is that brms doesn't currently support setting constraints on individual correlation terms in the model, even in the form of constant priors. [There seems to be a plan](https://github.com/paul-buerkner/brms/issues/957) to include more SEM-like flexibility of covariance matrixes in general in brms 3.0, but for now we'll need to specify the model using raw Stan code. There seem to be a few strategies for avoiding convergence issues when fitting this kind of model in Stan. I worked closely from the strategy provided in [this Stan forum comment](https://discourse.mc-stan.org/t/non-convergence-of-latent-variable-model/12450/13) by Mauricio Garnier-Villarre, making a few minor adjustments such as removing latent intercepts from the linear predictors to better align this model with the {{lavaan}} defaults. 

Here is the raw Stan code. I also declare it as a string in a hidden R block so we can use {{cmdstanr}}'s autoformatting function before we compile. 

```stan
data {
  int N; // sample size
  int P; // number of variables
  int D; // number of factors
  array[N] vector[P] X; // data matrix of order [N,P]
  int n_lam; // how many factor loadings are estimated
}
parameters {
  matrix[N, D] FS_UNC; // factor scores, matrix of order [N,D]
  cholesky_factor_corr[D] L_corr_d; // cholesky correlation between factors
  vector<lower=0>[P] sd_p; // residual sd for each variable
  vector<lower=-30, upper=30>[n_lam] lam_UNC; // A bunch of lightly-constrained factor loadings
}
transformed parameters {
  // a vector to hold the factor means, which will be a bunch of 0s
  vector[D] M;
  M = rep_vector(0, D);
  
  // a vector to hold the factor SDs, which will be a bunch of 1s. 
  vector<lower=0>[D] Sd_d;
  Sd_d = rep_vector(1, D);
  
  // A vector to hold the linear predictors for the manifest variables, which are each a deterministic function of loading*factor_score
  array[N] vector[P] mu_UNC;
  for (i in 1 : N) {
    mu_UNC[i, 1] = lam_UNC[1] * FS_UNC[i, 1];
    mu_UNC[i, 2] = lam_UNC[2] * FS_UNC[i, 1];
    mu_UNC[i, 3] = lam_UNC[3] * FS_UNC[i, 1];
    mu_UNC[i, 4] = lam_UNC[4] * FS_UNC[i, 2];
    mu_UNC[i, 5] = lam_UNC[5] * FS_UNC[i, 2];
    mu_UNC[i, 6] = lam_UNC[6] * FS_UNC[i, 2];
  }
  
  // the var-covar matrix for the factors, a deterministic function of the deterministic SDs and the estimated rhos defined in parameters{}
  cholesky_factor_cov[D] L_Sigma;
  L_Sigma = diag_pre_multiply(Sd_d, L_corr_d);
}
model {
  // Declare some priors
  L_corr_d ~ lkj_corr_cholesky(1); // Prior on factor corrs
  lam_UNC ~ normal(0, 10); // Prior on loadings
  sd_p ~ cauchy(0, 2.5); // Prior on residual SDs of manifest variables
  
  // Set up the likelihoods of the manifest and latent factors
  for (i in 1 : N) {
    for (j in 1 : P) {
      // Manifest variables
      X[i, j] ~ normal(mu_UNC[i, j], sd_p[j]);
    }
    // Latent factors
    FS_UNC[i] ~ multi_normal_cholesky(M, L_Sigma);
  }
}
generated quantities {
  array[N] real log_lik; // log likelihood of each datapoint
  real dev; // global deviance
  real log_lik0; // global log likelihood
  vector[N] log_lik_row;
  
  cov_matrix[P] Sigma; // Covariance matrix of the manifest variables
  cov_matrix[D] Phi_lv; // Covariance matrix of the latent factors
  matrix[P, P] lambda_phi_lambda; // I think these are the terms of the reconstructed var-covar matrix of the data?
  cholesky_factor_cov[P] L_Sigma_model; // I think this is the cholesky decomposition of the reconstructued var-covar matrix above?
  matrix[P, P] theta_del; // I think this is Matrix containing the error terms for the reconstructed empirical var-covar matrix?
  
  corr_matrix[D] Rho_UNC; /// correlation matrix
  corr_matrix[D] Rho; /// correlation matrix
  matrix[P, D] lam; // factor loadings
  matrix[N, D] FS; // factor scores, matrix of order [N,D]
  
  // Do some fancy things to sign-correct the parameter estimates.
  // The idea seems to be that when we estimate the loadings with unconstrained signs
  // It can lead to identification issues. So we take these steps to correct the signs.
  // See this Stan forum comment for more: https://discourse.mc-stan.org/t/non-convergence-of-latent-variable-model/12450/10?
  Rho_UNC = multiply_lower_tri_self_transpose(L_corr_d);
  Rho = Rho_UNC;
  FS = FS_UNC;
  lam = rep_matrix(0, P, D);
  lam[1 : 3, 1] = to_vector(lam_UNC[1 : 3]);
  lam[4 : 6, 2] = to_vector(lam_UNC[4 : 6]);
  
  // factor 1
  if (lam_UNC[1] < 0) {
    lam[1 : 3, 1] = to_vector(-1 * lam_UNC[1 : 3]);
    FS[ : , 1] = to_vector(-1 * FS_UNC[ : , 1]);
    
    if (lam_UNC[4] > 0) {
      Rho[1, 2] = -1 * Rho_UNC[1, 2];
      Rho[2, 1] = -1 * Rho_UNC[2, 1];
    }
  }
  // factor 2
  if (lam_UNC[4] < 0) {
    lam[4 : 6, 2] = to_vector(-1 * lam_UNC[4 : 6]);
    FS[ : , 2] = to_vector(-1 * FS_UNC[ : , 2]);
    
    if (lam_UNC[1] > 0) {
      Rho[2, 1] = -1 * Rho_UNC[2, 1];
      Rho[1, 2] = -1 * Rho_UNC[1, 2];
    }
  }
  
  /// marginal log-likelihood based on signed corrected parameters
  Phi_lv = quad_form_diag(Rho, Sd_d);
  lambda_phi_lambda = quad_form_sym(Phi_lv, transpose(lam));
  theta_del = diag_matrix(sd_p);
  
  Sigma = lambda_phi_lambda + theta_del;
  L_Sigma_model = cholesky_decompose(Sigma);
  
  for (i in 1 : N) {
    log_lik[i] = multi_normal_cholesky_lpdf(X[i] | rep_vector(0, P), L_Sigma_model);
  }
  
  log_lik0 = sum(log_lik); // global log-likelihood
  dev = -2 * log_lik0; // model deviance
}

```

```{r}
#| label: stan-string-2-fac
#| eval: false
#| include: false

# We like the syntax highlighting of declaring the model in a Stan block,
# But here I also secretly declare it as a string in an R block so we can use autoformatting before compile time. 
stan_model_code <- "
data {
  int N; // sample size
  int P; // number of variables
  int D; // number of factors
  array[N] vector[P] X; // data matrix of order [N,P]
  int n_lam; // how many factor loadings are estimated
}
parameters {
  matrix[N, D] FS_UNC; // factor scores, matrix of order [N,D]
  cholesky_factor_corr[D] L_corr_d; // cholesky correlation between factors
  vector<lower=0>[P] sd_p; // residual sd for each variable
  vector<lower=-30, upper=30>[n_lam] lam_UNC; // A bunch of lightly-constrained factor loadings
}
transformed parameters {
  // a vector to hold the factor means, which will be a bunch of 0s
  vector[D] M;
  M = rep_vector(0, D);
  
  // a vector to hold the factor SDs, which will be a bunch of 1s. 
  vector<lower=0>[D] Sd_d;
  Sd_d = rep_vector(1, D);
  
  // A vector to hold the linear predictors for the manifest variables, which are each a deterministic function of loading*factor_score
  array[N] vector[P] mu_UNC;
  for (i in 1 : N) {
    mu_UNC[i, 1] = lam_UNC[1] * FS_UNC[i, 1];
    mu_UNC[i, 2] = lam_UNC[2] * FS_UNC[i, 1];
    mu_UNC[i, 3] = lam_UNC[3] * FS_UNC[i, 1];
    mu_UNC[i, 4] = lam_UNC[4] * FS_UNC[i, 2];
    mu_UNC[i, 5] = lam_UNC[5] * FS_UNC[i, 2];
    mu_UNC[i, 6] = lam_UNC[6] * FS_UNC[i, 2];
  }
  
  // the var-covar matrix for the factors, a deterministic function of the deterministic SDs and the estimated rhos defined in parameters{}
  cholesky_factor_cov[D] L_Sigma;
  L_Sigma = diag_pre_multiply(Sd_d, L_corr_d);
}
model {
  // Declare some priors
  L_corr_d ~ lkj_corr_cholesky(1); // Prior on factor corrs
  lam_UNC ~ normal(0, 10); // Prior on loadings
  sd_p ~ cauchy(0, 2.5); // Prior on residual SDs of manifest variables
  
  // Set up the likelihoods of the manifest and latent factors
  for (i in 1 : N) {
    for (j in 1 : P) {
      // Manifest variables
      X[i, j] ~ normal(mu_UNC[i, j], sd_p[j]);
    }
    // Latent factors
    FS_UNC[i] ~ multi_normal_cholesky(M, L_Sigma);
  }
}
generated quantities {
  array[N] real log_lik; // log likelihood of each datapoint
  real dev; // global deviance
  real log_lik0; // global log likelihood
  vector[N] log_lik_row;
  
  cov_matrix[P] Sigma; // Covariance matrix of the manifest variables
  cov_matrix[D] Phi_lv; // Covariance matrix of the latent factors
  matrix[P, P] lambda_phi_lambda; // I think these are the terms of the reconstructed var-covar matrix of the data?
  cholesky_factor_cov[P] L_Sigma_model; // I think this is the cholesky decomposition of the reconstructued var-covar matrix above?
  matrix[P, P] theta_del; // I think this is Matrix containing the error terms for the reconstructed empirical var-covar matrix?
  
  corr_matrix[D] Rho_UNC; /// correlation matrix
  corr_matrix[D] Rho; /// correlation matrix
  matrix[P, D] lam; // factor loadings
  matrix[N, D] FS; // factor scores, matrix of order [N,D]
  
  // Do some fancy things to sign-correct the parameter estimates.
  // The idea seems to be that when we estimate the loadings with unconstrained signs
  // It can lead to identification issues. So we take these steps to correct the signs.
  // See this Stan forum comment for more: https://discourse.mc-stan.org/t/non-convergence-of-latent-variable-model/12450/10
  Rho_UNC = multiply_lower_tri_self_transpose(L_corr_d);
  Rho = Rho_UNC;
  FS = FS_UNC;
  lam = rep_matrix(0, P, D);
  lam[1 : 3, 1] = to_vector(lam_UNC[1 : 3]);
  lam[4 : 6, 2] = to_vector(lam_UNC[4 : 6]);
  
  // factor 1
  if (lam_UNC[1] < 0) {
    lam[1 : 3, 1] = to_vector(-1 * lam_UNC[1 : 3]);
    FS[ : , 1] = to_vector(-1 * FS_UNC[ : , 1]);
    
    if (lam_UNC[4] > 0) {
      Rho[1, 2] = -1 * Rho_UNC[1, 2];
      Rho[2, 1] = -1 * Rho_UNC[2, 1];
    }
  }
  // factor 2
  if (lam_UNC[4] < 0) {
    lam[4 : 6, 2] = to_vector(-1 * lam_UNC[4 : 6]);
    FS[ : , 2] = to_vector(-1 * FS_UNC[ : , 2]);
    
    if (lam_UNC[1] > 0) {
      Rho[2, 1] = -1 * Rho_UNC[2, 1];
      Rho[1, 2] = -1 * Rho_UNC[1, 2];
    }
  }
}
"

```

Now we can compile and run the model

```{r}
#| label: stanfit-2-fac
#| eval: false

# Write the raw Stan code from an R string to a Stan file
stan_file <- cmdstanr::write_stan_file(stan_model_code)

# Autoformat the model syntax to preempt any warnings at compile time
model <- cmdstanr::cmdstan_model(stan_file, compile = FALSE)
model$format(canonicalize = TRUE)

# Compile the model from the Stan file
model <- cmdstanr::cmdstan_model(stan_file)

# Put the data into a list format Stan can understand
X <- as.matrix(fake_dat)
P <- ncol(fake_dat) 
N <- nrow(fake_dat)
D <- 2

data_list <- list(N=N,P=P,D=D,X=X, n_lam=6)
param <- c("lam","Rho","sd_p","M","Sd_d",
           "log_lik0","dev","log_lik")

# Fit the model
fit <- model$sample(
  data = data_list,
  seed = 199,
  chains = 4,
  parallel_chains = 4,
  iter_warmup = 1000,
  iter_sampling = 6000,
  adapt_delta=0.99, 
  max_treedepth = 12,
  refresh = 1 # print update every 500 iters
)

# Save the resulting model fit
saveRDS(fit, "fits/b07.02.rds")


```

The model took about 5 hours to sample and the resulting files are enormous -- on the order of 2.5GB each. Fortunately we are able to selectively load only the draws from variables we're directly interested in, so we have no problem just working in memory. 

```{r}
#| label: wrangle-draws-2-fac
#| eval: false

# Load the cmdstanr model object
fit <- readRDS("fits/b07.02.rds")

# Load the MCMC draws from the variables we care about
draws <- fit$draws(
  variables = c(
  "lam[1,1]",
  "lam[2,1]",
  "lam[3,1]",
  "lam[4,2]",
  "lam[5,2]",
  "lam[6,2]",
  "Rho[2,1]"
  )
)

# Save the result
saveRDS(draws, "fits/b07.02.samples.rds")

```

Now we can explore the results. One important point is that we can interpret the Stan model's estimates of the correlation coefficient between the two factors directly as covariances (IE analogous to the true parameter simulated with lavaan), because we fixed the factor standard deviations to be equal to 1. 
IE because our sigmas are both equal to 1, this equation:
$\rho(X, Y) = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y}$

Just becomes this:
$\rho(X, Y) = \text{Cov}(X, Y)$

We're interested in the loadings and the between-factor covariance. So we can wrangle the MCMC samples for those variables and plot them to look at the estimated posteriers. Based on this plot it looks like the model did pretty well -- the posterior peak is close to the true value for all of the loadings:

```{r}
#| label: viz-draws-2-fac
#| message: false
#| warnings: false

# Load the draws for the loadings and correlation 
draws <- readRDS("fits/b07.02.samples.rds")

# Plot
draws |>

  # Get the raw draws for the factor loadings and the beween-factor correlation coefficient
  gather_draws(
    `lam[1,1]`,
    `lam[2,1]`,
    `lam[3,1]`,
    `lam[4,2]`,
    `lam[5,2]`,
    `lam[6,2]`,
    `Rho[2,1]`
  ) |>

  # Do some renaming for clarity
  mutate(.variable = case_when(
    .variable == "lam[1,1]" ~ "Loading for m1",
    .variable == "lam[2,1]" ~ "Loading for m2",
    .variable == "lam[3,1]" ~ "Loading for m3",
    .variable == "lam[4,2]" ~ "Loading for m4",
    .variable == "lam[5,2]" ~ "Loading for m5",
    .variable == "lam[6,2]" ~ "Loading for m6",
    .variable == "Rho[2,1]" ~ "Covariance for f1 ~ f2",
  )) |>

  # Add the true factor loadings for plotting
  mutate(true_loading = case_when(
    .variable == "Loading for m1" ~ .8,
    .variable == "Loading for m2" ~ .6,
    .variable == "Loading for m3" ~ .9,
    .variable == "Loading for m4" ~ .1,
    .variable == "Loading for m5" ~ .2,
    .variable == "Loading for m6" ~ -.4,
    .variable == "Covariance for f1 ~ f2" ~ .4
  )) |>

  ggplot(aes(x = .value, fill = .variable)) +
  stat_halfeye(fill = "mediumorchid") +
  geom_vline(aes(xintercept = true_loading), linetype = 2) + 
  scale_x_continuous(expand = c(0, 0.015)) +
  scale_y_continuous(expand = c(0, 0.015)) +
  guides(fill = "none") +
  labs(x = "lab",
     y = NULL)  +
  facet_wrap(~.variable) +
  theme_minimal() + 
  theme(panel.grid.major = element_blank())

```

In this exampe we can feel good about model performance because it successfully recovered the true simulation parameter estimates for the samples of interest. But in a real application where we don't know the true parameter values we would also want to look at the model convergence diagnostics such as rhat and effective number of samples. Here we see that we have rhats slightly greater than 1 for a few of the loadings and for rho. We also see that loading 4, loading 6, and rho have a pretty low number of effective samples. This is cause for concern, and in a real application I would probably increase the number of MCMC iterations to see if we can improve that sample size. The trace plots also look a bit weird at times for a few of the parameters, with some occasional spikes and some parts where one chain goes berzerk relative to the others. But overall they seem to show healthy mixing. 

```{r}
#| label: diagnostics-2-fac
#| message: false
#| warnings: false

# Create a summary table
draws |>

  summary() |>

  select(variable, rhat, ess_bulk, ess_tail) |>

  mutate(across(where(is.numeric), ~round(., 2))) |>

  knitr::kable()

# Look at traceplots
# fit |> 

#  bayesplot::mcmc_trace()

```

Or instead we can go with {{blavaan}}. The model fits way faster than under the raw Stan approach and recovers the true parameter estimates, but is less flexible overall. Crucially for the present analysis, there is no way to do survival analysis with {{blavaan}}. 

But for posterity, below is the syntax we would use to fit the model with {{blavaan}}. If we specify `mcmcfile = TRUE` then the resulting Stan file gets written to a new directory called `lavExport`, or we can supply a filepath as the argument in which case the Stan file will get written there. The file is very large (over 1000 lines) and confusing, I think because it is precompiled to handle any possible model you could specify in R. So it doesn't seem practical to really get in there and customize things directly. The Stan documentation includes [an example for specifying custom priors in a blavaan model](https://mc-stan.org/users/documentation/case-studies/sem.html#Confirmatory_Factor_Analysis_(CFA))

```{r}
#| label: blavaan-2-fac
#| message: false
#| warnings: false
#| eval: false

# Fit the model
blavfit.1 <- bcfa(correlated.factors.model, data=fake_dat, mcmcfile = T)

# Gaze at parameter estimates
blavfit.1 |> summary()

# Make sure the MCMC diagnostics are ok
blavInspect(blavfit.1, "mcobj")

```

## Third Example: MTMM 

In the previous section we moved beyond {{brms}} into raw Stan, which allowed us to estimate the correlation between latent variables. But often in latent variable modelling we also want to account for the possibility that certain measured variables are confounded by unmeasured features. And rather than declaring a new factor for these unmeasured features, it can be simpler to estimate a covariance term for these measured variables in the variance-covariance matrix of the shared multivariable likelihood function. So now the off-diagonals are not entirely 0. This is what @Brown2006 calls a 'correlated uniqueness model'. I have more detailed notes on these ideas in [Chapter -@sec-mtmm]. 

For example, say we have two latent factors each with 3 measurements. We're worried that the first measurement of each variable (m1 and m4) are confounded because they were both collected by participant survey, while the other four measures were each collected by different means. We can simulate data from this DAG using {{lavaan}} like we did for the two models above.

First we'll need to add a new parameter to our Stan model to account for the possibility of shared method confounding. This only involves a few changes, namely:


```stan
data {
  int N; // sample size
  int P; // number of variables
  int D; // number of factors
  array[N] vector[P] X; // data matrix of order [N,P]
  int n_lam; // how many factor loadings are estimated
}
parameters {
  matrix[N, D] FS_UNC; // factor scores, matrix of order [N,D]
  cholesky_factor_corr[D] L_corr_d; // cholesky correlation between factors
  cholesky_factor_corr[2] L_corr_m1_m4; // cholesky correlation between the m1 and m4
  vector<lower=0>[P] sd_p; // residual sd for each variable
  vector<lower=-30, upper=30>[n_lam] lam_UNC; // A bunch of lightly-constrained factor loadings
}
transformed parameters {
  // a vector to hold the factor means, which will be a bunch of 0s
  vector[D] M;
  M = rep_vector(0, D);
  
  // a vector to hold the factor SDs, which will be a bunch of 1s. 
  vector<lower=0>[D] Sd_d;
  Sd_d = rep_vector(1, D);
  
  // A vector to hold the linear predictors for the manifest variables, which are each a deterministic function of loading*factor_score
  array[N] vector[P] mu_UNC;
  for (i in 1 : N) {
    mu_UNC[i, 1] = lam_UNC[1] * FS_UNC[i, 1];
    mu_UNC[i, 2] = lam_UNC[2] * FS_UNC[i, 1];
    mu_UNC[i, 3] = lam_UNC[3] * FS_UNC[i, 1];
    mu_UNC[i, 4] = lam_UNC[4] * FS_UNC[i, 2];
    mu_UNC[i, 5] = lam_UNC[5] * FS_UNC[i, 2];
    mu_UNC[i, 6] = lam_UNC[6] * FS_UNC[i, 2];
  }
  
  // the var-covar matrix for the factors, a deterministic function of the deterministic SDs and the estimated rhos defined in the parameters block
  cholesky_factor_cov[D] L_Sigma;
  L_Sigma = diag_pre_multiply(Sd_d, L_corr_d);

  // Same, but for the var-covar matrix of m1 and m4
  cholesky_factor_cov[D] L_Sigma_m1_m4;
  L_Sigma_m1_m4 = diag_pre_multiply(sd_p[{1, 4}], L_corr_m1_m4);
}
model {
  // Declare some priors
  L_corr_d ~ lkj_corr_cholesky(1); // Prior on factor corrs
  L_corr_m1_m4 ~ lkj_corr_cholesky(1); // Prior on corr between m1 and m4
  lam_UNC ~ normal(0, 10); // Prior on loadings
  sd_p ~ cauchy(0, 2.5); // Prior on residual SDs of manifest variables

  // Set up the joint log likelihood function
  for (i in 1 : N) {
    // The uncorrelated measured variables
    for (j in {2,3,5,6}) {
      // Manifest variables
      X[i, j] ~ normal(mu_UNC[i, j], sd_p[j]);
    }

    // m1 and m4, which get special treatment for being correlated
    to_vector({X[i, 1], X[i, 4]}) ~ multi_normal_cholesky([mu_UNC[i, 1], mu_UNC[i, 4]]', L_Sigma_m1_m4);

    // Latent factors
    FS_UNC[i] ~ multi_normal_cholesky(M, L_Sigma);
  }
}
generated quantities {
  corr_matrix[D] Rho_UNC; /// correlation matrix for factors
  corr_matrix[D] Rho_factors; /// correlation matrix for factors
  corr_matrix[D] Rho_m1_m4; /// correlation matrix for m1 and m4
  matrix[P, D] lam; // factor loadings
  matrix[N, D] FS; // factor scores, matrix of order [N,D]
  
  // Do some fancy things to sign-correct the parameter estimates.
  // The idea seems to be that when we estimate the loadings with unconstrained signs
  // It can lead to identification issues. So we take these steps to correct the signs.
  // See this Stan forum comment for more: https://discourse.mc-stan.org/t/non-convergence-of-latent-variable-model/12450/10
  Rho_UNC = multiply_lower_tri_self_transpose(L_corr_d);
  Rho_factors = Rho_UNC;
  Rho_m1_m4 = multiply_lower_tri_self_transpose(L_corr_m1_m4);
  FS = FS_UNC;
  lam = rep_matrix(0, P, D);
  lam[1 : 3, 1] = to_vector(lam_UNC[1 : 3]);
  lam[4 : 6, 2] = to_vector(lam_UNC[4 : 6]);
  
  // factor 1
  if (lam_UNC[1] < 0) {
    lam[1 : 3, 1] = to_vector(-1 * lam_UNC[1 : 3]);
    FS[ : , 1] = to_vector(-1 * FS_UNC[ : , 1]);
    
    if (lam_UNC[4] > 0) {
      Rho_factors[1, 2] = -1 * Rho_UNC[1, 2];
      Rho_factors[2, 1] = -1 * Rho_UNC[2, 1];
    }
  }
  // factor 2
  if (lam_UNC[4] < 0) {
    lam[4 : 6, 2] = to_vector(-1 * lam_UNC[4 : 6]);
    FS[ : , 2] = to_vector(-1 * FS_UNC[ : , 2]);
    
    if (lam_UNC[1] > 0) {
      Rho_factors[2, 1] = -1 * Rho_UNC[2, 1];
      Rho_factors[1, 2] = -1 * Rho_UNC[1, 2];
    }
  }
}
```

```{r}
#| label: stan-string-mtmm
#| eval: false
#| include: false

stan_model_code <- "
data {
  int N; // sample size
  int P; // number of variables
  int D; // number of factors
  array[N] vector[P] X; // data matrix of order [N,P]
  int n_lam; // how many factor loadings are estimated
}
parameters {
  matrix[N, D] FS_UNC; // factor scores, matrix of order [N,D]
  cholesky_factor_corr[D] L_corr_d; // cholesky correlation between factors
  cholesky_factor_corr[2] L_corr_m1_m4; // cholesky correlation between the m1 and m4
  vector<lower=0>[P] sd_p; // residual sd for each variable
  vector<lower=-30, upper=30>[n_lam] lam_UNC; // A bunch of lightly-constrained factor loadings
}
transformed parameters {
  // a vector to hold the factor means, which will be a bunch of 0s
  vector[D] M;
  M = rep_vector(0, D);
  
  // a vector to hold the factor SDs, which will be a bunch of 1s. 
  vector<lower=0>[D] Sd_d;
  Sd_d = rep_vector(1, D);
  
  // A vector to hold the linear predictors for the manifest variables, which are each a deterministic function of loading*factor_score
  array[N] vector[P] mu_UNC;
  for (i in 1 : N) {
    mu_UNC[i, 1] = lam_UNC[1] * FS_UNC[i, 1];
    mu_UNC[i, 2] = lam_UNC[2] * FS_UNC[i, 1];
    mu_UNC[i, 3] = lam_UNC[3] * FS_UNC[i, 1];
    mu_UNC[i, 4] = lam_UNC[4] * FS_UNC[i, 2];
    mu_UNC[i, 5] = lam_UNC[5] * FS_UNC[i, 2];
    mu_UNC[i, 6] = lam_UNC[6] * FS_UNC[i, 2];
  }
  
  // the var-covar matrix for the factors, a deterministic function of the deterministic SDs and the estimated rhos defined in the parameters block
  cholesky_factor_cov[D] L_Sigma;
  L_Sigma = diag_pre_multiply(Sd_d, L_corr_d);

  // Same, but for the var-covar matrix of m1 and m4
  cholesky_factor_cov[D] L_Sigma_m1_m4;
  L_Sigma_m1_m4 = diag_pre_multiply(sd_p[{1, 4}], L_corr_m1_m4);
}
model {
  // Declare some priors
  L_corr_d ~ lkj_corr_cholesky(1); // Prior on factor corrs
  L_corr_m1_m4 ~ lkj_corr_cholesky(1); // Prior on corr between m1 and m4
  lam_UNC ~ normal(0, 10); // Prior on loadings
  sd_p ~ cauchy(0, 2.5); // Prior on residual SDs of manifest variables

  // Set up the joint log likelihood function
  for (i in 1 : N) {
    // The uncorrelated measured variables
    for (j in {2,3,5,6}) {
      // Manifest variables
      X[i, j] ~ normal(mu_UNC[i, j], sd_p[j]);
    }

    // m1 and m4, which get special treatment for being correlated
    to_vector({X[i, 1], X[i, 4]}) ~ multi_normal_cholesky([mu_UNC[i, 1], mu_UNC[i, 4]]', L_Sigma_m1_m4);

    // Latent factors
    FS_UNC[i] ~ multi_normal_cholesky(M, L_Sigma);
  }
}
generated quantities {
  corr_matrix[D] Rho_UNC; /// correlation matrix for factors
  corr_matrix[D] Rho_factors; /// correlation matrix for factors
  corr_matrix[D] Rho_m1_m4; /// correlation matrix for m1 and m4
  matrix[P, D] lam; // factor loadings
  matrix[N, D] FS; // factor scores, matrix of order [N,D]
  
  // Do some fancy things to sign-correct the parameter estimates.
  // The idea seems to be that when we estimate the loadings with unconstrained signs
  // It can lead to identification issues. So we take these steps to correct the signs.
  // See this Stan forum comment for more: https://discourse.mc-stan.org/t/non-convergence-of-latent-variable-model/12450/10
  Rho_UNC = multiply_lower_tri_self_transpose(L_corr_d);
  Rho_factors = Rho_UNC;
  Rho_m1_m4 = multiply_lower_tri_self_transpose(L_corr_m1_m4);
  FS = FS_UNC;
  lam = rep_matrix(0, P, D);
  lam[1 : 3, 1] = to_vector(lam_UNC[1 : 3]);
  lam[4 : 6, 2] = to_vector(lam_UNC[4 : 6]);
  
  // factor 1
  if (lam_UNC[1] < 0) {
    lam[1 : 3, 1] = to_vector(-1 * lam_UNC[1 : 3]);
    FS[ : , 1] = to_vector(-1 * FS_UNC[ : , 1]);
    
    if (lam_UNC[4] > 0) {
      Rho_factors[1, 2] = -1 * Rho_UNC[1, 2];
      Rho_factors[2, 1] = -1 * Rho_UNC[2, 1];
    }
  }
  // factor 2
  if (lam_UNC[4] < 0) {
    lam[4 : 6, 2] = to_vector(-1 * lam_UNC[4 : 6]);
    FS[ : , 2] = to_vector(-1 * FS_UNC[ : , 2]);
    
    if (lam_UNC[1] > 0) {
      Rho_factors[2, 1] = -1 * Rho_UNC[2, 1];
      Rho_factors[1, 2] = -1 * Rho_UNC[1, 2];
    }
  }
}
"

```

Now we can compile and run the model

```{r}
#| label: stanfit-mtmm
#| eval: false

# Write the raw Stan code from an R string to a Stan file
stan_file <- cmdstanr::write_stan_file(stan_model_code)

# Autoformat the model syntax to preempt any warnings at compile time
model <- cmdstanr::cmdstan_model(stan_file, compile = FALSE)
model$format(canonicalize = TRUE)

# Compile the model from the Stan file
model <- cmdstanr::cmdstan_model(stan_file)

# Declare the new lavaan model where m1 and m4 covary
mtmm.model <- ' 
  f1 =~ .8*m1 + .6*m2 + .9*m3
  f2 =~ .1*m4 + .2*m5 + -.4*m6
  f1 ~~ .4*f2
  m1 ~~ .4*m4
'

# Simulate data from the lavaan model
fake_dat <- lavaan::simulateData(model = mtmm.model, sample.nobs = 4000)

# Put the data into a list format Stan can understand
X <- as.matrix(fake_dat)
P <- ncol(fake_dat) 
N <- nrow(fake_dat)
D <- 2

data_list <- list(N=N,P=P,D=D,X=X, n_lam=6)

# Fit the model
fit <- model$sample(
  data = data_list,
  seed = 199,
  chains = 4,
  parallel_chains = 4,
  iter_warmup = 1000,
  iter_sampling = 6000,
 # adapt_delta=0.99, 
 # max_treedepth = 12,
  refresh = 1 # print update at each iter
)

# Save the resulting model fit
saveRDS(fit, "fits/b07.03.rds")

```

Once again, the resulting object is enormous. So let's pare it down to only contain the draws of substantive interest.
```{r}
#| label: wrangle-draws-mtmm
#| eval: false

# Load the cmdstanr model object
fit <- readRDS("fits/b07.03.rds")

# Load the MCMC draws from the variables we care about
draws <- fit$draws(
  variables = c(
  "lam[1,1]",
  "lam[2,1]",
  "lam[3,1]",
  "lam[4,2]",
  "lam[5,2]",
  "lam[6,2]",
  "Rho_factors[2,1]", 
  "Rho_m1_m4[2,1]"
  )
)

# Save the result
saveRDS(draws, "fits/b07.03.samples.rds")

```

How did that go? We can plot the draws and look at the MCMC diagnostics like we did above.
```{r}

# Load the draws saved in the previous block
draws <- readRDS("fits/b07.03.samples.rds")

# Plot
draws |>

  # Get the raw draws for the factor loadings and the beween-factor correlation coefficient
  gather_draws(
   `lam[1,1]`,
   `lam[2,1]`,
   `lam[3,1]`,
   `lam[4,2]`,
   `lam[5,2]`,
   `lam[6,2]`,
   `Rho_factors[2,1]`,
   `Rho_m1_m4[2,1]`,
  ) |>

  # Do some renaming for clarity
  mutate(.variable = case_when(
    .variable == "lam[1,1]" ~ "Loading for m1",
    .variable == "lam[2,1]" ~ "Loading for m2",
    .variable == "lam[3,1]" ~ "Loading for m3",
    .variable == "lam[4,2]" ~ "Loading for m4",
    .variable == "lam[5,2]" ~ "Loading for m5",
    .variable == "lam[6,2]" ~ "Loading for m6",
    .variable == "Rho_factors[2,1]" ~ "Covariance for f1 ~ f2",
    .variable == "Rho_m1_m4[2,1]" ~ "Covariance for m1 ~ m4"
  )) |>

  # Add the true factor loadings for plotting
  mutate(true_loading = case_when(
    .variable == "Loading for m1" ~ .8,
    .variable == "Loading for m2" ~ .6,
    .variable == "Loading for m3" ~ .9,
    .variable == "Loading for m4" ~ .1,
    .variable == "Loading for m5" ~ .2,
    .variable == "Loading for m6" ~ -.4,
    .variable == "Covariance for f1 ~ f2" ~ .4,
    .variable == "Covariance for m1 ~ m4" ~ .4
  )) |>

  ggplot(aes(x = .value, fill = .variable)) +
  stat_halfeye(fill = "mediumorchid") +
  geom_vline(aes(xintercept = true_loading), linetype = 2) + 
  scale_x_continuous(expand = c(0, 0.015)) +
  scale_y_continuous(expand = c(0, 0.015)) +
  guides(fill = "none") +
  labs(x = "lab",
     y = NULL)  +
  facet_wrap(~.variable) +
  theme_minimal() + 
  theme(panel.grid.major = element_blank())
```


## Fourth Example: Survival Analysis

Now that we're confident our measurement model for the correlated factors is well-specified, we can move on to modelling the outcome of interest: time to employment. The updated model is shown below. The only change from the MTMM model above is the two top lines: now we imagine time-to-event $T$ for each person $i$ to be drawn from a shared Weibull distribution, whose location parameter is a linear model of the latent factors $f_1$ and $f_2$, as well as other measured variables in the matrix $\mathbf{X}$, each with its own coefficient in the vector $\boldsymbol{\beta}$. 

$$
\begin{aligned}
T_i &\sim \text{Weibull}(\theta_i, k) \\
\theta_i &= \alpha + \beta_1 f_{1i} + \beta_2 f_{2i} + \mathbf{X}_i \boldsymbol{\beta} \\
\\
\begin{bmatrix}
m_{1} \\
m_{2} \\
m_{3} \\
m_{4} \\
m_{5} \\
m_{6}
\end{bmatrix}
&\sim \text{MVNormal}
\left(
\begin{bmatrix}
\mu_{m1} \\
\mu_{m2} \\
\mu_{m3} \\
\mu_{m4} \\
\mu_{m5} \\
\mu_{m6}
\end{bmatrix},
\Sigma_m
\right) \\
\\
\mu_{m1} &= \lambda_1 f_1, \quad \mu_{m2} = \lambda_2 f_1, \quad \mu_{m3} = \lambda_3 f_1 \\
\mu_{m4} &= \lambda_4 f_2, \quad \mu_{m5} = \lambda_5 f_2, \quad \mu_{m6} = \lambda_6 f_2 \\
\\
\begin{bmatrix}
f_{1} \\
f_{2}
\end{bmatrix}
&\sim \text{MVNormal}
\left(
\begin{bmatrix}
\mu_1 \\
\mu_2
\end{bmatrix},
\Sigma_f
\right) \\
\\
\Sigma_f &=
\begin{bmatrix}
\sigma_{f1}^2 & \rho \sigma_{f1} \sigma_{f2} \\
\rho \sigma_{f1} \sigma_{f2} & \sigma_{f2}^2
\end{bmatrix} \\
\\
\Sigma_m &=
\begin{bmatrix}
\sigma_{m1}^2 & 0 & 0 & \rho \sigma_{m1} \sigma_{m4} & 0 & 0 \\
0 & \sigma_{m2}^2 & 0 & 0 & 0 & 0 \\
0 & 0 & \sigma_{m3}^2 & 0 & 0 & 0 \\
\rho \sigma_{m1} \sigma_{m4} & 0 & 0 & \sigma_{m4}^2 & 0 & 0 \\
0 & 0 & 0 & 0 & \sigma_{m5}^2 & 0 \\
0 & 0 & 0 & 0 & 0 & \sigma_{m6}^2
\end{bmatrix}
\end{aligned}

$$

Why a Weibull likelihood? According to @Collett2003, this is a common choice for parameteric survival analysis because it corresponds to a flexible monotonic baseline hazard function, which often feels like a safe assumption that is neither too rigid (like an exponential likelihood and corresponding flat baseline hazard function) nor too flexible (like a spline with many knots). It also has the advantage of being interpretable as either a Proportional Hazards regression or as an Accelerated Failure Time regression, which gives us flexibility in how we communicate about the parameter estimates, and allows us to not worry too much about the proportional hazards assumption. 

### Simulating data

For this final section we won't be able to rely on `lavaan::simulateData()` to create our simulated data, because it can't generate time-to-event data. So we'll need to generate the data ourselves. [Mark Lai](https://scholar.google.com/citations?user=s2LhwXAAAAAJ&hl=en) has shared [some nice code](https://bookdown.org/marklhc/notes/simulation-example-on-structural-equation-modeling-sem.html#full-example-of-a-small-scale-simulation) to do this, so we can take his approach, adapting it slightly allow for covariance between m1 and m4 as in the MTMM model above. For now we'll simulate the measured variables as continuous and gaussian, even though it is more realistic that in this context they would be ordinal likert-style. 

```{r}

# Define sample size
N <- 4000

# Define the Fixed Parameters
alpha <- c(0, 0)  # latent means

# latent variances/covariances
Phi <- matrix(c(1, 0.4, 
                0.4, 1), nrow = 2)  

# factor loadings
Lambda <- cbind(c(.8, .6, .9, 0, 0, 0), 
                c(0, 0, 0, .1, .2, -.4))  

# Error structure of measured variables
Theta <- diag(c(c(0.5, .2, .2, .3, .4, .5)))
Theta[4, 1] <- .38
Theta[1, 4] <- .38

# Generate latent factor scores
eta <- MASS::mvrnorm(N, mu = alpha, Sigma = Phi)

# Generate residuals
e <- MASS::mvrnorm(N, mu = rep(0, 6), Sigma = Theta)

# Compute outcome scores: m_i = t(Lambda %*% eta_i) + e
m <- tcrossprod(eta, Lambda) + e

# Pack the measured variables and factor scores into a dataset
fake_dat <- tibble(
  m1 = m[,1],
  m2 = m[,2],
  m3 = m[,3],
  m4 = m[,4],
  m5 = m[,5],
  m6 = m[,6],
  f1 = eta[,1],
  f2 = eta[,2]
)

```

Now we have our 'measured' variables, simulated according to our chosen factor loadings and error structure. Just to be safe, let's make sure lavaan can recover the true parameter estimates. We need to specify `std.lv = TRUE` to override lavaan's default behaviour of setting the first loading of each factor to 1 and instead fix the variances of the latent variables to 1, or else it doesn't return the true simulated parameter estimates because the scales of the latent variables get messed up. Looks like everything works fine:

```{r}

surv.measurement.model <- ' 
  f1 =~ m1 + m2 + m3
  f2 =~ m4 + m5 + m6

  f1 ~~ f1
  f2 ~~ f2

  m1 ~~ m4
'

# Fit the model
fit_test <- cfa(surv.measurement.model, data = fake_dat |> select(starts_with("m")), std.lv = TRUE)

# Make sure it works
fit_test |> 

  broom::tidy() |>

  select(term, estimate) |>

  knitr::kable()

```

Now that we have the factor scores and their measured manifestations, we can simulate the demographic variables and time-to-event data and add them to the dataset. We can draw the covariates straightforwardly from the relevent distributions, but the times-to-event will require a bit more thought. There are existing packages for simulating survival data in R (such as [simsurv](https://cran.r-project.org/web/packages/simsurv/vignettes/simsurv_usage.html) by Sam Brilleman of rstanarm fame!) but since we're taking an explicit approach for the latent variables this time, it's nice to continue in that fashion for the entire dataset. I found [this Stack Exchange answer](https://stats.stackexchange.com/a/472260/337075) by user [Ryan SY Kwan](https://stats.stackexchange.com/users/206635/ryan-sy-kwan) helpful for learning an explicit workflow for simulating survival data from a Weibull distribution, which I implement here. 

The goal is to find a way of expressing the Weibull distribution's shape parameter with reference to the linear model, so that we can use R's built-in `rweibull()` function to sample our event times. We can start from [the PDF of the Weibull distribution](https://en.wikipedia.org/wiki/Weibull_distribution):

$$

\begin{equation}
f(x; k, \lambda) = \begin{cases}
\frac{k}{\lambda} \left( \frac{x}{\lambda} \right)^{k-1} e^{-(x/\lambda)^{k}} & x > 0, \\
0 & x \leq 0.
\end{cases}
\end{equation}

$$

And its cumulative density function is given by:

$$

\begin{equation}
F(x; k, \lambda) = 1 - \exp\left(-\left(\frac{x}{\lambda}\right)^{k}\right)
\end{equation}

$$

So the survival curve, given by 1 - CDF, is:

$$

\begin{equation}
S(x; k, \lambda) = \exp\left(-\left(\frac{x}{\lambda}\right)^{k}\right)
\end{equation}

$$

And using the classic negative-log relationship between the survival function and the cumulative hazard function, we can say:

$$

\begin{equation}
H(t) = -\log\left(\exp\left(-\left(\frac{t}{\lambda}\right)^{\rho}\right)\right)
\end{equation}

\\[1em]

\begin{equation}
H(t) = \left(\frac{t}{\lambda}\right)^{\rho}
\end{equation}

$$

Now we have everything we need to implement the the classic form of a Cox Proportional Hazards regression, given by:
$$

\begin{equation}
H(t | \mathbf{X}) = H_0(t) \exp(\mathbf{\beta}' \mathbf{X})
\end{equation}

$$

Since this is directly equivalent to the model for the actual function of interest, the baseline hazard. As @Singer+Willett2003 put it, _"This direct equivalence may not be intuitive, but it certainly is invaluable."_

$$

\begin{equation}
h(t | \mathbf{X}) = h_0(t) \exp(\mathbf{\beta}' \mathbf{X})
\end{equation}

$$

So we can take the definition of the Weibull cumulative hazard function we found above and substitute it into this equation, then take its negative exp to get it back in terms of the survival function, which is what we actually want to be sampling event times from:

$$

\begin{equation}
S(t \mid \mathbf{x}) = \exp\left(-\left(\frac{t}{\lambda}\right)^{\rho} \exp(\mathbf{\beta}' \mathbf{x})\right)
\end{equation}

$$

Now we can rearrange things to make the scale parameter $\lambda$ itself a function of the linear model. This allows us to use R's built-in `rweibull()` function to generate the event times. All we need is some fancy algebra:

$$

\begin{align*}
S(t \mid x, \lambda) &= \exp\left(-\left(\frac{t}{\lambda}\right)^{\rho} \exp(x' \beta)\right) \\
&= \exp\left(-\left(\frac{t}{\lambda}\right)^{\rho} \exp\left(\frac{x' \beta}{\rho}\right)\rho\right) \\
&= \exp\left(-\left(\frac{t}{\frac{\lambda}{\exp\left(\frac{x' \beta}{\rho}\right)}}\right)^{\rho}\right) \\
\end{align*}

$$

Now the denominator incorporates both $\lambda$ and the linear model. So we can just take the whole denominator and call it its own new thing $\lambda'$:

$$

\begin{align*}
\lambda' = \frac{\lambda}{\exp\left(\frac{x' \beta}{\rho}\right)}
\end{align*}

$$

So now we've snuck our linear model into the classic no-model definition of the Weibull survival function we saw above, just with $\lambda'$ instead of $\lambda$:

$$

\begin{equation}
S(x; k, \lambda) = \exp\left(-\left(\frac{x}{\lambda'}\right)^{k}\right)
\end{equation}

$$

This is the parameterization we can implement when we go to create our event times. We'll also imagine censoring times to be drawn from an unconditional exponential distribution, which creates non-informative censoring. We'll generate an event time and a censoring time for each person, and a person's status will be the shorter of those two times. 

Note for choosing the simulated parameter estimates for the linear model: there's some tricky conversion we need to do to interpret our parametric estimates when we use the survival function, because the Weibull (and exponential, which is a special case of Weibull) results have the magic property of being able to be interpreted either as hazard ratios *or* acceleration factors. [This nice markdown document](https://rstudio-pubs-static.s3.amazonaws.com/5564_bc9e2d9a458c4660aa82882df90b7a6b.html) provides a walkthrough of the transformations required to move between these two interpretations.

```{r}

# Declare some parameter values for baseline hazard
lambda <- 1
rho <- 1
lambda_wiki <- lambda^(-1 / rho) 

# Declare some true linear model parameters
beta_f1 <- 1
beta_f2 <- .5
beta_age <- .8
beta_ei <- .3

# Declare the rate for the exponential distribution from which we will sample censoring times
censor_rate = .02

# Simulate the covariates and event times
fake_dat <- fake_dat |>

  rowid_to_column() |>

  mutate(

    # Simulate the covariates
    age = rnorm(N), 
    ei_status = rbinom(N, 1, .4),
    n_previous_programs = sample(c(1, 2, 3, 4), N, prob = c(.5, .3, .15, .05), replace = TRUE)

  ) |>

  group_by(rowid) |>

  mutate(

    # Simulate the event times and censoring times
    lambda_prime = lambda_wiki / exp((beta_f1*f1 + beta_f2*f2 + beta_age*age + beta_ei*ei_status) / rho),
    time_event = rweibull(1, shape=rho, scale=lambda_prime),
    time_censor = rexp(1, censor_rate),
    time = min(time_event, time_censor),
    censored = ifelse(time_censor <= time_event, 1, 0),
    status = ifelse(time_censor > time_event, 1, 0)

  ) |>

  ungroup() |>

  # Remove the intermediary variables we don't need
  select(-c(
    lambda_prime,
    time_event,
    time_censor
  ))

```

If we've done everything correctly then a basic survival analysis should be able to recover the true parameter estimates:

```{r}

test <- survreg(Surv(time, status) ~ f1 + f2 + age + ei_status, data=fake_dat, 
dist='weibull')

test |> 

  broom::tidy() |> 

  knitr::kable()

 # mutate(estimate = exp(estimate))

```

Yes!

### Prior Predictive Distributions

Check priors for the betas for f1 and f2, and for the sigmas. 


## Multilevel Survival Analysis

Let's say like 30 clusters. How to simulate?