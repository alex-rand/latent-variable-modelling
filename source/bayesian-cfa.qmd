---
title: "Bayesian CFA"
format:
  html:
    theme: default
---

```{r message = FALSE}
library(tidyverse)
library(lavaan)
library(blavaan)
library(brms)
library(tidybayes)

# MCMC specifications
options(brms.backend = "cmdstanr")
options(mc.cores = parallel::detectCores())
```

Can brms replicate results from lavaan?

{{brms}} doesn't officially have SEM capabilities [(but they do seem to be coming soon!)](https://github.com/paul-buerkner/brms/issues/304). But [STAN forum contributer Jack Bailey](https://discourse.mc-stan.org/t/confirmatory-factor-analysis-using-brms/23139) has 'patant√©' a solution using artful prior specifications.

To demonstrate, we can simulate some data using {{lavaan}} and show that `brms::brm()` is able to recover the 'true' factor loadings.

## First Example: Simple Factor Structure

First we can simulate some data with a simple factor structure using lavaan's handy `simulateData()` function:
```{r message = FALSE, warning = FALSE}

pop.model <- ' 
  f1 =~ .8*m1 + .1*m2 + .6*m3 + .2*m4 + .9*m5 + -.4*m6
'

# Simulate data from the model
fake_dat <- lavaan::simulateData(model = pop.model, sample.nobs = 4000)

# Visualize the measured dat
fake_dat |>

  select(m1, m2, m3, m4, m5, m6) |>
  
  pivot_longer(everything(), names_to = "var", values_to = "measurement") |>

  ggplot() +
  geom_histogram(aes(x = measurement)) +
  facet_wrap(~var) +
  theme_bw()

```

Unsurprisingly, the lavaan `cfa()` function is able to recover the true parameters for the data it generated.

```{r}

# Fit the model
fit_1 <- cfa(pop.model, data = fake_dat)

# Make sure it works
fit_1 |> broom::tidy()

```

But can brms recover the true parameter values? I like this approach because it gives a nice conceptual perspective on what we're actually doing when we're doing latant variable modelling: we're just doing a linear regression where the measured variables are all drawn from a sharedd multivariate normal distribution, and where the linear model component of their location parameters include a covariate for which we only have missing data. It's a pretty cool and weird thing.

```{r eval = FALSE}

# Add a latent variable to the dataset
fake_dat$f1 <- NA_real_

bfit.1 <- brm(
    formula =
      bf(m1 ~ 0 + mi(f1)) +
      bf(m2 ~ 0 + mi(f1)) +
      bf(m3 ~ 0 + mi(f1)) +
      bf(m4 ~ 0 + mi(f1)) +
      bf(m5 ~ 0 + mi(f1)) +
      bf(m6 ~ 0 + mi(f1)) +
      bf(f1| mi() ~ 1) + 
      set_rescor(rescor = FALSE),
    family = gaussian(),
    prior =
      prior(constant(1), class = "b", resp = "m1") +
      prior(constant(1), class = "sigma", resp = "m1") +
      prior(normal(0, 10), class = "b", resp = "m2") +
      prior(constant(1), class = "sigma", resp = "m2") +
      prior(normal(0, 10), class = "b", resp = "m3") +
      prior(constant(1), class = "sigma", resp = "m3") +
      prior(normal(0, 10), class = "b", resp = "m4") +
      prior(constant(1), class = "sigma", resp = "m4") +
      prior(normal(0, 10), class = "b", resp = "m5") +
      prior(constant(1), class = "sigma", resp = "m5") +
      prior(normal(0, 10), class = "b", resp = "m6") +
      prior(constant(1), class = "sigma", resp = "m6") +
      prior(normal(0, 10), class = "Intercept", resp = "f1") +
      prior(cauchy(0, 1), class = "sigma", resp = "f1"),
    data = fake_dat,
    warmup = 1000,
    iter = 6000,
    file = "fits/b07.01.rds"
  )

```

A Bayesian missing data model treats each missing observation as a parameter to estimate. And since the factor is missing in each row of the dataset, this means we end up with 6000 samples for each row of data we have, resulting in a pretty big model file. This is too big for Github, so I'll only push the draws we need to replicate the figure below.

First we can put the draws we need into a tidy format and do some cleaning:

```{r eval = FALSE}

bfit.1.samples <- bfit.1 |>

  # Get the raw MCMC samples
  gather_draws(
    bsp_m2_mif1,
    bsp_m3_mif1,
    bsp_m4_mif1,
    bsp_m5_mif1,
    bsp_m6_mif1
  ) |>

  # Do some renaming for clarity
  mutate(.variable = case_when(
    .variable == "bsp_m2_mif1" ~ "Loading for m2",
    .variable == "bsp_m3_mif1" ~ "Loading for m3",
    .variable == "bsp_m4_mif1" ~ "Loading for m4",
    .variable == "bsp_m5_mif1" ~ "Loading for m5",
    .variable == "bsp_m6_mif1" ~ "Loading for m6"
  )) |> 

  # Add the true factor loadings from above
  mutate(true_loading = case_when(
    .variable == "Loading for m2" ~ .1,
    .variable == "Loading for m3" ~ .6,
    .variable == "Loading for m4" ~ .2,
    .variable == "Loading for m5" ~ .9,
    .variable == "Loading for m6" ~ -.4
  )) 

# Save the tidy draws for reproducibility
saveRDS(bfit.1.samples, "fits/b07.01.samples.rds")
```

Now we can plot. Looks like STAN does an OK job at recovering the true factor loadings (we exclude the first loading since its loading was held constant at 1 to provide the scale for the latent variable):
```{r}
#| fig-cap: "Posterior distributions of estimated factor loadings for measured variables 2-6. The dashed line is the 'true' simulated loading for each variable."
#| fig-alt: "Posterior distributions of estimated factor loadings for measured variables 2-6. The dashed line is the 'true' simulated loading for each variable."

# Load the tidy samples
bfit.1.samples <- readRDS("source/fits/b07.01.samples.rds")

# Plot
bfit.1.samples|>

  ggplot(aes(x = .value, fill = .variable)) +
    stat_halfeye(fill = "mediumorchid") +
    geom_vline(aes(xintercept = true_loading), linetype = 2) + 
    scale_x_continuous(expand = c(0, 0.015)) +
    scale_y_continuous(expand = c(0, 0.015)) +
    guides(fill = "none") +
    labs(x = "lab",
       y = NULL)  +
    facet_wrap(~.variable) +
    theme_minimal() + 
    theme(panel.grid.major = element_blank())
  
```

In all cases the model is __pretty close__ to the true parameter value. It's a bit concerning when the standard errors are so small they fail to capture the true values, but this illustrates the general proof of concept: we can do a pretty good CFA in {{brms}}.

## Second Example: Correlated factors

Now an example where we assume the latent vactors are themselves dependent, so that discriminant validity is questionable. So we can include a term for their correlation structure in the model. We'll need to amend the lavaan model structure to implement these changes.

```{r}

# Declare the new model with correlated factors
mtmm.model <- ' 
  f1 =~ .8*m1 + .6*m2 + .9*m3
  f2 =~ .1*m4 + .2*m5 + -.4*m6
  f1 ~~ .4*f2
'

# Simulate data from the model
fake_dat <- lavaan::simulateData(model = mtmm.model, sample.nobs = 4000)

```

The complication here is that brms doesn't currently support setting constraints on individual correlation terms in the model, even in the form of constant priors. [There seems to be a plan](https://github.com/paul-buerkner/brms/issues/957) to include more SEM-like flexibility of covariance matrixes in general in brms 3.0, but for now we'll need to specify the model using raw Stan code. There seem to be a few strategies for avoiding convergence issues when fitting this kind of model in Stan. I worked closely from the strategy provided in [this Stan forum comment](https://discourse.mc-stan.org/t/non-convergence-of-latent-variable-model/12450/13) by Mauricio Garnier-Villarre, making a few minor adjustments such as removing latent intercepts from the linear predictors to better align this model with the {{lavaan}} defaults. 

Here is the raw Stan code:

```{r eval = FALSE}

stan_model_code <- "
data {
  int N; // sample size
  int P; // number of variables
  int D; // number of factors
  array[N] vector[P] X; // data matrix of order [N,P]
  int n_lam; // how many factor loadings are estimated
}
parameters {
  matrix[N, D] FS_UNC; // factor scores, matrix of order [N,D]
  cholesky_factor_corr[D] L_corr_d; // cholesky correlation between factors
  vector<lower=0>[P] sd_p; // residual sd for each variable
  vector<lower=-30, upper=30>[n_lam] lam_UNC; // A bunch of lightly-constrained factor loadings
}
transformed parameters {
  // a vector to hold the factor means, which will be a bunch of 0s
  vector[D] M;
  M = rep_vector(0, D);
  
  // a vector to hold the factor SDs, which will be a bunch of 1s. 
  vector<lower=0>[D] Sd_d;
  Sd_d = rep_vector(1, D);
  
  // A vector to hold the linear predictors for the manifest variables, which are each a deterministic function of loading*factor_score
  array[N] vector[P] mu_UNC;
  for (i in 1 : N) {
    mu_UNC[i, 1] = lam_UNC[1] * FS_UNC[i, 1];
    mu_UNC[i, 2] = lam_UNC[2] * FS_UNC[i, 1];
    mu_UNC[i, 3] = lam_UNC[3] * FS_UNC[i, 1];
    mu_UNC[i, 4] = lam_UNC[4] * FS_UNC[i, 2];
    mu_UNC[i, 5] = lam_UNC[5] * FS_UNC[i, 2];
    mu_UNC[i, 6] = lam_UNC[6] * FS_UNC[i, 2];
  }
  
  // the var-covar matrix for the factors, a deterministic function of the deterministic SDs and the estimated rhos defined in parameters{}
  cholesky_factor_cov[D] L_Sigma;
  L_Sigma = diag_pre_multiply(Sd_d, L_corr_d);
}
model {
  // Declare some priors
  L_corr_d ~ lkj_corr_cholesky(1); // Prior on factor corrs
  lam_UNC ~ normal(0, 10); // Prior on loadings
  sd_p ~ cauchy(0, 2.5); // Prior on residual SDs of manifest variables
  
  // Set up the likelihoods of the manifest and latent factors
  for (i in 1 : N) {
    for (j in 1 : P) {
      // Manifest variables
      X[i, j] ~ normal(mu_UNC[i, j], sd_p[j]);
    }
    // Latent factors
    FS_UNC[i] ~ multi_normal_cholesky(M, L_Sigma);
  }
}
generated quantities {
  array[N] real log_lik; // log likelihood of each datapoint
  real dev; // global deviance
  real log_lik0; // global log likelihood
  vector[N] log_lik_row;
  
  cov_matrix[P] Sigma; // Covariance matrix of the manifest variables
  cov_matrix[D] Phi_lv; // Covariance matrix of the latent factors
  matrix[P, P] lambda_phi_lambda; // I think these are the terms of the reconstructed var-covar matrix of the data?
  cholesky_factor_cov[P] L_Sigma_model; // I think this is the cholesky decomposition of the reconstructued var-covar matrix above?
  matrix[P, P] theta_del; // I think this is Matrix containing the error terms for the reconstructed empirical var-covar matrix?
  
  corr_matrix[D] Rho_UNC; /// correlation matrix
  corr_matrix[D] Rho; /// correlation matrix
  matrix[P, D] lam; // factor loadings
  matrix[N, D] FS; // factor scores, matrix of order [N,D]
  
  // Do some fancy things to sign-correct the parameter estimates.
  // The idea seems to be that when we estimate the loadings with unconstrained signs
  // It can lead to identification issues. So we take these steps to correct the signs.
  // See this Stan forum comment for more: https://discourse.mc-stan.org/t/non-convergence-of-latent-variable-model/12450/10?u=alex.b.r
  Rho_UNC = multiply_lower_tri_self_transpose(L_corr_d);
  Rho = Rho_UNC;
  FS = FS_UNC;
  lam = rep_matrix(0, P, D);
  lam[1 : 3, 1] = to_vector(lam_UNC[1 : 3]);
  lam[4 : 6, 2] = to_vector(lam_UNC[4 : 6]);
  
  // factor 1
  if (lam_UNC[1] < 0) {
    lam[1 : 3, 1] = to_vector(-1 * lam_UNC[1 : 3]);
    FS[ : , 1] = to_vector(-1 * FS_UNC[ : , 1]);
    
    if (lam_UNC[4] > 0) {
      Rho[1, 2] = -1 * Rho_UNC[1, 2];
      Rho[2, 1] = -1 * Rho_UNC[2, 1];
    }
  }
  // factor 2
  if (lam_UNC[4] < 0) {
    lam[4 : 6, 2] = to_vector(-1 * lam_UNC[4 : 6]);
    FS[ : , 2] = to_vector(-1 * FS_UNC[ : , 2]);
    
    if (lam_UNC[1] > 0) {
      Rho[2, 1] = -1 * Rho_UNC[2, 1];
      Rho[1, 2] = -1 * Rho_UNC[1, 2];
    }
  }
  
  /// marginal log-likelihood based on signed corrected parameters
  Phi_lv = quad_form_diag(Rho, Sd_d);
  lambda_phi_lambda = quad_form_sym(Phi_lv, transpose(lam));
  theta_del = diag_matrix(sd_p);
  
  Sigma = lambda_phi_lambda + theta_del;
  L_Sigma_model = cholesky_decompose(Sigma);
  
  for (i in 1 : N) {
    log_lik[i] = multi_normal_cholesky_lpdf(X[i] | rep_vector(0, P), L_Sigma_model);
  }
  
  log_lik0 = sum(log_lik); // global log-likelihood
  dev = -2 * log_lik0; // model deviance
}
"

```

Now we can compile and run the model

```{r eval = FALSE}

# Write the raw code above from an R string to a Stan file
stan_file <- cmdstanr::write_stan_file(stan_model_code)

# Autoformat the model syntax to preempt any warnings at compile time
model <- cmdstanr::cmdstan_model(stan_file, compile = FALSE)
model$format(canonicalize = TRUE)

# Compile the model from the Stan file
model <- cmdstanr::cmdstan_model(stan_file)

# Put the data into a list format Stan can understand
X <- as.matrix(fake_dat)
P <- ncol(fake_dat) 
N <- nrow(fake_dat)
D <- 2

data_list <- list(N=N,P=P,D=D,X=X, n_lam=6)
param <- c("lam","Rho","sd_p","M","Sd_d",
           "log_lik0","dev","log_lik")

# Fit the model
fit <- model$sample(
  data = data_list,
  seed = 199,
  chains = 4,
  parallel_chains = 4,
  iter_warmup = 1000,
  iter_sampling = 6000,
  adapt_delta=0.99, 
  max_treedepth = 12,
  refresh = 1 # print update every 500 iters
)

# Save the resulting model fit
saveRDS(fit, "source/fits/b07.02b.rds")


```

The model took about 90 minutes to sample and the resulting files are enormous -- on the order of 2.5GB each. Fortunately we are able to selectively load only the draws from variables we're directly interested in, so we have no problem just working in memory. 

```{r eval = FALSE}

# Load the cmdstanr model object
fit <- readRDS("source/fits/b07.02.rds")

# Load the MCMC draws from the variables we care about
test <- fit$draws(
  variables = c(
  "lam[1,1]",
  "lam[2,1]",
  "lam[3,1]",
  "lam[4,2]",
  "lam[5,2]",
  "lam[6,2]",
  "Rho[2,1]"
  )
)

# Save the result
saveRDS(test, "source/fits/b07.02b.samples.rds")

```

Now we can explore the results. One important point is that we can interpret the Stan model's estimates of the correlation coefficient between the two factors directly as covariances (IE analogous to the true parameter simulated with lavaan), because we fixed the factor standard deviations to be equal to 1. 
IE because our sigmas are both equal to 1, this equation:
$\rho(X, Y) = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y}$

Just becomes this:
$\rho(X, Y) = \frac{\text{Cov}(X, Y)}$

We're interested in the loadings and the between-factor covariance. So let's wrangle the MCMC samples for those variables and plot them to look at the estimated posteriers. We can also check convergence diagnostics to get a sense of whether things went smoothly. 

```{r}

fit <- readRDS("source/fits/b07.02b.samples.rds")

fit |> summary()
fit |>

  # Get the raw draws for the factor loadings and the beween-factor correlation coefficient
  gather_draws(
    `lam[1,1]`,
    `lam[2,1]`,
    `lam[3,1]`,
    `lam[4,2]`,
    `lam[5,2]`,
    `lam[6,2]`,
    `Rho[2,1]`
  ) |>

  # Do some renaming for clarity
  mutate(.variable = case_when(
    .variable == "lam[1,1]" ~ "Loading for m1",
    .variable == "lam[2,1]" ~ "Loading for m2",
    .variable == "lam[3,1]" ~ "Loading for m3",
    .variable == "lam[4,2]" ~ "Loading for m4",
    .variable == "lam[5,2]" ~ "Loading for m5",
    .variable == "lam[6,2]" ~ "Loading for m6",
    .variable == "Rho[2,1]" ~ "Covariance for f1 ~ f2",
  )) |>

  # Add the true factor loadings for plotting
  mutate(true_loading = case_when(
    .variable == "Loading for m1" ~ .8,
    .variable == "Loading for m2" ~ .6,
    .variable == "Loading for m3" ~ .9,
    .variable == "Loading for m4" ~ .1,
    .variable == "Loading for m5" ~ .2,
    .variable == "Loading for m6" ~ -.4,
    .variable == "Covariance for f1 ~ f2" ~ .4
  )) |>

  ggplot(aes(x = .value, fill = .variable)) +
  stat_halfeye(fill = "mediumorchid") +
  geom_vline(aes(xintercept = true_loading), linetype = 2) + 
  scale_x_continuous(expand = c(0, 0.015)) +
  scale_y_continuous(expand = c(0, 0.015)) +
  guides(fill = "none") +
  labs(x = "lab",
     y = NULL)  +
  facet_wrap(~.variable) +
  theme_minimal() + 
  theme(panel.grid.major = element_blank())

```

Or instead we can go with {{blavaan}}. The model fits way faster than under the {{brms}} approach and recovers the true parameter estimates. If we specify `mcmcfile = TRUE` then the resulting Stan file gets written to a new directory called `lavExport`, or we can supply a filepath as the argument in which case the Stan file will get written there. The file is very large (over 1000 lines) and confusing, I think because it is precompiled to handle any possible model you could specify in R. So it doesn't seem practical to really get in there and customize things directly. 

The Stan documentation includes [an example for specifying custom priors in a blavaan model](https://mc-stan.org/users/documentation/case-studies/sem.html#Confirmatory_Factor_Analysis_(CFA))

```{r eval = FALSE}
 
# Fit the model
blavfit.1 <- bcfa(mtmm.model, data=fake_dat, mcmcfile = T)

# Gaze at parameter estimates
blavfit.1 |> summary()

# Make sure the MCMC diagnostics are ok
blavInspect(blavfit.1, "mcobj")

```


ANOTHER TEST, copying [this example here](https://discourse.mc-stan.org/t/non-convergence-of-latent-variable-model/12450/13). This approach uses the built-in `cholesky_factor_corr[]` Stan function to set up the correlation matrix for the factors. We just make a few examples to the code given, namely:
- Remove the intercepts from the linear models of the manifest variables, to align our model with the lavaan default.
```{r}


```
## Third Example: MTMM 