---
title: "Bayesian CFA"
format:
  html:
    theme: default
---

```{r message = FALSE}
library(tidyverse)
library(lavaan)
library(blavaan)
library(brms)
library(tidybayes)

# brms backend specifications
options(brms.backend = "cmdstanr")
options(mc.cores = parallel::detectCores())
```

Can brms replicate results from lavaan?

{{brms}} doesn't officially have SEM capabilities [(but they do seem to be coming soon!)](https://github.com/paul-buerkner/brms/issues/304). But [STAN forum contributer Jack Bailey](https://discourse.mc-stan.org/t/confirmatory-factor-analysis-using-brms/23139) has 'patant√©' a solution using artful prior specifications.

To demonstrate, we can simulate some data using {{lavaan}} and show that `brms::brm()` is able to recover the 'true' factor loadings.

## First Example: Simple Factor Structure

First we can simulate some data with a simple factor structure using lavaan's handy `simulateData()` function:
```{r message = FALSE, warning = FALSE}

pop.model <- ' 
  f1 =~ .8*m1 + .1*m2 + .6*m3 + .2*m4 + .9*m5 + -.4*m6
'

# Simulate data from the model
fake_dat <- lavaan::simulateData(model = pop.model, sample.nobs = 4000)

# Visualize the measured dat
fake_dat |>

  select(m1, m2, m3, m4, m5, m6) |>
  
  pivot_longer(everything(), names_to = "var", values_to = "measurement") |>

  ggplot() +
  geom_histogram(aes(x = measurement)) +
  facet_wrap(~var) +
  theme_bw()

```

Unsurprisingly, the lavaan `cfa()` function is able to recover the true parameters for the data it generated.

```{r}

# Fit the model
fit_1 <- cfa(pop.model, data = fake_dat)

# Make sure it works
fit_1 |> broom::tidy()

```

But can brms recover the true parameter values?

```{r eval = FALSE}

# Add a latent variable to the dataset
fake_dat$f1 <- NA_real_

bfit.1 <- brm(
    formula =
      bf(m1 ~ 0 + mi(f1)) +
      bf(m2 ~ 0 + mi(f1)) +
      bf(m3 ~ 0 + mi(f1)) +
      bf(m4 ~ 0 + mi(f1)) +
      bf(m5 ~ 0 + mi(f1)) +
      bf(m6 ~ 0 + mi(f1)) +
      bf(f1| mi() ~ 1) + 
      set_rescor(rescor = FALSE),
    family = gaussian(),
    prior =
      prior(constant(1), class = "b", resp = "m1") +
      prior(constant(1), class = "sigma", resp = "m1") +
      prior(normal(0, 10), class = "b", resp = "m2") +
      prior(constant(1), class = "sigma", resp = "m2") +
      prior(normal(0, 10), class = "b", resp = "m3") +
      prior(constant(1), class = "sigma", resp = "m3") +
      prior(normal(0, 10), class = "b", resp = "m4") +
      prior(constant(1), class = "sigma", resp = "m4") +
      prior(normal(0, 10), class = "b", resp = "m5") +
      prior(constant(1), class = "sigma", resp = "m5") +
      prior(normal(0, 10), class = "b", resp = "m6") +
      prior(constant(1), class = "sigma", resp = "m6") +
      prior(normal(0, 10), class = "Intercept", resp = "f1") +
      prior(cauchy(0, 1), class = "sigma", resp = "f1"),
    data = fake_dat,
    warmup = 1000,
    iter = 6000,
    file = "fits/b07.01.rds"
  )

```

A Bayesian missing data model treats each missing observation as a parameter to estimate. And since the factor is missing in each row of the dataset, this means we end up with 6000 samples for each row of data we have, resulting in a pretty big model file. This is too big for Github, so I'll only push the draws we need to replicate the figure below.

First we can put the draws we need into a tidy format and do some cleaning:

```{r eval = FALSE}

bfit.1.samples <- bfit.1 |>

  # Get the raw MCMC samples
  gather_draws(
    bsp_m2_mif1,
    bsp_m3_mif1,
    bsp_m4_mif1,
    bsp_m5_mif1,
    bsp_m6_mif1
  ) |>

  # Do some renaming for clarity
  mutate(.variable = case_when(
    .variable == "bsp_m2_mif1" ~ "Loading for m2",
    .variable == "bsp_m3_mif1" ~ "Loading for m3",
    .variable == "bsp_m4_mif1" ~ "Loading for m4",
    .variable == "bsp_m5_mif1" ~ "Loading for m5",
    .variable == "bsp_m6_mif1" ~ "Loading for m6"
  )) |> 

  # Add the true factor loadings from above
  mutate(true_loading = case_when(
    .variable == "Loading for m2" ~ .1,
    .variable == "Loading for m3" ~ .6,
    .variable == "Loading for m4" ~ .2,
    .variable == "Loading for m5" ~ .9,
    .variable == "Loading for m6" ~ -.4
  )) 

# Save the tidy draws for reproducibility
saveRDS(bfit.1.samples, "fits/b07.01.samples.rds")
```

Now we can plot. Looks like STAN does an OK job at recovering the true factor loadings (we exclude the first loading since its loading was held constant at 1 to provide the scale for the latent variable):
```{r}
#| fig-cap: "Posterior distributions of estimated factor loadings for measured variables 2-6. The dashed line is the 'true' simulated loading for each variable."
#| fig-alt: "Posterior distributions of estimated factor loadings for measured variables 2-6. The dashed line is the 'true' simulated loading for each variable."

# Load the tidy samples
bfit.1.samples <- readRDS("source/fits/b07.01.samples.rds")

# Plot
bfit.1.samples|>

  ggplot(aes(x = .value, fill = .variable)) +
    stat_halfeye(fill = "mediumorchid") +
    geom_vline(aes(xintercept = true_loading), linetype = 2) + 
    scale_x_continuous(expand = c(0, 0.015)) +
    scale_y_continuous(expand = c(0, 0.015)) +
    guides(fill = "none") +
    labs(x = "lab",
       y = NULL)  +
    facet_wrap(~.variable) +
    theme_minimal() + 
    theme(panel.grid.major = element_blank())
  
```

In all cases the model is __pretty close__ to the true parameter value. It's a bit concerning when the standard errors are so small they fail to capture the true values, but this illustrates the general proof of concept: we can do a pretty good CFA in {{brms}}.

## Second Example: Correlated factors

Now an example where we assume the latent vactors are themselves dependent, so that discriminant validity is questionable. So we can include a term for their correlation structure in the model. We'll need to amend the lavaan model structure to implement these changes.

```{r}

# Declare the new model with correlated factors
mtmm.model <- ' 
  f1 =~ .8*m1 + .6*m2 + .9*m3
  f2 =~ .1*m4 + .2*m5 + -.4*m6
  f1 ~~ .4*f2
'

# Simulate data from the model
fake_dat <- lavaan::simulateData(model = mtmm.model, sample.nobs = 500)

```

The complication here is that brms doesn't currently support setting constraints on individual correlation terms in the model, even in the form of constant priors. [There seems to be a plan](https://github.com/paul-buerkner/brms/issues/957) to include more SEM-like flexibility of covariance matrixes in general in brms 3.0, but for now we'll need to specify the model using raw Stan code. 

```{r}

# Add the latent variables to the dataset
fake_dat$f1 <- NA_real_
fake_dat$f2 <- NA_real_

# Declare the model and retrieve the raw Stan code
raw_stancode <- make_stancode( 
  formula =
    bf(m1 ~ 0 + mi(f1)) +
    bf(m2 ~ 0 + mi(f1)) +
    bf(m3 ~ 0 + mi(f1)) +
    bf(m4 ~ 0 + mi(f2)) +
    bf(m5 ~ 0 + mi(f2)) +
    bf(m6 ~ 0 + mi(f2)) +
    bf(f1| mi() ~ 1) + 
    bf(f2| mi() ~ 1) + 
    set_rescor(rescor = TRUE),
  family = gaussian(),
  prior =
      prior(constant(1), class = "b", resp = "m1") +
      prior(constant(1), class = "sigma", resp = "m1") +
      prior(normal(0, 10), class = "b", resp = "m2") +
      prior(constant(1), class = "sigma", resp = "m2") +
      prior(normal(0, 10), class = "b", resp = "m3") +
      prior(constant(1), class = "sigma", resp = "m3") +
      prior(constant(1), class = "b", resp = "m4") +
      prior(constant(1), class = "sigma", resp = "m4") +
      prior(normal(0, 10), class = "b", resp = "m5") +
      prior(constant(1), class = "sigma", resp = "m5") +
      prior(normal(0, 10), class = "b", resp = "m6") +
      prior(constant(1), class = "sigma", resp = "m6") +
      prior(normal(0, 10), class = "Intercept", resp = "f1") +
      prior(cauchy(0, 1), class = "sigma", resp = "f1"),
      prior(normal(0, 10), class = "Intercept", resp = "f2") +
      prior(cauchy(0, 1), class = "sigma", resp = "f2"),
  data = fake_dat 
)


test$data$N_f1
#/DELETE
raw_stancode
```

Once we have the raw Stan code we can make a few modifications so that it models the factors as correlated. All we need to do is specify the factors as drawn from a shared multivariate normal distribution, add the new correlation parameter for the var-covar matrix parameteter of that distribution, and put a prior on those correlation terms. Below I've pasted the raw Stan code generated by the `brms` code above, but with comments that start with 'ADDED' to indicate where I've made changes:

```{r}

# Modify the raw Stan code
stan_model_code <- "
// generated with brms 2.20.1
functions {
  
}
data {
  int<lower=1> N; // total number of observations
  int<lower=1> N_m1; // number of observations
  vector[N_m1] Y_m1; // response variable
  int<lower=1> Ksp_m1; // number of special effects terms
  int<lower=1> N_m2; // number of observations
  vector[N_m2] Y_m2; // response variable
  int<lower=1> Ksp_m2; // number of special effects terms
  int<lower=1> N_m3; // number of observations
  vector[N_m3] Y_m3; // response variable
  int<lower=1> Ksp_m3; // number of special effects terms
  int<lower=1> N_m4; // number of observations
  vector[N_m4] Y_m4; // response variable
  int<lower=1> Ksp_m4; // number of special effects terms
  int<lower=1> N_m5; // number of observations
  vector[N_m5] Y_m5; // response variable
  int<lower=1> Ksp_m5; // number of special effects terms
  int<lower=1> N_m6; // number of observations
  vector[N_m6] Y_m6; // response variable
  int<lower=1> Ksp_m6; // number of special effects terms
  int<lower=1> N_f1; // number of observations
  vector[N_f1] Y_f1; // response variable
  int<lower=0> Nmi_f1; // number of missings
  array[Nmi_f1] int<lower=1> Jmi_f1; // positions of missings
  int<lower=1> N_f2; // number of observations
  vector[N_f2] Y_f2; // response variable
  int<lower=0> Nmi_f2; // number of missings
  array[Nmi_f2] int<lower=1> Jmi_f2; // positions of missings
  int<lower=1> nresp; // number of responses
  int nrescor; // number of residual correlations
  int prior_only; // should the likelihood be ignored?
}
transformed data {
  array[N] vector[nresp] Y; // response array
  for (n in 1 : N) {
    Y[n] = transpose([Y_m1[n], Y_m2[n], Y_m3[n], Y_m4[n], Y_m5[n], Y_m6[n],
                      Y_f1[n], Y_f2[n]]);
  }
}
parameters {
  vector[Ksp_m2] bsp_m2; // special effects coefficients
  vector[Ksp_m3] bsp_m3; // special effects coefficients
  vector[Ksp_m5] bsp_m5; // special effects coefficients
  vector[Ksp_m6] bsp_m6; // special effects coefficients
  vector[Nmi_f1] Ymi_f1; // estimated missings
  real Intercept_f1; // temporary intercept for centered predictors
  real<lower=0> sigma_f1; // dispersion parameter
  vector[Nmi_f2] Ymi_f2; // estimated missings
  real Intercept_f2; // temporary intercept for centered predictors
  real<lower=0> sigma_f2; // dispersion parameter
  real<lower=-1, upper=1> corr_f1_f2; // correlation between f1 and f2
}
transformed parameters {
  vector[Ksp_m1] bsp_m1; // special effects coefficients
  real<lower=0> sigma_m1; // dispersion parameter
  real<lower=0> sigma_m2; // dispersion parameter
  real<lower=0> sigma_m3; // dispersion parameter
  vector[Ksp_m4] bsp_m4; // special effects coefficients
  real<lower=0> sigma_m4; // dispersion parameter
  real<lower=0> sigma_m5; // dispersion parameter
  real<lower=0> sigma_m6; // dispersion parameter
  real lprior = 0; // prior contributions to the log posterior
  bsp_m1 = rep_vector(1, rows(bsp_m1));
  sigma_m1 = 1;
  sigma_m2 = 1;
  sigma_m3 = 1;
  bsp_m4 = rep_vector(1, rows(bsp_m4));
  sigma_m4 = 1;
  sigma_m5 = 1;
  sigma_m6 = 1;
  lprior += normal_lpdf(bsp_m2 | 0, 10);
  lprior += normal_lpdf(bsp_m3 | 0, 10);
  lprior += normal_lpdf(bsp_m5 | 0, 10);
  lprior += normal_lpdf(bsp_m6 | 0, 10);
  lprior += normal_lpdf(Intercept_f1 | 0, 10);
  lprior += cauchy_lpdf(sigma_f1 | 0, 1) - 1 * cauchy_lccdf(0 | 0, 1);
  lprior += student_t_lpdf(Intercept_f2 | 3, 0, 2.5);
  lprior += student_t_lpdf(sigma_f2 | 3, 0, 2.5)
            - 1 * student_t_lccdf(0 | 3, 0, 2.5);
  // Create the residual correlation matrix
  corr_matrix[nresp] Rescor = diag_matrix(rep_vector(1, nresp));
  Rescor[7, 8] = corr_f1_f2;
  Rescor[8, 7] = corr_f1_f2;

  // cholesky factor of residual covariance matrix
  vector[nresp] sigma = [sigma_m1, sigma_m2, sigma_m3, sigma_m4, sigma_m5, sigma_m6, sigma_f1, sigma_f2]';
  matrix[nresp, nresp] LSigma = diag_pre_multiply(sigma, cholesky_decompose(Rescor));
}
model {
  // likelihood including constants
  if (!prior_only) {
    // vector combining observed and missing responses
    vector[N_f1] Yl_f1 = Y_f1;
    // vector combining observed and missing responses
    vector[N_f2] Yl_f2 = Y_f2;
    array[N] vector[nresp] Yl = Y;
    // initialize linear predictor term
    vector[N_m1] mu_m1 = rep_vector(0.0, N_m1);
    // initialize linear predictor term
    vector[N_m2] mu_m2 = rep_vector(0.0, N_m2);
    // initialize linear predictor term
    vector[N_m3] mu_m3 = rep_vector(0.0, N_m3);
    // initialize linear predictor term
    vector[N_m4] mu_m4 = rep_vector(0.0, N_m4);
    // initialize linear predictor term
    vector[N_m5] mu_m5 = rep_vector(0.0, N_m5);
    // initialize linear predictor term
    vector[N_m6] mu_m6 = rep_vector(0.0, N_m6);
    // initialize linear predictor term
    vector[N_f1] mu_f1 = rep_vector(0.0, N_f1);
    // initialize linear predictor term
    vector[N_f2] mu_f2 = rep_vector(0.0, N_f2);
    // multivariate predictor array
    array[N] vector[nresp] Mu;
    Yl_f1[Jmi_f1] = Ymi_f1;
    Yl_f2[Jmi_f2] = Ymi_f2;
    mu_f1 += Intercept_f1;
    mu_f2 += Intercept_f2;
    for (n in 1 : N_m1) {
      // add more terms to the linear predictor
      mu_m1[n] += bsp_m1[1] * Yl_f1[n];
    }
    for (n in 1 : N_m2) {
      // add more terms to the linear predictor
      mu_m2[n] += bsp_m2[1] * Yl_f1[n];
    }
    for (n in 1 : N_m3) {
      // add more terms to the linear predictor
      mu_m3[n] += bsp_m3[1] * Yl_f1[n];
    }
    for (n in 1 : N_m4) {
      // add more terms to the linear predictor
      mu_m4[n] += bsp_m4[1] * Yl_f2[n];
    }
    for (n in 1 : N_m5) {
      // add more terms to the linear predictor
      mu_m5[n] += bsp_m5[1] * Yl_f2[n];
    }
    for (n in 1 : N_m6) {
      // add more terms to the linear predictor
      mu_m6[n] += bsp_m6[1] * Yl_f2[n];
    }
    // combine univariate parameters
    for (n in 1 : N) {
      Yl[n][7] = Yl_f1[n];
      Yl[n][8] = Yl_f2[n];
    }
    // combine univariate parameters
    for (n in 1 : N) {
      Mu[n] = transpose([mu_m1[n], mu_m2[n], mu_m3[n], mu_m4[n], mu_m5[n],
                         mu_m6[n], mu_f1[n], mu_f2[n]]);
    }
    target += multi_normal_cholesky_lpdf(Yl | Mu, LSigma);
  }
  // priors including constants
  target += lprior;
}
generated quantities {
  // actual population-level intercept
  real b_f1_Intercept = Intercept_f1;
  // actual population-level intercept
  real b_f2_Intercept = Intercept_f2;
  // residual correlations
  
  vector[1] rescor;
  rescor[1] = corr_f1_f2;
}

"

stan_file <- cmdstanr::write_stan_file(stan_model_code)

```

Now we can run the model

```{r}

# Compile the model from the Stan file
model <- cmdstanr::cmdstan_model(stan_file)

# Put the data into a list format Stan can understand
data_list <- list(
  
    N = nrow(fake_dat),

    N_m1 = nrow(fake_dat),
    Y_m1 = fake_dat$m1,
    Ksp_m1 = length(fake_dat$m1),
    
    N_m2 = nrow(fake_dat),
    Y_m2 = fake_dat$m2,
    Ksp_m2 = length(fake_dat$m2),
    
    N_m3 = nrow(fake_dat),
    Y_m3 = fake_dat$m3,
    Ksp_m3 = length(fake_dat$m3),
    
    N_m4 = nrow(fake_dat),
    Y_m4 = fake_dat$m4,
    Ksp_m4 = length(fake_dat$m4),
    
    N_m5 = nrow(fake_dat),
    Y_m5 = fake_dat$m5,
    Ksp_m5 = length(fake_dat$m5),
    
    N_m6 = nrow(fake_dat),
    Y_m6 = fake_dat$m6,
    Ksp_m6 = length(fake_dat$m6),
    
    N_f1 = nrow(fake_dat),
    Y_f1 = rep(0, nrow(fake_dat)),
    Nmi_f1 = sum(is.na(fake_dat$f1)),
    Jmi_f1 = which(is.na(fake_dat$f1)),
    
    N_f2 = nrow(fake_dat),
    Y_f2 = rep(0, nrow(fake_dat)),
    Nmi_f2 = sum(is.na(fake_dat$f2)),
    Jmi_f2 = which(is.na(fake_dat$f2)),

    prior_only = 0

)

# Fit the model
fit <- model$sample(
  data = data_list,
  seed = 199,
  chains = 4,
  parallel_chains = 4,
  refresh = 500 # print update every 500 iters
)

```

Or instead we can go with {{blavaan}}. The model fits way faster than under the {{brms}} approach and recovers the true parameter estimates. If we specify `mcmcfile = TRUE` then the resulting Stan file gets written to a new directory called `lavExport`, or we can supply a filepath as the argument in which case the Stan file will get written there. The file is very large (over 1000 lines) and confusing, I think because it is precompiled to handle any possible model you could specify in R. So it doesn't seem practical to really get in there and customize things directly. 

The Stan documentation includes [an example for specifying custom priors in a blavaan model](https://mc-stan.org/users/documentation/case-studies/sem.html#Confirmatory_Factor_Analysis_(CFA))

```{r eval = FALSE}
 
# Fit the model
blavfit.1 <- bcfa(mtmm.model, data=fake_dat, mcmcfile = T)

# Gaze at parameter estimates
blavfit.1 |> summary()

# Make sure the MCMC diagnostics are ok
blavInspect(blavfit.1, "mcobj")

```



## Third Example: MTMM 