[
  {
    "objectID": "bayesian-cfa.html",
    "href": "bayesian-cfa.html",
    "title": "6  Bayesian CFA",
    "section": "",
    "text": "library(tidyverse)\nlibrary(lavaan)\nlibrary(brms)\nlibrary(tidybayes)\n\n# brms backend specifications\noptions(brms.backend = \"cmdstanr\")\noptions(mc.cores = parallel::detectCores())\nCan brms replicate results from lavaan?\n{{brms}} doesn’t officially have SEM capabilities (but they do seem to be coming soon!). But STAN forum contributer Jack Bailey has ‘patanté’ a solution using artful prior specifications.\nTo demonstrate, we can simulate some data using {{lavaan}} and show that brms::brm() is able to recover the ‘true’ factor loadings."
  },
  {
    "objectID": "bayesian-cfa.html#first-example-simple-factor-structure",
    "href": "bayesian-cfa.html#first-example-simple-factor-structure",
    "title": "6  Bayesian CFA",
    "section": "6.1 First Example: Simple Factor Structure",
    "text": "6.1 First Example: Simple Factor Structure\nFirst we can simulate some data with a simple factor structure using lavaan’s handy simulateData() function:\n\npop.model <- ' \n  f1 =~ .8*m1 + .1*m2 + .6*m3 + .2*m4 + .9*m5 + -.4*m6\n'\n\n# Simulate data from the model\nfake_dat <- lavaan::simulateData(model = pop.model, sample.nobs = 4000)\n\n# Visualize the measured dat\nfake_dat |>\n\n  select(m1, m2, m3, m4, m5, m6) |>\n  \n  pivot_longer(everything(), names_to = \"var\", values_to = \"measurement\") |>\n\n  ggplot() +\n  geom_histogram(aes(x = measurement)) +\n  facet_wrap(~var) +\n  theme_bw()\n\n\n\n\nUnsurprisingly, the lavaan cfa() function is able to recover the true parameters for the data it generated.\n\n# Fit the model\nfit_1 <- cfa(pop.model, data = fake_dat)\n\n# Make sure it works\nfit_1 |> broom::tidy()\n\n# A tibble: 13 × 9\n   term     op    estimate std.error statistic p.value std.lv std.all std.nox\n   <chr>    <chr>    <dbl>     <dbl>     <dbl>   <dbl>  <dbl>   <dbl>   <dbl>\n 1 f1 =~ m1 =~       0.8      0           NA        NA  0.805  0.624   0.624 \n 2 f1 =~ m2 =~       0.1      0           NA        NA  0.101  0.0984  0.0984\n 3 f1 =~ m3 =~       0.6      0           NA        NA  0.604  0.510   0.510 \n 4 f1 =~ m4 =~       0.2      0           NA        NA  0.201  0.196   0.196 \n 5 f1 =~ m5 =~       0.9      0           NA        NA  0.906  0.678   0.678 \n 6 f1 =~ m6 =~      -0.4      0           NA        NA -0.403 -0.370  -0.370 \n 7 m1 ~~ m1 ~~       1.02     0.0291      35.0       0  1.02   0.611   0.611 \n 8 m2 ~~ m2 ~~       1.04     0.0232      44.6       0  1.04   0.990   0.990 \n 9 m3 ~~ m3 ~~       1.04     0.0263      39.4       0  1.04   0.740   0.740 \n10 m4 ~~ m4 ~~       1.02     0.0231      44.1       0  1.02   0.962   0.962 \n11 m5 ~~ m5 ~~       0.966    0.0306      31.5       0  0.966  0.541   0.541 \n12 m6 ~~ m6 ~~       1.02     0.0242      42.4       0  1.02   0.863   0.863 \n13 f1 ~~ f1 ~~       1.01     0.0346      29.3       0  1      1       1     \n\n\nBut can brms recover the true parameter values?\n\n# Add a latent variable to the dataset\nfake_dat$f1 <- NA_real_\n\nbfit.1 <- brm(\n    formula =\n      bf(m1 ~ 0 + mi(f1)) +\n      bf(m2 ~ 0 + mi(f1)) +\n      bf(m3 ~ 0 + mi(f1)) +\n      bf(m4 ~ 0 + mi(f1)) +\n      bf(m5 ~ 0 + mi(f1)) +\n      bf(m6 ~ 0 + mi(f1)) +\n      bf(f1| mi() ~ 1) + \n      set_rescor(rescor = FALSE),\n    family = gaussian(),\n    prior =\n      prior(constant(1), class = \"b\", resp = \"m1\") +\n      prior(constant(1), class = \"sigma\", resp = \"m1\") +\n      prior(normal(0, 10), class = \"b\", resp = \"m2\") +\n      prior(constant(1), class = \"sigma\", resp = \"m2\") +\n      prior(normal(0, 10), class = \"b\", resp = \"m3\") +\n      prior(constant(1), class = \"sigma\", resp = \"m3\") +\n      prior(normal(0, 10), class = \"b\", resp = \"m4\") +\n      prior(constant(1), class = \"sigma\", resp = \"m4\") +\n      prior(normal(0, 10), class = \"b\", resp = \"m5\") +\n      prior(constant(1), class = \"sigma\", resp = \"m5\") +\n      prior(normal(0, 10), class = \"b\", resp = \"m6\") +\n      prior(constant(1), class = \"sigma\", resp = \"m6\") +\n      prior(normal(0, 10), class = \"Intercept\", resp = \"f1\") +\n      prior(cauchy(0, 1), class = \"sigma\", resp = \"f1\"),\n    data = fake_dat,\n    warmup = 1000,\n    iter = 6000,\n    file = \"fits/b07.01.rds\"\n  )\n\nA Bayesian missing data model treats each missing observation as a parameter to estimate. And since the factor is missing in each row of the dataset, this means we end up with 6000 samples for each row of data we have, resulting in a pretty big model file. This is too big for Github, so I’ll only push the draws we need to replicate the figure below.\nFirst we can put the draws we need into a tidy format and do some cleaning:\n\nbfit.1.samples <- bfit.1 |>\n\n  # Get the raw MCMC samples\n  gather_draws(\n    bsp_m2_mif1,\n    bsp_m3_mif1,\n    bsp_m4_mif1,\n    bsp_m5_mif1,\n    bsp_m6_mif1\n  ) |>\n\n  # Do some renaming for clarity\n  mutate(.variable = case_when(\n    .variable == \"bsp_m2_mif1\" ~ \"Loading for m2\",\n    .variable == \"bsp_m3_mif1\" ~ \"Loading for m3\",\n    .variable == \"bsp_m4_mif1\" ~ \"Loading for m4\",\n    .variable == \"bsp_m5_mif1\" ~ \"Loading for m5\",\n    .variable == \"bsp_m6_mif1\" ~ \"Loading for m6\"\n  )) |> \n\n  # Add the true factor loadings from above\n  mutate(true_loading = case_when(\n    .variable == \"Loading for m2\" ~ .1,\n    .variable == \"Loading for m3\" ~ .6,\n    .variable == \"Loading for m4\" ~ .2,\n    .variable == \"Loading for m5\" ~ .9,\n    .variable == \"Loading for m6\" ~ -.4\n  )) \n\n# Save the tidy draws for reproducibility\nsaveRDS(bfit.1.samples, \"fits/b07.01.samples.rds\")\n\nNow we can plot. Looks like STAN does an OK job at recovering the true factor loadings (we exclude the first loading since its loading was held constant at 1 to provide the scale for the latent variable):\n\n# Load the tidy samples\nbfit.1.samples <- readRDS(\"fits/b07.01.samples.rds\")\n\n# Plot\nbfit.1.samples|>\n\n  ggplot(aes(x = .value, fill = .variable)) +\n    stat_halfeye(fill = \"mediumorchid\") +\n    geom_vline(aes(xintercept = true_loading), linetype = 2) + \n    scale_x_continuous(expand = c(0, 0.015)) +\n    scale_y_continuous(expand = c(0, 0.015)) +\n    guides(fill = \"none\") +\n    labs(x = \"lab\",\n       y = NULL)  +\n    facet_wrap(~.variable) +\n    theme_minimal() + \n    theme(panel.grid.major = element_blank())\n\n\n\n\nPosterior distributions of estimated factor loadings for measured variables 2-6. The dashed line is the ‘true’ simulated loading for each variable.\n\n\n\n\nIn all cases the model is pretty close to the true parameter value. It’s a bit concerning when the standard errors are so small they fail to capture the true values, but this illustrates the general proof of concept: we can do a pretty good CFA in {{brms}}.\n\nfit3 <- brm(formula = rating ~ treat + period,\ndata = inhaler, family = cumulative)\n\nStart sampling\n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: ordered_logistic: Cut-points is not a valid ordered vector. The element at 3 is 3.3742e+41, but should be greater than the previous element, 3.3742e+41 (in '/var/folders/n6/wwxpw0j17gq8j_040cqdb5fw0000gn/T/RtmpRiOrv4/model-b226315032a7.stan', line 84, column 6 to column 63)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: ordered_logistic: Cut-points is not a valid ordered vector. The element at 3 is 5.91618e+41, but should be greater than the previous element, 5.91618e+41 (in '/var/folders/n6/wwxpw0j17gq8j_040cqdb5fw0000gn/T/RtmpRiOrv4/model-b226315032a7.stan', line 84, column 6 to column 63)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: ordered_logistic: Cut-points is not a valid ordered vector. The element at 2 is -7228.43, but should be greater than the previous element, -7228.43 (in '/var/folders/n6/wwxpw0j17gq8j_040cqdb5fw0000gn/T/RtmpRiOrv4/model-b226315032a7.stan', line 84, column 6 to column 63)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: ordered_logistic: Cut-points is not a valid ordered vector. The element at 2 is -206.57, but should be greater than the previous element, -206.57 (in '/var/folders/n6/wwxpw0j17gq8j_040cqdb5fw0000gn/T/RtmpRiOrv4/model-b226315032a7.stan', line 84, column 6 to column 63)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: ordered_logistic: Cut-points is not a valid ordered vector. The element at 3 is 8.0387e+35, but should be greater than the previous element, 8.0387e+35 (in '/var/folders/n6/wwxpw0j17gq8j_040cqdb5fw0000gn/T/RtmpRiOrv4/model-b226315032a7.stan', line 84, column 6 to column 63)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: ordered_logistic: Cut-points is not a valid ordered vector. The element at 3 is 2.96396e+35, but should be greater than the previous element, 2.96396e+35 (in '/var/folders/n6/wwxpw0j17gq8j_040cqdb5fw0000gn/T/RtmpRiOrv4/model-b226315032a7.stan', line 84, column 6 to column 63)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: ordered_logistic: Cut-points is not a valid ordered vector. The element at 3 is 4.66516e+73, but should be greater than the previous element, 4.66516e+73 (in '/var/folders/n6/wwxpw0j17gq8j_040cqdb5fw0000gn/T/RtmpRiOrv4/model-b226315032a7.stan', line 84, column 6 to column 63)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: ordered_logistic: Cut-points is not a valid ordered vector. The element at 3 is 1.02581e+38, but should be greater than the previous element, 1.02581e+38 (in '/var/folders/n6/wwxpw0j17gq8j_040cqdb5fw0000gn/T/RtmpRiOrv4/model-b226315032a7.stan', line 84, column 6 to column 63)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: ordered_logistic: Cut-points is not a valid ordered vector. The element at 3 is 2.94854e+37, but should be greater than the previous element, 2.94854e+37 (in '/var/folders/n6/wwxpw0j17gq8j_040cqdb5fw0000gn/T/RtmpRiOrv4/model-b226315032a7.stan', line 84, column 6 to column 63)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: ordered_logistic: Cut-points is not a valid ordered vector. The element at 3 is inf, but should be greater than the previous element, inf (in '/var/folders/n6/wwxpw0j17gq8j_040cqdb5fw0000gn/T/RtmpRiOrv4/model-b226315032a7.stan', line 84, column 6 to column 63)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: ordered_logistic: Cut-points is not a valid ordered vector. The element at 2 is 6.51117, but should be greater than the previous element, 6.51117 (in '/var/folders/n6/wwxpw0j17gq8j_040cqdb5fw0000gn/T/RtmpRiOrv4/model-b226315032a7.stan', line 84, column 6 to column 63)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: ordered_logistic: Cut-points is not a valid ordered vector. The element at 2 is 6.71665, but should be greater than the previous element, 6.71665 (in '/var/folders/n6/wwxpw0j17gq8j_040cqdb5fw0000gn/T/RtmpRiOrv4/model-b226315032a7.stan', line 84, column 6 to column 63)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: ordered_logistic: Cut-points is not a valid ordered vector. The element at 3 is inf, but should be greater than the previous element, inf (in '/var/folders/n6/wwxpw0j17gq8j_040cqdb5fw0000gn/T/RtmpRiOrv4/model-b226315032a7.stan', line 84, column 6 to column 63)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\nfit3 |> summary()\n\n Family: cumulative \n  Links: mu = logit; disc = identity \nFormula: rating ~ treat + period \n   Data: inhaler (Number of observations: 572) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept[1]     0.54      0.09     0.36     0.72 1.00     4383     2946\nIntercept[2]     3.18      0.21     2.79     3.60 1.00     5255     3022\nIntercept[3]     4.40      0.37     3.73     5.16 1.00     4835     3233\ntreat           -1.01      0.18    -1.36    -0.67 1.00     4781     2837\nperiod           0.15      0.18    -0.20     0.50 1.00     5487     2956\n\nFamily Specific Parameters: \n     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ndisc     1.00      0.00     1.00     1.00   NA       NA       NA\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "bayesian-cfa.html#second-example-correlated-factors",
    "href": "bayesian-cfa.html#second-example-correlated-factors",
    "title": "6  Bayesian CFA",
    "section": "6.2 Second Example: Correlated factors",
    "text": "6.2 Second Example: Correlated factors"
  },
  {
    "objectID": "bayesian-cfa.html#third-example-mtmm",
    "href": "bayesian-cfa.html#third-example-mtmm",
    "title": "6  Bayesian CFA",
    "section": "6.3 Third Example: MTMM",
    "text": "6.3 Third Example: MTMM"
  },
  {
    "objectID": "sem-intro.html",
    "href": "sem-intro.html",
    "title": "Structural Equation Modelling",
    "section": "",
    "text": "Someone on the stan forums recommended this book for a bayesian example: https://www.guilford.com/books/Bayesian-Structural-Equation-Modeling/Sarah-Depaoli/9781462547746"
  },
  {
    "objectID": "latent-variable-regression.html",
    "href": "latent-variable-regression.html",
    "title": "7  Example: Survival Analysis with Latent Variables",
    "section": "",
    "text": "The line between SEM and classic regression starts to blur when we realize we can mix-and-match latent and observed variables in a regression, defining whichever relationships make the most sense given our background knowledge.\nA traditional way of incorporating a CFA measurement model into a fuller regression analysis proceeds in two steps:\nBut this approach is unsatisfactory because factor scores are a function of the CFA model’s parameter estimates (the estimates of the factor loadings), about which there is uncertainty. So if we only give our substantive regression model a point-estimate factor score from the CFA model, we ignore the uncertainty contained in the standard errors of the CFA model’s factor loading estimates. This is the approach taken by Kankaraš, Feron, and Renbarger (2019); they use point-estimate factor scores as predictors in subsequent regressions. In some cases they even do this iteratively, fitting measurement models on point-estimate factor scores from measurement models fit on point-estimate factor scores, and using those point-estimates as regression predictors! In their analysis they don’t even report the standard errors of the factor loadings from their measurement models.\nA better option is to fit a Bayesian model that carries out both the measurement model and the substantive regression model simultaneously, so that the model can incorporate its uncertainty from the CFA model into its substantive parameter estimates and predictions. This section provides an example of how to implement just such a Bayesian model in a survival analysis context in STAN via the brms R package."
  },
  {
    "objectID": "latent-variable-regression.html#background-and-data-simulation",
    "href": "latent-variable-regression.html#background-and-data-simulation",
    "title": "7  Example: Survival Analysis with Latent Variables",
    "section": "7.1 Background and Data Simulation",
    "text": "7.1 Background and Data Simulation\n\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(ordinal)\n\nFor this example we’ll simulate some data related to the subject matter of Kankaraš, Feron, and Renbarger (2019). Specifically, we’ll simulate some data in which latent social and emotional skills predict time-to-employment to varying degrees. More specifically, here is what the dataset should contain:\n\nn participants in a skills training program;\nSome continuous and categorical demographic variables for each participant, such as age, gender priority group (this should be TRUE/FALSE), barriers to employment, and n previous services accessed.\na latent variable called ‘communication’ with an arbitrary scale of 0-100.\na scale of 5 measurements each for this latent variable, each on a likert scale from 1-5. These should each be a function of the corresponding latent variable plus noise, where the degree of correlation should be controllable by the user via function arguments.\nthe outcome variable, time, which should be a function of the latent variables and the demographhic variables and noise\na status variable indicating whether time refers to time-to-employment or time-to-censoring. This should be random for non-informative censoring, with the probability of censoring being controllable by the user.\n\nFor now we’ll assume the latent variables are not correlated, even though this is a bit of a silly assumption in practice.\nOne improvement we can make on the traitional factor analyses we looked at in previous sections is that we can specify our measurement model with ordinal relationships between the factor and its measurements. This makes sense given that the measurements are likert, andd it doesn’t make sense to assume likert variables move linearly.\nIt would be most efficient to just define a function that creates the dataset we need based on various parameters. But I’ll instead go step-by-step and explain each choice as we go. First we can define the dataframe and do the easy work of adding the variables that don’t depend on any of the others:\n\nn = 60000\ndat <- tibble(\n\n        # Simulate demographic variables\n        age = runif(n, 20, 60),\n        gender_priority_group = sample(c(TRUE, FALSE), n, replace = TRUE),\n        barriers_to_employment = rpois(n, lambda = 1),\n        previous_services = rpois(n, lambda = 3), \n\n        # Simulate latent variables on an arbitrary scale\n        adaptability = runif(n, 0, 100),\n        collaboration = runif(n, 0, 100),\n        communication = runif(n, 0, 100)\n\n)\n\nNow we’ll simulate the likert-scale ‘measurements’ of the latent variables we defined above. The first step will be to define the threshold coefficients, or what McElreath (2020) calls ‘cutpoints’. These are just the intercepts of the linear models that define each of the cumulative probabilities via a logit link. Another way of thinking about these is that they are the logit of each cumulative probability when the substantive predictors in the model are all equal to 0.\nSo how shall we choose these for our example? We can do whatever we want. Let’s imagine a situation where the people grading these participants have a tendency to give out lots of 3s and 5s, and relatively very few of the other options. Since we’ve defined our latent variables on an arbitrary scale from 1-100, we can define the threshold coefficients to slice up that scale to give us the pattern we want. We can also define some ‘true’ factor loadings that determine the degree to which the measurements are actually influenced by the underlying latent variable. Usually in a real-world context we find that some measurements are tighter than others.\n\n# Define the threshold coefficients\ntheta <- c(\n    5,  # You get a 1 if your true 'latent' score is <5 and all predictors are 0. \n    50, # You get a 2 if your true 'latent' score is between 5 and 50.\n    60, # You get a 3 if your true 'latent' score is between 50 and 60.\n    65  # You get a 4 if your true 'latent' score is between 60 and 65.\n#   NA  # You get a 5 if your true 'latent' score is >65.\n)\n\n# Define the factor loadings\nloadings <- c(\n    1,\n    1,\n    1,\n    1,\n    1\n)\n\nNow we can use those cutpoints to generate some data. Here I use a slightly modified version of the simulation approach for ordinal outcomes given in this STAN forum thread by Conor Goold.\n\n# Define a function to generate a single ordinal data point\n# @K number of ordinal categories\n# @theta vector of latent cutpoints\n# @eta linear predictor\ngen_ordinal_measurements <- function(K, theta, eta, loading){\n\n   if(length(theta) != (K - 1))\n        stop(paste0(\"theta must have length \", K - 1, \" not \", length(theta)))\n\n   n <- length(eta)\n   result <- numeric(n)\n\n   for(i in 1:n) {\n       probs <- numeric(K)\n       for(k in 1:K){\n           if(k == 1) prev_thresh = -Inf\n           else prev_thresh = theta[k - 1]\n\n           if(k == K) next_thresh = Inf\n           else next_thresh = theta[k]\n\n           probs[k] = plogis(next_thresh - (eta[i] * loading)) - plogis(prev_thresh - (eta[i] * loading))\n       }\n       result[i] <- sample(K, 1, prob=probs)\n   }\n  \n   return(result)\n   \n}\n\n# Simulate the measured outcomes\ndat <- dat |>\n\n    # Simulate items for latent variable measurements\n    mutate(\n\n        communication_m1 = gen_ordinal_measurements(5, theta, communication, loadings[1]),\n        communication_m2 = gen_ordinal_measurements(5, theta, communication, loadings[2]),\n        communication_m3 = gen_ordinal_measurements(5, theta, communication, loadings[3]),\n        communication_m4 = gen_ordinal_measurements(5, theta, communication, loadings[4]),\n        communication_m5 = gen_ordinal_measurements(5, theta, communication, loadings[5])\n\n    )\n\nWe can do some quick visualization to make sure the data have that ‘mostly 2s and 5s’ pattern we were going for, but that it varies slightly such that measured variables with higher loadings tend to have more 3s and 4s than variables with lower loadings (I think?)\n\ndat |> \n\n   select(matches('^communication')) |>\n\n   pivot_longer(matches(\"_m\\\\d+$\"), names_to = \"var\", values_to = \"measurement\") |>\n\n   ggplot(aes(x = communication, y = measurement)) + \n   geom_point() + \n   facet_wrap(~var)\n\n\n\nmodel='communication ~ communication_m1+communication_m2+communication_m3+communication_m4+communication_m5'\n\n# lavaan test\nmodel.ord = cfa(model, data=dat |> rename(\"communication\" = communication), ordered=c(\n    \"communication_m1\",\n    \"communication_m2\",\n    \"communication_m3\",\n    \"communication_m4\",\n    \"communication_m5\"\n    )   \n)\n\nWarning in lav_data_full(data = data, group = group, cluster = cluster, : lavaan\nWARNING: exogenous variable(s) declared as ordered in data: communication_m1\ncommunication_m2 communication_m3 communication_m4 communication_m5\n\n\nWarning in lav_partable_check(lavpartable, categorical =\nlavoptions$.categorical, : lavaan WARNING: parameter table does not contain\nthresholds\n\nWarning in lav_partable_check(lavpartable, categorical =\nlavoptions$.categorical, : lavaan WARNING: parameter table does not contain\nthresholds\n\nmodel.ord |> summary()\n\nlavaan 0.6.16 ended normally after 17 iterations\n\n  Estimator                                       DWLS\n  Optimization method                           NLMINB\n  Number of model parameters                         7\n\n  Number of observations                         60000\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                 0.000       0.000\n  Degrees of freedom                                 0           0\n\nParameter Estimates:\n\n  Standard errors                           Robust.sem\n  Information                                 Expected\n  Information saturated (h1) model        Unstructured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  communication ~                                     \n    communicatn_m1    3.838    0.198   19.387    0.000\n    communicatn_m2    3.762    0.199   18.941    0.000\n    communicatn_m3    3.529    0.199   17.724    0.000\n    communicatn_m4    3.952    0.198   19.954    0.000\n    communicatn_m5    3.744    0.197   19.046    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .communication   -10.237    0.113  -90.748    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .communication   114.510    0.438  261.655    0.000\n\n\nLastly, we’ll simulate the outcome variables for survival analysis. We’ll use a Weibull likelihood. This is a popular choice for parametric survival analysis because it can fit a pretty flexible monotonic baseline hazard function, and can be interpretted as both a proportional hazards and an acceleration failure time model, which gives us flexibility in how we communicate results and frees us from the burden of needing to check the proportional hazards assumption.\nCould make it fancy by clustering people and doing it multilevel?\n\n\n\n\nclm(factor(communication_m1) ~ communication, data = dat)\n\nformula: factor(communication_m1) ~ communication\ndata:    dat\n\n link  threshold nobs  logLik   AIC      niter max.grad cond.H \n logit flexible  60000 -7874.79 15759.59 12(0) 1.90e-10 8.1e+06\n\nCoefficients:\ncommunication \n       0.9979 \n\nThreshold coefficients:\n   1|2    2|3    3|4    4|5 \n 5.049 49.908 59.864 64.845 \n\ndat |> \n\n #   filter(adaptability_m1 %in% c(1, 2, 3)) |>\n\n    count(communication_m1) |>\n\n    mutate(p = n / sum(n))\n\n# A tibble: 5 × 3\n  communication_m1     n      p\n             <dbl> <int>  <dbl>\n1                1  3018 0.0503\n2                2 27017 0.450 \n3                3  5984 0.0997\n4                4  3052 0.0509\n5                5 20929 0.349 \n\n\nThe idea is that if the latent variables predict time-to-employment then that is consistent with them being well-measured, etc.\nThis example has closely mirrored an analysis I carried out for a real client. In that example the latent variables had no predictive validity."
  },
  {
    "objectID": "latent-variable-regression.html#prior-predictive-checks",
    "href": "latent-variable-regression.html#prior-predictive-checks",
    "title": "7  Example: Survival Analysis with Latent Variables",
    "section": "7.2 Prior Predictive Checks",
    "text": "7.2 Prior Predictive Checks"
  },
  {
    "objectID": "latent-variable-regression.html#fitting-the-model",
    "href": "latent-variable-regression.html#fitting-the-model",
    "title": "7  Example: Survival Analysis with Latent Variables",
    "section": "7.3 Fitting the Model",
    "text": "7.3 Fitting the Model"
  },
  {
    "objectID": "latent-variable-regression.html#modelling-likert-measurements-as-ordinal-variables",
    "href": "latent-variable-regression.html#modelling-likert-measurements-as-ordinal-variables",
    "title": "7  Example: Survival Analysis with Latent Variables",
    "section": "7.4 Modelling Likert Measurements as Ordinal Variables",
    "text": "7.4 Modelling Likert Measurements as Ordinal Variables\nWe can improve the model."
  },
  {
    "objectID": "latent-variable-regression.html#using-raw-stan-for-correlated-factors",
    "href": "latent-variable-regression.html#using-raw-stan-for-correlated-factors",
    "title": "7  Example: Survival Analysis with Latent Variables",
    "section": "7.5 Using Raw STAN for Correlated Factors",
    "text": "7.5 Using Raw STAN for Correlated Factors\n\n\n\n\nKankaraš, Miloš, Eva Feron, and Rachel Renbarger. 2019. “Assessing Students’ Social and Emotional Skills Through Triangulation of Assessment Methods,” no. 208. https://doi.org/https://doi.org/https://doi.org/10.1787/717ad7f2-en.\n\n\nMcElreath, Richard; 2020. Statistical Rethinking: A Bayesian Course with Examples in r and STAN. 2nd ed. CRC Press LLC."
  }
]