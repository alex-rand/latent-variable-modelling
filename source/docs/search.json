[
  {
    "objectID": "bayesian-cfa.html",
    "href": "bayesian-cfa.html",
    "title": "6  Bayesian CFA",
    "section": "",
    "text": "library(tidyverse)\nlibrary(lavaan)\nlibrary(blavaan)\nlibrary(brms)\nlibrary(tidybayes)\n\n# MCMC specifications\noptions(brms.backend = \"cmdstanr\")\noptions(mc.cores = parallel::detectCores())\nCan brms replicate results from lavaan?\n{{brms}} doesn’t officially have SEM capabilities (but they do seem to be coming soon!). But STAN forum contributer Jack Bailey has ‘patanté’ a solution using artful prior specifications.\nTo demonstrate, we can simulate some data using {{lavaan}} and show that brms::brm() is able to recover the ‘true’ factor loadings."
  },
  {
    "objectID": "bayesian-cfa.html#first-example-simple-factor-structure",
    "href": "bayesian-cfa.html#first-example-simple-factor-structure",
    "title": "6  Bayesian CFA",
    "section": "6.1 First Example: Simple Factor Structure",
    "text": "6.1 First Example: Simple Factor Structure\nFirst we can simulate some data with a simple factor structure using lavaan’s handy simulateData() function:\n\npop.model <- ' \n  f1 =~ .8*m1 + .1*m2 + .6*m3 + .2*m4 + .9*m5 + -.4*m6\n'\n\n# Simulate data from the model\nfake_dat <- lavaan::simulateData(model = pop.model, sample.nobs = 4000)\n\n# Visualize the measured dat\nfake_dat |>\n\n  select(m1, m2, m3, m4, m5, m6) |>\n  \n  pivot_longer(everything(), names_to = \"var\", values_to = \"measurement\") |>\n\n  ggplot() +\n  geom_histogram(aes(x = measurement)) +\n  facet_wrap(~var) +\n  theme_bw()\n\n\n\n\nUnsurprisingly, the lavaan cfa() function is able to recover the true parameters for the data it generated.\n\n# Fit the model\nfit_1 <- cfa(pop.model, data = fake_dat)\n\n# Make sure it works\nfit_1 |> broom::tidy()\n\n# A tibble: 13 × 9\n   term     op    estimate std.error statistic p.value std.lv std.all std.nox\n   <chr>    <chr>    <dbl>     <dbl>     <dbl>   <dbl>  <dbl>   <dbl>   <dbl>\n 1 f1 =~ m1 =~       0.8      0           NA        NA  0.828   0.636   0.636\n 2 f1 =~ m2 =~       0.1      0           NA        NA  0.104   0.102   0.102\n 3 f1 =~ m3 =~       0.6      0           NA        NA  0.621   0.527   0.527\n 4 f1 =~ m4 =~       0.2      0           NA        NA  0.207   0.204   0.204\n 5 f1 =~ m5 =~       0.9      0           NA        NA  0.932   0.682   0.682\n 6 f1 =~ m6 =~      -0.4      0           NA        NA -0.414  -0.391  -0.391\n 7 m1 ~~ m1 ~~       1.01     0.0291      34.7       0  1.01    0.596   0.596\n 8 m2 ~~ m2 ~~       1.03     0.0230      44.6       0  1.03    0.990   0.990\n 9 m3 ~~ m3 ~~       1.00     0.0256      39.2       0  1.00    0.722   0.722\n10 m4 ~~ m4 ~~       0.988    0.0224      44.1       0  0.988   0.958   0.958\n11 m5 ~~ m5 ~~       0.996    0.0313      31.8       0  0.996   0.534   0.534\n12 m6 ~~ m6 ~~       0.952    0.0226      42.2       0  0.952   0.847   0.847\n13 f1 ~~ f1 ~~       1.07     0.0358      29.9       0  1       1       1    \n\n\nBut can brms recover the true parameter values? I like this approach because it gives a nice conceptual perspective on what we’re actually doing when we’re doing latant variable modelling: we’re just doing a linear regression where the measured variables are all drawn from a sharedd multivariate normal distribution, and where the linear model component of their location parameters include a covariate for which we only have missing data. It’s a pretty cool and weird thing.\n\n# Add a latent variable to the dataset\nfake_dat$f1 <- NA_real_\n\nbfit.1 <- brm(\n    formula =\n      bf(m1 ~ 0 + mi(f1)) +\n      bf(m2 ~ 0 + mi(f1)) +\n      bf(m3 ~ 0 + mi(f1)) +\n      bf(m4 ~ 0 + mi(f1)) +\n      bf(m5 ~ 0 + mi(f1)) +\n      bf(m6 ~ 0 + mi(f1)) +\n      bf(f1| mi() ~ 1) + \n      set_rescor(rescor = FALSE),\n    family = gaussian(),\n    prior =\n      prior(constant(1), class = \"b\", resp = \"m1\") +\n      prior(constant(1), class = \"sigma\", resp = \"m1\") +\n      prior(normal(0, 10), class = \"b\", resp = \"m2\") +\n      prior(constant(1), class = \"sigma\", resp = \"m2\") +\n      prior(normal(0, 10), class = \"b\", resp = \"m3\") +\n      prior(constant(1), class = \"sigma\", resp = \"m3\") +\n      prior(normal(0, 10), class = \"b\", resp = \"m4\") +\n      prior(constant(1), class = \"sigma\", resp = \"m4\") +\n      prior(normal(0, 10), class = \"b\", resp = \"m5\") +\n      prior(constant(1), class = \"sigma\", resp = \"m5\") +\n      prior(normal(0, 10), class = \"b\", resp = \"m6\") +\n      prior(constant(1), class = \"sigma\", resp = \"m6\") +\n      prior(normal(0, 10), class = \"Intercept\", resp = \"f1\") +\n      prior(cauchy(0, 1), class = \"sigma\", resp = \"f1\"),\n    data = fake_dat,\n    warmup = 1000,\n    iter = 6000,\n    file = \"fits/b07.01.rds\"\n  )\n\nA Bayesian missing data model treats each missing observation as a parameter to estimate. And since the factor is missing in each row of the dataset, this means we end up with 6000 samples for each row of data we have, resulting in a pretty big model file. This is too big for Github, so I’ll only push the draws we need to replicate the figure below.\nFirst we can put the draws we need into a tidy format and do some cleaning:\n\nbfit.1.samples <- bfit.1 |>\n\n  # Get the raw MCMC samples\n  gather_draws(\n    bsp_m2_mif1,\n    bsp_m3_mif1,\n    bsp_m4_mif1,\n    bsp_m5_mif1,\n    bsp_m6_mif1\n  ) |>\n\n  # Do some renaming for clarity\n  mutate(.variable = case_when(\n    .variable == \"bsp_m2_mif1\" ~ \"Loading for m2\",\n    .variable == \"bsp_m3_mif1\" ~ \"Loading for m3\",\n    .variable == \"bsp_m4_mif1\" ~ \"Loading for m4\",\n    .variable == \"bsp_m5_mif1\" ~ \"Loading for m5\",\n    .variable == \"bsp_m6_mif1\" ~ \"Loading for m6\"\n  )) |> \n\n  # Add the true factor loadings from above\n  mutate(true_loading = case_when(\n    .variable == \"Loading for m2\" ~ .1,\n    .variable == \"Loading for m3\" ~ .6,\n    .variable == \"Loading for m4\" ~ .2,\n    .variable == \"Loading for m5\" ~ .9,\n    .variable == \"Loading for m6\" ~ -.4\n  )) \n\n# Save the tidy draws for reproducibility\nsaveRDS(bfit.1.samples, \"fits/b07.01.samples.rds\")\n\nNow we can plot. Looks like STAN does an OK job at recovering the true factor loadings (we exclude the first loading since it was held constant at 1 to provide the scale for the latent variable):\n\n# Load the tidy samples\nbfit.1.samples <- readRDS(\"fits/b07.01.samples.rds\")\n\n# Plot\nbfit.1.samples|>\n\n  ggplot(aes(x = .value, fill = .variable)) +\n    stat_halfeye(fill = \"mediumorchid\") +\n    geom_vline(aes(xintercept = true_loading), linetype = 2) + \n    scale_x_continuous(expand = c(0, 0.015)) +\n    scale_y_continuous(expand = c(0, 0.015)) +\n    guides(fill = \"none\") +\n    labs(x = \"lab\",\n       y = NULL)  +\n    facet_wrap(~.variable) +\n    theme_minimal() + \n    theme(panel.grid.major = element_blank())\n\n\n\n\nPosterior distributions of estimated factor loadings for measured variables 2-6. The dashed line is the ‘true’ simulated loading for each variable.\n\n\n\n\nIn all cases the model is pretty close to the true parameter value. It’s a bit concerning when the standard errors are so small they fail to capture the true values, but this illustrates the general proof of concept: we can do a pretty good CFA in {{brms}}."
  },
  {
    "objectID": "bayesian-cfa.html#second-example-correlated-factors",
    "href": "bayesian-cfa.html#second-example-correlated-factors",
    "title": "6  Bayesian CFA",
    "section": "6.2 Second Example: Correlated factors",
    "text": "6.2 Second Example: Correlated factors\nNow an example where we assume the latent vactors are themselves dependent, so that discriminant validity is questionable. So we can include a term for their correlation structure in the model. We’ll need to amend the lavaan model structure to implement these changes.\n\n# Declare the new model with correlated factors\ncorrelated.factors.model <- ' \n  f1 =~ .8*m1 + .6*m2 + .9*m3\n  f2 =~ .1*m4 + .2*m5 + -.4*m6\n  f1 ~~ .4*f2\n'\n\n# Simulate data from the model\nfake_dat <- lavaan::simulateData(model = correlated.factors.model, sample.nobs = 4000)\n\nThe complication here is that brms doesn’t currently support setting constraints on individual correlation terms in the model, even in the form of constant priors. There seems to be a plan to include more SEM-like flexibility of covariance matrixes in general in brms 3.0, but for now we’ll need to specify the model using raw Stan code. There seem to be a few strategies for avoiding convergence issues when fitting this kind of model in Stan. I worked closely from the strategy provided in this Stan forum comment by Mauricio Garnier-Villarre, making a few minor adjustments such as removing latent intercepts from the linear predictors to better align this model with the {{lavaan}} defaults.\nHere is the raw Stan code. I also declare it as a string in a hidden R block so we can use {{cmdstanr}}’s autoformatting function before we compile.\ndata {\n  int N; // sample size\n  int P; // number of variables\n  int D; // number of factors\n  array[N] vector[P] X; // data matrix of order [N,P]\n  int n_lam; // how many factor loadings are estimated\n}\nparameters {\n  matrix[N, D] FS_UNC; // factor scores, matrix of order [N,D]\n  cholesky_factor_corr[D] L_corr_d; // cholesky correlation between factors\n  vector<lower=0>[P] sd_p; // residual sd for each variable\n  vector<lower=-30, upper=30>[n_lam] lam_UNC; // A bunch of lightly-constrained factor loadings\n}\ntransformed parameters {\n  // a vector to hold the factor means, which will be a bunch of 0s\n  vector[D] M;\n  M = rep_vector(0, D);\n  \n  // a vector to hold the factor SDs, which will be a bunch of 1s. \n  vector<lower=0>[D] Sd_d;\n  Sd_d = rep_vector(1, D);\n  \n  // A vector to hold the linear predictors for the manifest variables, which are each a deterministic function of loading*factor_score\n  array[N] vector[P] mu_UNC;\n  for (i in 1 : N) {\n    mu_UNC[i, 1] = lam_UNC[1] * FS_UNC[i, 1];\n    mu_UNC[i, 2] = lam_UNC[2] * FS_UNC[i, 1];\n    mu_UNC[i, 3] = lam_UNC[3] * FS_UNC[i, 1];\n    mu_UNC[i, 4] = lam_UNC[4] * FS_UNC[i, 2];\n    mu_UNC[i, 5] = lam_UNC[5] * FS_UNC[i, 2];\n    mu_UNC[i, 6] = lam_UNC[6] * FS_UNC[i, 2];\n  }\n  \n  // the var-covar matrix for the factors, a deterministic function of the deterministic SDs and the estimated rhos defined in parameters{}\n  cholesky_factor_cov[D] L_Sigma;\n  L_Sigma = diag_pre_multiply(Sd_d, L_corr_d);\n}\nmodel {\n  // Declare some priors\n  L_corr_d ~ lkj_corr_cholesky(1); // Prior on factor corrs\n  lam_UNC ~ normal(0, 10); // Prior on loadings\n  sd_p ~ cauchy(0, 2.5); // Prior on residual SDs of manifest variables\n  \n  // Set up the likelihoods of the manifest and latent factors\n  for (i in 1 : N) {\n    for (j in 1 : P) {\n      // Manifest variables\n      X[i, j] ~ normal(mu_UNC[i, j], sd_p[j]);\n    }\n    // Latent factors\n    FS_UNC[i] ~ multi_normal_cholesky(M, L_Sigma);\n  }\n}\ngenerated quantities {\n  array[N] real log_lik; // log likelihood of each datapoint\n  real dev; // global deviance\n  real log_lik0; // global log likelihood\n  vector[N] log_lik_row;\n  \n  cov_matrix[P] Sigma; // Covariance matrix of the manifest variables\n  cov_matrix[D] Phi_lv; // Covariance matrix of the latent factors\n  matrix[P, P] lambda_phi_lambda; // I think these are the terms of the reconstructed var-covar matrix of the data?\n  cholesky_factor_cov[P] L_Sigma_model; // I think this is the cholesky decomposition of the reconstructued var-covar matrix above?\n  matrix[P, P] theta_del; // I think this is Matrix containing the error terms for the reconstructed empirical var-covar matrix?\n  \n  corr_matrix[D] Rho_UNC; /// correlation matrix\n  corr_matrix[D] Rho; /// correlation matrix\n  matrix[P, D] lam; // factor loadings\n  matrix[N, D] FS; // factor scores, matrix of order [N,D]\n  \n  // Do some fancy things to sign-correct the parameter estimates.\n  // The idea seems to be that when we estimate the loadings with unconstrained signs\n  // It can lead to identification issues. So we take these steps to correct the signs.\n  // See this Stan forum comment for more: https://discourse.mc-stan.org/t/non-convergence-of-latent-variable-model/12450/10?\n  Rho_UNC = multiply_lower_tri_self_transpose(L_corr_d);\n  Rho = Rho_UNC;\n  FS = FS_UNC;\n  lam = rep_matrix(0, P, D);\n  lam[1 : 3, 1] = to_vector(lam_UNC[1 : 3]);\n  lam[4 : 6, 2] = to_vector(lam_UNC[4 : 6]);\n  \n  // factor 1\n  if (lam_UNC[1] < 0) {\n    lam[1 : 3, 1] = to_vector(-1 * lam_UNC[1 : 3]);\n    FS[ : , 1] = to_vector(-1 * FS_UNC[ : , 1]);\n    \n    if (lam_UNC[4] > 0) {\n      Rho[1, 2] = -1 * Rho_UNC[1, 2];\n      Rho[2, 1] = -1 * Rho_UNC[2, 1];\n    }\n  }\n  // factor 2\n  if (lam_UNC[4] < 0) {\n    lam[4 : 6, 2] = to_vector(-1 * lam_UNC[4 : 6]);\n    FS[ : , 2] = to_vector(-1 * FS_UNC[ : , 2]);\n    \n    if (lam_UNC[1] > 0) {\n      Rho[2, 1] = -1 * Rho_UNC[2, 1];\n      Rho[1, 2] = -1 * Rho_UNC[1, 2];\n    }\n  }\n  \n  /// marginal log-likelihood based on signed corrected parameters\n  Phi_lv = quad_form_diag(Rho, Sd_d);\n  lambda_phi_lambda = quad_form_sym(Phi_lv, transpose(lam));\n  theta_del = diag_matrix(sd_p);\n  \n  Sigma = lambda_phi_lambda + theta_del;\n  L_Sigma_model = cholesky_decompose(Sigma);\n  \n  for (i in 1 : N) {\n    log_lik[i] = multi_normal_cholesky_lpdf(X[i] | rep_vector(0, P), L_Sigma_model);\n  }\n  \n  log_lik0 = sum(log_lik); // global log-likelihood\n  dev = -2 * log_lik0; // model deviance\n}\nNow we can compile and run the model\n\n# Write the raw Stan code from an R string to a Stan file\nstan_file <- cmdstanr::write_stan_file(stan_model_code)\n\n# Autoformat the model syntax to preempt any warnings at compile time\nmodel <- cmdstanr::cmdstan_model(stan_file, compile = FALSE)\nmodel$format(canonicalize = TRUE)\n\n# Compile the model from the Stan file\nmodel <- cmdstanr::cmdstan_model(stan_file)\n\n# Put the data into a list format Stan can understand\nX <- as.matrix(fake_dat)\nP <- ncol(fake_dat) \nN <- nrow(fake_dat)\nD <- 2\n\ndata_list <- list(N=N,P=P,D=D,X=X, n_lam=6)\nparam <- c(\"lam\",\"Rho\",\"sd_p\",\"M\",\"Sd_d\",\n           \"log_lik0\",\"dev\",\"log_lik\")\n\n# Fit the model\nfit <- model$sample(\n  data = data_list,\n  seed = 199,\n  chains = 4,\n  parallel_chains = 4,\n  iter_warmup = 1000,\n  iter_sampling = 6000,\n  adapt_delta=0.99, \n  max_treedepth = 12,\n  refresh = 1 # print update every 500 iters\n)\n\n# Save the resulting model fit\nsaveRDS(fit, \"fits/b07.02.rds\")\n\nThe model took about 5 hours to sample and the resulting files are enormous – on the order of 2.5GB each. Fortunately we are able to selectively load only the draws from variables we’re directly interested in, so we have no problem just working in memory.\n\n# Load the cmdstanr model object\nfit <- readRDS(\"fits/b07.02.rds\")\n\n# Load the MCMC draws from the variables we care about\ndraws <- fit$draws(\n  variables = c(\n  \"lam[1,1]\",\n  \"lam[2,1]\",\n  \"lam[3,1]\",\n  \"lam[4,2]\",\n  \"lam[5,2]\",\n  \"lam[6,2]\",\n  \"Rho[2,1]\"\n  )\n)\n\n# Save the result\nsaveRDS(draws, \"fits/b07.02.samples.rds\")\n\nNow we can explore the results. One important point is that we can interpret the Stan model’s estimates of the correlation coefficient between the two factors directly as covariances (IE analogous to the true parameter simulated with lavaan), because we fixed the factor standard deviations to be equal to 1. IE because our sigmas are both equal to 1, this equation: \\(\\rho(X, Y) = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\\)\nJust becomes this: \\(\\rho(X, Y) = \\text{Cov}(X, Y)\\)\nWe’re interested in the loadings and the between-factor covariance. So we can wrangle the MCMC samples for those variables and plot them to look at the estimated posteriers. Based on this plot it looks like the model did pretty well – the posterior peak is close to the true value for all of the loadings:\n\n# Load the draws for the loadings and correlation \ndraws <- readRDS(\"fits/b07.02.samples.rds\")\n\n# Plot\ndraws |>\n\n  # Get the raw draws for the factor loadings and the beween-factor correlation coefficient\n  gather_draws(\n    `lam[1,1]`,\n    `lam[2,1]`,\n    `lam[3,1]`,\n    `lam[4,2]`,\n    `lam[5,2]`,\n    `lam[6,2]`,\n    `Rho[2,1]`\n  ) |>\n\n  # Do some renaming for clarity\n  mutate(.variable = case_when(\n    .variable == \"lam[1,1]\" ~ \"Loading for m1\",\n    .variable == \"lam[2,1]\" ~ \"Loading for m2\",\n    .variable == \"lam[3,1]\" ~ \"Loading for m3\",\n    .variable == \"lam[4,2]\" ~ \"Loading for m4\",\n    .variable == \"lam[5,2]\" ~ \"Loading for m5\",\n    .variable == \"lam[6,2]\" ~ \"Loading for m6\",\n    .variable == \"Rho[2,1]\" ~ \"Covariance for f1 ~ f2\",\n  )) |>\n\n  # Add the true factor loadings for plotting\n  mutate(true_loading = case_when(\n    .variable == \"Loading for m1\" ~ .8,\n    .variable == \"Loading for m2\" ~ .6,\n    .variable == \"Loading for m3\" ~ .9,\n    .variable == \"Loading for m4\" ~ .1,\n    .variable == \"Loading for m5\" ~ .2,\n    .variable == \"Loading for m6\" ~ -.4,\n    .variable == \"Covariance for f1 ~ f2\" ~ .4\n  )) |>\n\n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(fill = \"mediumorchid\") +\n  geom_vline(aes(xintercept = true_loading), linetype = 2) + \n  scale_x_continuous(expand = c(0, 0.015)) +\n  scale_y_continuous(expand = c(0, 0.015)) +\n  guides(fill = \"none\") +\n  labs(x = \"lab\",\n     y = NULL)  +\n  facet_wrap(~.variable) +\n  theme_minimal() + \n  theme(panel.grid.major = element_blank())\n\n\n\n\nIn this exampe we can feel good about model performance because it successfully recovered the true simulation parameter estimates for the samples of interest. But in a real application where we don’t know the true parameter values we would also want to look at the model convergence diagnostics such as rhat and effective number of samples. Here we see that we have rhats slightly greater than 1 for a few of the loadings and for rho. We also see that loading 4, loading 6, and rho have a pretty low number of effective samples. This is cause for concern, and in a real application I would probably increase the number of MCMC iterations to see if we can improve that sample size. The trace plots also look a bit weird at times for a few of the parameters, with some occasional spikes and some parts where one chain goes berzerk relative to the others. But overall they seem to show healthy mixing.\n\n# Create a summary table\ndraws |>\n\n  summary() |>\n\n  select(variable, rhat, ess_bulk, ess_tail) |>\n\n  mutate(across(where(is.numeric), ~round(., 2))) |>\n\n  knitr::kable()\n\n\n\n\nvariable\nrhat\ness_bulk\ness_tail\n\n\n\n\nlam[1,1]\n1.00\n8935.85\n15354.16\n\n\nlam[2,1]\n1.00\n16440.47\n18512.91\n\n\nlam[3,1]\n1.00\n6788.10\n11132.78\n\n\nlam[4,2]\n1.01\n806.68\n450.49\n\n\nlam[5,2]\n1.00\n1929.60\n2266.63\n\n\nlam[6,2]\n1.02\n345.23\n122.68\n\n\nRho[2,1]\n1.02\n294.12\n124.70\n\n\n\n\n# Look at traceplots\n# fit |> \n\n#  bayesplot::mcmc_trace()\n\nOr instead we can go with {{blavaan}}. The model fits way faster than under the raw Stan approach and recovers the true parameter estimates, but is less flexible overall. Crucially for the present analysis, there is no way to do survival analysis with {{blavaan}}.\nBut for posterity, below is the syntax we would use to fit the model with {{blavaan}}. If we specify mcmcfile = TRUE then the resulting Stan file gets written to a new directory called lavExport, or we can supply a filepath as the argument in which case the Stan file will get written there. The file is very large (over 1000 lines) and confusing, I think because it is precompiled to handle any possible model you could specify in R. So it doesn’t seem practical to really get in there and customize things directly. The Stan documentation includes an example for specifying custom priors in a blavaan model\n\n# Fit the model\nblavfit.1 <- bcfa(correlated.factors.model, data=fake_dat, mcmcfile = T)\n\n# Gaze at parameter estimates\nblavfit.1 |> summary()\n\n# Make sure the MCMC diagnostics are ok\nblavInspect(blavfit.1, \"mcobj\")"
  },
  {
    "objectID": "bayesian-cfa.html#third-example-mtmm",
    "href": "bayesian-cfa.html#third-example-mtmm",
    "title": "6  Bayesian CFA",
    "section": "6.3 Third Example: MTMM",
    "text": "6.3 Third Example: MTMM\nIn the previous section we moved beyond {{brms}} into raw Stan, which allowed us to estimate the correlation between latent variables. But often in latent variable modelling we also want to account for the possibility that certain measured variables are confounded by unmeasured features. And rather than declaring a new factor for these unmeasured features, it can be simpler to estimate a covariance term for these measured variables in the variance-covariance matrix of the shared multivariable likelihood function. So now the off-diagonals are not entirely 0. This is what Brown (2006) calls a ‘correlated uniqueness model’. I have more detailed notes on these ideas in Chapter 3.\nFor example, say we have two latent factors each with 3 measurements. We’re worried that the first measurement of each variable (m1 and m4) are confounded because they were both collected by participant survey, while the other four measures were each collected by different means. We can simulate data from this DAG using {{lavaan}} like we did for the two models above.\nFirst we’ll need to add a new parameter to our Stan model to account for the possibility of shared method confounding. This only involves a few changes, namely:\ndata {\n  int N; // sample size\n  int P; // number of variables\n  int D; // number of factors\n  array[N] vector[P] X; // data matrix of order [N,P]\n  int n_lam; // how many factor loadings are estimated\n}\nparameters {\n  matrix[N, D] FS_UNC; // factor scores, matrix of order [N,D]\n  cholesky_factor_corr[D] L_corr_d; // cholesky correlation between factors\n  cholesky_factor_corr[2] L_corr_m1_m4; // cholesky correlation between the m1 and m4\n  vector<lower=0>[P] sd_p; // residual sd for each variable\n  vector<lower=-30, upper=30>[n_lam] lam_UNC; // A bunch of lightly-constrained factor loadings\n}\ntransformed parameters {\n  // a vector to hold the factor means, which will be a bunch of 0s\n  vector[D] M;\n  M = rep_vector(0, D);\n  \n  // a vector to hold the factor SDs, which will be a bunch of 1s. \n  vector<lower=0>[D] Sd_d;\n  Sd_d = rep_vector(1, D);\n  \n  // A vector to hold the linear predictors for the manifest variables, which are each a deterministic function of loading*factor_score\n  array[N] vector[P] mu_UNC;\n  for (i in 1 : N) {\n    mu_UNC[i, 1] = lam_UNC[1] * FS_UNC[i, 1];\n    mu_UNC[i, 2] = lam_UNC[2] * FS_UNC[i, 1];\n    mu_UNC[i, 3] = lam_UNC[3] * FS_UNC[i, 1];\n    mu_UNC[i, 4] = lam_UNC[4] * FS_UNC[i, 2];\n    mu_UNC[i, 5] = lam_UNC[5] * FS_UNC[i, 2];\n    mu_UNC[i, 6] = lam_UNC[6] * FS_UNC[i, 2];\n  }\n  \n  // the var-covar matrix for the factors, a deterministic function of the deterministic SDs and the estimated rhos defined in the parameters block\n  cholesky_factor_cov[D] L_Sigma;\n  L_Sigma = diag_pre_multiply(Sd_d, L_corr_d);\n\n  // Same, but for the var-covar matrix of m1 and m4\n  cholesky_factor_cov[D] L_Sigma_m1_m4;\n  L_Sigma_m1_m4 = diag_pre_multiply(sd_p[{1, 4}], L_corr_m1_m4);\n}\nmodel {\n  // Declare some priors\n  L_corr_d ~ lkj_corr_cholesky(1); // Prior on factor corrs\n  L_corr_m1_m4 ~ lkj_corr_cholesky(1); // Prior on corr between m1 and m4\n  lam_UNC ~ normal(0, 10); // Prior on loadings\n  sd_p ~ cauchy(0, 2.5); // Prior on residual SDs of manifest variables\n\n  // Set up the joint log likelihood function\n  for (i in 1 : N) {\n    // The uncorrelated measured variables\n    for (j in {2,3,5,6}) {\n      // Manifest variables\n      X[i, j] ~ normal(mu_UNC[i, j], sd_p[j]);\n    }\n\n    // m1 and m4, which get special treatment for being correlated\n    to_vector({X[i, 1], X[i, 4]}) ~ multi_normal_cholesky([mu_UNC[i, 1], mu_UNC[i, 4]]', L_Sigma_m1_m4);\n\n    // Latent factors\n    FS_UNC[i] ~ multi_normal_cholesky(M, L_Sigma);\n  }\n}\ngenerated quantities {\n  corr_matrix[D] Rho_UNC; /// correlation matrix for factors\n  corr_matrix[D] Rho_factors; /// correlation matrix for factors\n  corr_matrix[D] Rho_m1_m4; /// correlation matrix for m1 and m4\n  matrix[P, D] lam; // factor loadings\n  matrix[N, D] FS; // factor scores, matrix of order [N,D]\n  \n  // Do some fancy things to sign-correct the parameter estimates.\n  // The idea seems to be that when we estimate the loadings with unconstrained signs\n  // It can lead to identification issues. So we take these steps to correct the signs.\n  // See this Stan forum comment for more: https://discourse.mc-stan.org/t/non-convergence-of-latent-variable-model/12450/10\n  Rho_UNC = multiply_lower_tri_self_transpose(L_corr_d);\n  Rho_factors = Rho_UNC;\n  Rho_m1_m4 = multiply_lower_tri_self_transpose(L_corr_m1_m4);\n  FS = FS_UNC;\n  lam = rep_matrix(0, P, D);\n  lam[1 : 3, 1] = to_vector(lam_UNC[1 : 3]);\n  lam[4 : 6, 2] = to_vector(lam_UNC[4 : 6]);\n  \n  // factor 1\n  if (lam_UNC[1] < 0) {\n    lam[1 : 3, 1] = to_vector(-1 * lam_UNC[1 : 3]);\n    FS[ : , 1] = to_vector(-1 * FS_UNC[ : , 1]);\n    \n    if (lam_UNC[4] > 0) {\n      Rho_factors[1, 2] = -1 * Rho_UNC[1, 2];\n      Rho_factors[2, 1] = -1 * Rho_UNC[2, 1];\n    }\n  }\n  // factor 2\n  if (lam_UNC[4] < 0) {\n    lam[4 : 6, 2] = to_vector(-1 * lam_UNC[4 : 6]);\n    FS[ : , 2] = to_vector(-1 * FS_UNC[ : , 2]);\n    \n    if (lam_UNC[1] > 0) {\n      Rho_factors[2, 1] = -1 * Rho_UNC[2, 1];\n      Rho_factors[1, 2] = -1 * Rho_UNC[1, 2];\n    }\n  }\n}\nNow we can compile and run the model\n\n# Write the raw Stan code from an R string to a Stan file\nstan_file <- cmdstanr::write_stan_file(stan_model_code)\n\n# Autoformat the model syntax to preempt any warnings at compile time\nmodel <- cmdstanr::cmdstan_model(stan_file, compile = FALSE)\nmodel$format(canonicalize = TRUE)\n\n# Compile the model from the Stan file\nmodel <- cmdstanr::cmdstan_model(stan_file)\n\n# Declare the new lavaan model where m1 and m4 covary\nmtmm.model <- ' \n  f1 =~ .8*m1 + .6*m2 + .9*m3\n  f2 =~ .1*m4 + .2*m5 + -.4*m6\n  f1 ~~ .4*f2\n  m1 ~~ .4*m4\n'\n\n# Simulate data from the lavaan model\nfake_dat <- lavaan::simulateData(model = mtmm.model, sample.nobs = 4000)\n\n# Put the data into a list format Stan can understand\nX <- as.matrix(fake_dat)\nP <- ncol(fake_dat) \nN <- nrow(fake_dat)\nD <- 2\n\ndata_list <- list(N=N,P=P,D=D,X=X, n_lam=6)\n\n# Fit the model\nfit <- model$sample(\n  data = data_list,\n  seed = 199,\n  chains = 4,\n  parallel_chains = 4,\n  iter_warmup = 1000,\n  iter_sampling = 6000,\n # adapt_delta=0.99, \n # max_treedepth = 12,\n  refresh = 1 # print update at each iter\n)\n\n# Save the resulting model fit\nsaveRDS(fit, \"fits/b07.03.rds\")\n\nOnce again, the resulting object is enormous. So let’s pare it down to only contain the draws of substantive interest.\n\n# Load the cmdstanr model object\nfit <- readRDS(\"fits/b07.03.rds\")\n\n# Load the MCMC draws from the variables we care about\ndraws <- fit$draws(\n  variables = c(\n  \"lam[1,1]\",\n  \"lam[2,1]\",\n  \"lam[3,1]\",\n  \"lam[4,2]\",\n  \"lam[5,2]\",\n  \"lam[6,2]\",\n  \"Rho_factors[2,1]\", \n  \"Rho_m1_m4[2,1]\"\n  )\n)\n\n# Save the result\nsaveRDS(draws, \"fits/b07.03.samples.rds\")\n\nHow did that go? We can plot the draws and look at the MCMC diagnostics like we did above.\n\n# Load the draws saved in the previous block\ndraws <- readRDS(\"fits/b07.03.samples.rds\")\n\n# Plot\ndraws |>\n\n  # Get the raw draws for the factor loadings and the beween-factor correlation coefficient\n  gather_draws(\n   `lam[1,1]`,\n   `lam[2,1]`,\n   `lam[3,1]`,\n   `lam[4,2]`,\n   `lam[5,2]`,\n   `lam[6,2]`,\n   `Rho_factors[2,1]`,\n   `Rho_m1_m4[2,1]`,\n  ) |>\n\n  # Do some renaming for clarity\n  mutate(.variable = case_when(\n    .variable == \"lam[1,1]\" ~ \"Loading for m1\",\n    .variable == \"lam[2,1]\" ~ \"Loading for m2\",\n    .variable == \"lam[3,1]\" ~ \"Loading for m3\",\n    .variable == \"lam[4,2]\" ~ \"Loading for m4\",\n    .variable == \"lam[5,2]\" ~ \"Loading for m5\",\n    .variable == \"lam[6,2]\" ~ \"Loading for m6\",\n    .variable == \"Rho_factors[2,1]\" ~ \"Covariance for f1 ~ f2\",\n    .variable == \"Rho_m1_m4[2,1]\" ~ \"Covariance for m1 ~ m4\"\n  )) |>\n\n  # Add the true factor loadings for plotting\n  mutate(true_loading = case_when(\n    .variable == \"Loading for m1\" ~ .8,\n    .variable == \"Loading for m2\" ~ .6,\n    .variable == \"Loading for m3\" ~ .9,\n    .variable == \"Loading for m4\" ~ .1,\n    .variable == \"Loading for m5\" ~ .2,\n    .variable == \"Loading for m6\" ~ -.4,\n    .variable == \"Covariance for f1 ~ f2\" ~ .4,\n    .variable == \"Covariance for m1 ~ m4\" ~ .4\n  )) |>\n\n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(fill = \"mediumorchid\") +\n  geom_vline(aes(xintercept = true_loading), linetype = 2) + \n  scale_x_continuous(expand = c(0, 0.015)) +\n  scale_y_continuous(expand = c(0, 0.015)) +\n  guides(fill = \"none\") +\n  labs(x = \"lab\",\n     y = NULL)  +\n  facet_wrap(~.variable) +\n  theme_minimal() + \n  theme(panel.grid.major = element_blank())"
  },
  {
    "objectID": "sem-intro.html",
    "href": "sem-intro.html",
    "title": "Structural Equation Modelling",
    "section": "",
    "text": "Someone on the stan forums recommended this book for a bayesian example: https://www.guilford.com/books/Bayesian-Structural-Equation-Modeling/Sarah-Depaoli/9781462547746"
  },
  {
    "objectID": "latent-variable-regression.html",
    "href": "latent-variable-regression.html",
    "title": "7  Example: Survival Analysis with Latent Variables",
    "section": "",
    "text": "The line between SEM and classic regression starts to blur when we realize we can mix-and-match latent and observed variables in a regression, defining whichever relationships make the most sense given our background knowledge.\nA traditional way of incorporating a CFA measurement model into a fuller regression analysis proceeds in two steps:\nBut this approach is unsatisfactory because factor scores are a function of the CFA model’s parameter estimates (the estimates of the factor loadings), about which there is uncertainty. So if we only give our substantive regression model a point-estimate factor score from the CFA model, we ignore the uncertainty contained in the standard errors of the CFA model’s factor loading estimates. This is the approach taken by Kankaraš, Feron, and Renbarger (2019); they use point-estimate factor scores as predictors in subsequent regressions. In some cases they even do this iteratively, fitting measurement models on point-estimate factor scores from measurement models fit on point-estimate factor scores, and using those point-estimates as regression predictors! In their analysis they don’t even report the standard errors of the factor loadings from their measurement models.\nA better option is to fit a Bayesian model that carries out both the measurement model and the substantive regression model simultaneously, so that the model can incorporate its uncertainty from the CFA model into its substantive parameter estimates and predictions. This section provides an example of how to implement just such a Bayesian model in a survival analysis context in STAN via the brms R package."
  },
  {
    "objectID": "latent-variable-regression.html#background-and-data-simulation",
    "href": "latent-variable-regression.html#background-and-data-simulation",
    "title": "7  Example: Survival Analysis with Latent Variables",
    "section": "7.1 Background and Data Simulation",
    "text": "7.1 Background and Data Simulation\n\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(ordinal)\n\nFor this example we’ll simulate some data related to the subject matter of Kankaraš, Feron, and Renbarger (2019). Specifically, we’ll simulate some data in which latent social and emotional skills predict time-to-employment to varying degrees. More specifically, here is what the dataset should contain:\n\nn participants in a skills training program;\nSome continuous and categorical demographic variables for each participant, such as age, gender priority group (this should be TRUE/FALSE), barriers to employment, and n previous services accessed.\na latent variable called ‘communication’ with an arbitrary scale of 0-100.\na scale of 5 measurements each for this latent variable, each on a likert scale from 1-5. These should each be a function of the corresponding latent variable plus noise, where the degree of correlation should be controllable by the user via function arguments.\nthe outcome variable, time, which should be a function of the latent variables and the demographhic variables and noise\na status variable indicating whether time refers to time-to-employment or time-to-censoring. This should be random for non-informative censoring, with the probability of censoring being controllable by the user.\n\nFor now we’ll assume the latent variables are not correlated, even though this is a bit of a silly assumption in practice.\nOne improvement we can make on the traitional factor analyses we looked at in previous sections is that we can specify our measurement model with ordinal relationships between the factor and its measurements. This makes sense given that the measurements are likert, andd it doesn’t make sense to assume likert variables move linearly.\nIt would be most efficient to just define a function that creates the dataset we need based on various parameters. But I’ll instead go step-by-step and explain each choice as we go. First we can define the dataframe and do the easy work of adding the variables that don’t depend on any of the others:\n\nn = 60000\ndat <- tibble(\n\n        # Simulate demographic variables\n        age = runif(n, 20, 60),\n        gender_priority_group = sample(c(TRUE, FALSE), n, replace = TRUE),\n        barriers_to_employment = rpois(n, lambda = 1),\n        previous_services = rpois(n, lambda = 3), \n\n        # Simulate latent variables on an arbitrary scale\n        adaptability = runif(n, 0, 100),\n        collaboration = runif(n, 0, 100),\n        communication = runif(n, 0, 100)\n\n)\n\nNow we’ll simulate the likert-scale ‘measurements’ of the latent variables we defined above. The first step will be to define the threshold coefficients, or what McElreath (2020) calls ‘cutpoints’. These are just the intercepts of the linear models that define each of the cumulative probabilities via a logit link. Another way of thinking about these is that they are the logit of each cumulative probability when the substantive predictors in the model are all equal to 0.\nSo how shall we choose these for our example? We can do whatever we want. Let’s imagine a situation where the people grading these participants have a tendency to give out lots of 3s and 5s, and relatively very few of the other options. Since we’ve defined our latent variables on an arbitrary scale from 1-100, we can define the threshold coefficients to slice up that scale to give us the pattern we want. We can also define some ‘true’ factor loadings that determine the degree to which the measurements are actually influenced by the underlying latent variable. Usually in a real-world context we find that some measurements are tighter than others.\n\n# Define the threshold coefficients\ntheta <- c(\n    5,  # You get a 1 if your true 'latent' score is <5 and all predictors are 0. \n    50, # You get a 2 if your true 'latent' score is between 5 and 50.\n    60, # You get a 3 if your true 'latent' score is between 50 and 60.\n    65  # You get a 4 if your true 'latent' score is between 60 and 65.\n#   NA  # You get a 5 if your true 'latent' score is >65.\n)\n\n# Define the factor loadings\nloadings <- c(\n    1,\n    1,\n    1,\n    1,\n    1\n)\n\nNow we can use those cutpoints to generate some data. Here I use a slightly modified version of the simulation approach for ordinal outcomes given in this STAN forum thread by Conor Goold.\n\n# Define a function to generate a single ordinal data point\n# @K number of ordinal categories\n# @theta vector of latent cutpoints\n# @eta linear predictor\ngen_ordinal_measurements <- function(K, theta, eta, loading){\n\n   if(length(theta) != (K - 1))\n        stop(paste0(\"theta must have length \", K - 1, \" not \", length(theta)))\n\n   n <- length(eta)\n   result <- numeric(n)\n\n   for(i in 1:n) {\n       probs <- numeric(K)\n       for(k in 1:K){\n           if(k == 1) prev_thresh = -Inf\n           else prev_thresh = theta[k - 1]\n\n           if(k == K) next_thresh = Inf\n           else next_thresh = theta[k]\n\n           probs[k] = plogis(next_thresh - (eta[i] * loading)) - plogis(prev_thresh - (eta[i] * loading))\n       }\n       result[i] <- sample(K, 1, prob=probs)\n   }\n  \n   return(result)\n   \n}\n\n# Simulate the measured outcomes\ndat <- dat |>\n\n    # Simulate items for latent variable measurements\n    mutate(\n\n        communication_m1 = gen_ordinal_measurements(5, theta, communication, loadings[1]),\n        communication_m2 = gen_ordinal_measurements(5, theta, communication, loadings[2]),\n        communication_m3 = gen_ordinal_measurements(5, theta, communication, loadings[3]),\n        communication_m4 = gen_ordinal_measurements(5, theta, communication, loadings[4]),\n        communication_m5 = gen_ordinal_measurements(5, theta, communication, loadings[5])\n\n    )\n\nWe can do some quick visualization to make sure the data have that ‘mostly 2s and 5s’ pattern we were going for, but that it varies slightly such that measured variables with higher loadings tend to have more 3s and 4s than variables with lower loadings (I think?)\n\ndat |> \n\n   select(matches('^communication')) |>\n\n   pivot_longer(matches(\"_m\\\\d+$\"), names_to = \"var\", values_to = \"measurement\") |>\n\n   ggplot(aes(x = communication, y = measurement)) + \n   geom_point() + \n   facet_wrap(~var)\n\n\n\nmodel='communication ~ communication_m1+communication_m2+communication_m3+communication_m4+communication_m5'\n\n# lavaan test\nmodel.ord = cfa(model, data=dat |> rename(\"communication\" = communication), ordered=c(\n    \"communication_m1\",\n    \"communication_m2\",\n    \"communication_m3\",\n    \"communication_m4\",\n    \"communication_m5\"\n    )   \n)\n\nWarning in lav_data_full(data = data, group = group, cluster = cluster, : lavaan\nWARNING: exogenous variable(s) declared as ordered in data: communication_m1\ncommunication_m2 communication_m3 communication_m4 communication_m5\n\n\nWarning in lav_partable_check(lavpartable, categorical =\nlavoptions$.categorical, : lavaan WARNING: parameter table does not contain\nthresholds\n\nWarning in lav_partable_check(lavpartable, categorical =\nlavoptions$.categorical, : lavaan WARNING: parameter table does not contain\nthresholds\n\nmodel.ord |> summary()\n\nlavaan 0.6.16 ended normally after 17 iterations\n\n  Estimator                                       DWLS\n  Optimization method                           NLMINB\n  Number of model parameters                         7\n\n  Number of observations                         60000\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                 0.000       0.000\n  Degrees of freedom                                 0           0\n\nParameter Estimates:\n\n  Standard errors                           Robust.sem\n  Information                                 Expected\n  Information saturated (h1) model        Unstructured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  communication ~                                     \n    communicatn_m1    3.838    0.198   19.387    0.000\n    communicatn_m2    3.762    0.199   18.941    0.000\n    communicatn_m3    3.529    0.199   17.724    0.000\n    communicatn_m4    3.952    0.198   19.954    0.000\n    communicatn_m5    3.744    0.197   19.046    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .communication   -10.237    0.113  -90.748    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .communication   114.510    0.438  261.655    0.000\n\n\nLastly, we’ll simulate the outcome variables for survival analysis. We’ll use a Weibull likelihood. This is a popular choice for parametric survival analysis because it can fit a pretty flexible monotonic baseline hazard function, and can be interpretted as both a proportional hazards and an acceleration failure time model, which gives us flexibility in how we communicate results and frees us from the burden of needing to check the proportional hazards assumption.\nCould make it fancy by clustering people and doing it multilevel?\n\n\n\n\nclm(factor(communication_m1) ~ communication, data = dat)\n\nformula: factor(communication_m1) ~ communication\ndata:    dat\n\n link  threshold nobs  logLik   AIC      niter max.grad cond.H \n logit flexible  60000 -7874.79 15759.59 12(0) 1.90e-10 8.1e+06\n\nCoefficients:\ncommunication \n       0.9979 \n\nThreshold coefficients:\n   1|2    2|3    3|4    4|5 \n 5.049 49.908 59.864 64.845 \n\ndat |> \n\n #   filter(adaptability_m1 %in% c(1, 2, 3)) |>\n\n    count(communication_m1) |>\n\n    mutate(p = n / sum(n))\n\n# A tibble: 5 × 3\n  communication_m1     n      p\n             <dbl> <int>  <dbl>\n1                1  3018 0.0503\n2                2 27017 0.450 \n3                3  5984 0.0997\n4                4  3052 0.0509\n5                5 20929 0.349 \n\n\nThe idea is that if the latent variables predict time-to-employment then that is consistent with them being well-measured, etc.\nThis example has closely mirrored an analysis I carried out for a real client. In that example the latent variables had no predictive validity."
  },
  {
    "objectID": "latent-variable-regression.html#prior-predictive-checks",
    "href": "latent-variable-regression.html#prior-predictive-checks",
    "title": "7  Example: Survival Analysis with Latent Variables",
    "section": "7.2 Prior Predictive Checks",
    "text": "7.2 Prior Predictive Checks"
  },
  {
    "objectID": "latent-variable-regression.html#fitting-the-model",
    "href": "latent-variable-regression.html#fitting-the-model",
    "title": "7  Example: Survival Analysis with Latent Variables",
    "section": "7.3 Fitting the Model",
    "text": "7.3 Fitting the Model"
  },
  {
    "objectID": "latent-variable-regression.html#modelling-likert-measurements-as-ordinal-variables",
    "href": "latent-variable-regression.html#modelling-likert-measurements-as-ordinal-variables",
    "title": "7  Example: Survival Analysis with Latent Variables",
    "section": "7.4 Modelling Likert Measurements as Ordinal Variables",
    "text": "7.4 Modelling Likert Measurements as Ordinal Variables\nWe can improve the model."
  },
  {
    "objectID": "latent-variable-regression.html#using-raw-stan-for-correlated-factors",
    "href": "latent-variable-regression.html#using-raw-stan-for-correlated-factors",
    "title": "7  Example: Survival Analysis with Latent Variables",
    "section": "7.5 Using Raw STAN for Correlated Factors",
    "text": "7.5 Using Raw STAN for Correlated Factors\n\n\n\n\nKankaraš, Miloš, Eva Feron, and Rachel Renbarger. 2019. “Assessing Students’ Social and Emotional Skills Through Triangulation of Assessment Methods,” no. 208. https://doi.org/https://doi.org/https://doi.org/10.1787/717ad7f2-en.\n\n\nMcElreath, Richard; 2020. Statistical Rethinking: A Bayesian Course with Examples in r and STAN. 2nd ed. CRC Press LLC."
  },
  {
    "objectID": "bayesian-cfa.html#fourth-example-survival-analysis",
    "href": "bayesian-cfa.html#fourth-example-survival-analysis",
    "title": "6  Bayesian CFA",
    "section": "6.4 Fourth Example: Survival Analysis",
    "text": "6.4 Fourth Example: Survival Analysis\nNow that we’re confident our measurement model for the correlated factors is well-specified, we can move on to modelling the outcome of interest: time to employment. Here’s what our full model looks like. The only change from the MTMM model above is the two top lines: now we imagine time-to-event \\(T\\) for each person \\(i\\) to be drawn from a shared Weibull distribution, who’s location parameter is a linear model of the latent factors \\(f_1\\) and \\(f_2\\), as well as other measured variables \\(x\\). Why a Weibull likelihood?\n$$\n\\[\\begin{aligned}\nT_i &\\sim \\text{Weibull}(\\theta_i, k) \\\\\n\\theta_i &= \\alpha + \\beta_1 f_{1i} + \\beta_2 f_{2i} + \\mathbf{X}_i \\boldsymbol{\\beta} \\\\\n\\\\\n\\begin{bmatrix}\nm_{1} \\\\\nm_{2} \\\\\nm_{3} \\\\\nm_{4} \\\\\nm_{5} \\\\\nm_{6}\n\\end{bmatrix}\n&\\sim \\text{MVNormal}\n\\left(\n\\begin{bmatrix}\n\\mu_{m1} \\\\\n\\mu_{m2} \\\\\n\\mu_{m3} \\\\\n\\mu_{m4} \\\\\n\\mu_{m5} \\\\\n\\mu_{m6}\n\\end{bmatrix},\n\\Sigma_m\n\\right) \\\\\n\\\\\n\\mu_{m1} &= \\lambda_1 f_1, \\quad \\mu_{m2} = \\lambda_2 f_1, \\quad \\mu_{m3} = \\lambda_3 f_1 \\\\\n\\mu_{m4} &= \\lambda_4 f_2, \\quad \\mu_{m5} = \\lambda_5 f_2, \\quad \\mu_{m6} = \\lambda_6 f_2 \\\\\n\\\\\n\\begin{bmatrix}\nf_{1} \\\\\nf_{2}\n\\end{bmatrix}\n&\\sim \\text{MVNormal}\n\\left(\n\\begin{bmatrix}\n\\mu_1 \\\\\n\\mu_2\n\\end{bmatrix},\n\\Sigma_f\n\\right) \\\\\n\\\\\n\\Sigma_f &=\n\\begin{bmatrix}\n\\sigma_{f1}^2 & \\rho \\sigma_{f1} \\sigma_{f2} \\\\\n\\rho \\sigma_{f1} \\sigma_{f2} & \\sigma_{f2}^2\n\\end{bmatrix} \\\\\n\\\\\n\\Sigma_m &=\n\\begin{bmatrix}\n\\sigma_{m1}^2 & 0 & 0 & \\rho \\sigma_{m1} \\sigma_{m4} & 0 & 0 \\\\\n0 & \\sigma_{m2}^2 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & \\sigma_{m3}^2 & 0 & 0 & 0 \\\\\n\\rho \\sigma_{m1} \\sigma_{m4} & 0 & 0 & \\sigma_{m4}^2 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & \\sigma_{m5}^2 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & \\sigma_{m6}^2\n\\end{bmatrix}\n\\end{aligned}\\]\n$$\n\n6.4.1 Simulating data\nFor this final section we won’t be able to rely on lavaan::simulateData() to create our simulated data, because it can’t generate time-to-event data. So we’ll need to generate the data ourselves. Mark Lai has shared some nice code to do this, so we can take his approach. For now we’ll simulate the measured variables as continuous and gaussian, even though it is more realistic that in this context they would be ordinal likert-style.\n\n\n6.4.2 Prior Predictive Distributions\nCheck priors for the betas for f1 and f2, and for the sigmas."
  },
  {
    "objectID": "mtmm-and-error-structure-modelling.html",
    "href": "mtmm-and-error-structure-modelling.html",
    "title": "3  MTMM and Error Structure Modelling",
    "section": "",
    "text": "library(tidyverse)\nlibrary(lavaan)\nlibrary(ggdag)\n\nIn the previous example we saw how we can sometimes improve model fit by freeing-up some of the residual covariance terms, rather than doing the typical thing of fixing them at 0. But this feels a bit icky to me – just pumping out some modification indexes and using that as a basis for opening up some free parameters feels pretty overfitty, because we don’t have a strong theory-driven reason for changing the model in that way.\nBut there are more kosher-feeling theory-driven reasons for freeing up some of the residual covariance parameters. Let’s talk about two of them: the first relates to convergent validity, the second relates to discriminant validity.\nHere’s the first example: imagine I have a theory where there’s a thing called ‘exceptional leadership’, and it is made up of 3 unobservable features, like ‘self-confidence’, ‘oratorical skill’, and ‘robust compassionateness’. So I make up a survey where I ask 12 questions total, 4 per imagined factor. Then I fit a CFA model and find that it does a great job recreating the empirical variance-covariance matrix. There’s lots of great convergent validity between the questions I imagine to define the 3 factors. So I get published! But there’s a first problem: what if my within-factor variables are correlated not because they are cleanly confounded by ‘self-confidence’ (which is what I’m trying to convince you of), but instead because the within-factor survey questions are just worded in a really similar way, IE they are confounded by a latent factor we might call ‘wording similarity’? This possibility undermines my case for clean confounding.\nNow the second example: imagine I do the same analysis described above, but I find my discriminant validity actually doesn’t look so hot, IE there are some high between-factor correlations. It is possible that this is just being caused by some of the variables used in different factors being confounded by their shared measurement approach, which creates a backdoor path between the factors.\nAs Brown (2006) puts it:\n\n“when each construct is assessed by the same measurement approach (e.g., observer rating), it cannot be determined how much of the observed overlap (i.e., factor correlations) is due to method effects as opposed to”true” covariance of the traits.”\n\nSo we have these two risks:\n\nMaybe some of my within-factor variables are confounded by method effects, which creates the illusion of convergent validity. If I go to publish my paper and someone raises this concern, then maybe I won’t get published! I’ll need to find a way to make my model control for possible method-confounding and still show good convergent validity.\nMaybe some of my variables of different factors are confounded by method effects, so I don’t end up with great discriminant validity. This would be bad, but fitting a model that controls for method effects can maybe make things better.\n\nFear not: there are two ways of adjusting the model to control for measurement confounding, thereby addressing the above risks.\n\nAdd method-specific factors to my model (to control for them in the linear model of each variable). Brown (2006) calls this a Correlated Methods Model;\nJust freely fit the residual covariances between the observed variables that share a method. Brown (2006) calls this a Correlated Uniqueness Model. Because remember, ‘Uniqueness’ is just a fancy term for variable-specific residual variance.\n\nIt’s all still just basic linear modelling, and trying to show that the model’s results are consistent with the DAG of clean confounding. By adding a method factors or allowing some of the error residuals to be freely fit, I’m controlling for sources of confounding that a reviewer might bring up as a concern, or that might be pulling down my discriminant validity.\nHere’s how these approaches can improve convergent or divergent validity:\nConvergent validity: By adding method-factors to the model or freely fitting the residual covariances between the within-factor questions can help me make the case that “see, even when I allow for correlated errors due to other unobserved confounders (like common wording or common methods), the factors still do a good job recreating the empirical covariance structure, IE the loadings still look good, so my argument for mostly clean confounding is still reasonable.” I think this makes sense?\nDivergent validity: Maybe I can get better discriminant validity, IE reduce the between-factor correlations, by adding those method effects to the linear models, thereby controlling for them. I can do this either by literally adding in some new factors to represent each method, or just by allowing the residual covariances of like-method variables to be freely estimated.\n\nSimulating Data Based on a DAG\nNow let’s look at an example in detail. This example is taken from Brown (2006), chapter 6.\nSome researchers were curious about whether ‘happiness’ and ‘sadness’ are totally separate things vs two sides of a single shared spectrum. I guess the implication is that if they are totally separate things then I could be happy and sad at the same time, whereas if they’re two sides of a spectrum then I can only ever be one or the other.\nThis feels like a good factor analysis question! I can collect a bunch of data that I think map to ‘happy’ and a bunch of other data that I think map to ‘sad’, fit a CFA, and see whether the two factors have discriminant validity.\nThis is exactly what al (n.d.) did. They collected a few columns each for ‘happy’ and ‘sad’, fit a factor model, and fit a CFA. Each within-factor column had its own measurement approach, but shared a measurement approach with one of the columns of the other factor. So we are at risk of our estimate of between-factor correlations being confounding due to shared measurement approach, which could be hurting my case for discriminant validity!\nHere’s how we can show this situation in a DAG:\n\n# Set DAG coordinates\ndag_coords <- list(\n  x = c(\n    F1 = 1, \n    F2 = 1,\n    H1 = 2,\n    H2 = 2,\n    H3 = 2,\n    S1 = 2,\n    S2 = 2,\n    S3 = 2,\n    M1 = 3,\n    M2 = 3,\n    M3 = 3),\n  y = c(\n    F1 = 2.5,\n    F2 = 1.5,\n    H1 = 2.8,\n    H2 = 2.5,\n    H3 = 2.2,\n    S1 = 1.8,\n    S2 = 1.5,\n    S3 = 1.2,\n    M1 = 2.6,\n    M2 = 2,\n    M3 = 1.4\n  )\n)\n\n# Set DAG relationships and aesthetics\nmeasurement_confounding_dag <- ggdag::dagify(\n  H1 ~ F1,\n  H2 ~ F1,\n  H3 ~ F1,\n  S1 ~ F2,\n  S2 ~ F2,\n  S3 ~ F2,\n  H1 ~ M1,\n  S1 ~ M1,\n  H2 ~ M2,\n  S2 ~ M2,\n  H3 ~ M3,\n  S3 ~ M3,\n  coords = dag_coords\n) %>% \n  \n  tidy_dagitty() %>% \n  \n  mutate(\n    \n    node_colour = case_when(\n      grepl(\"^F|M\", name) ~ \"latent\",\n      grepl(\"^H|S\", name) ~ \"observed\"\n    ),\n    \n    edge_colour = case_when(\n      grepl(\"^M\", name) & grepl(\"1$\", to) ~ \"cornflower blue\",\n      grepl(\"^M\", name) & grepl(\"2$\", to) ~ \"#daed64\",\n      grepl(\"^M\", name) & grepl(\"3$\", to) ~ \"#ed7864\",\n      grepl(\"^F\", name)                   ~ \"black\"\n    )\n  )\n\n# Plot the DAG\nmeasurement_confounding_dag %>%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(colour = node_colour)) +\n  scale_colour_manual(values = c(\"dark blue\", \"#edbc64\")) + \n  geom_dag_edges(aes(edge_colour = edge_colour)) +\n  geom_dag_text() +\n  theme_void()\n\n\n\n\nSee how the measurment effects M1, M2, and M3 each create a backdoor path between the two factors F1 and F2. So if I want to get better-seeming (and, under the DAG, more accurate) estimate of between-factor correlation, then I need to find a way to close those backdoor paths. The classic way to close these paths would be to condition on the measurement effects by adding them to the linear model, but I can’t directly do this because they are unmeasured. But, as discussed above, I can still sort of do it by adding them as factors to my CFA model, or by freely estimating residual correlation between the observed variables that share a measurement approach, which should work if my DAG is mostly accurate.\nUnfortunately, the authors of this paper haven’t published their data. But we can take this as an opportunity to practice simulating a dataset with relationships implied by a DAG.\n\n### Simulate Data from the DAG\n\n# Set seed for replicable results\nset.seed(233)\n\n# Set sample size\nN <- 305\n\n# Create the dataset\ndat_fake <- tibble(\n  \n  # The factors are uncorrelated in reality, but\n  # will be confounded by the measurement effects!\n  F1 = rnorm(N, 0, 1),\n  F2 = rnorm(N, 0, 1),\n  \n  # The measurement effects\n  M1 = rnorm(N, 0, 1),\n  M2 = rnorm(N, 0, 1),\n  M3 = rnorm(N, 0, 1),\n  \n  # The DAG says the measurements are fully determined by the latent factors and measurement effects\n  H1 = .8*F1 + 0.7*M1 + rnorm(N, 0, .3),\n  H2 = .7*F1 + 0.7*M2 + rnorm(N, 0, .3),\n  H3 = .9*F1 + 0.7*M3 + rnorm(N, 0, .3),\n  S1 = .8*F2 + 0.7*M1 + rnorm(N, 0, .3),\n  S2 = .7*F2 + 0.7*M2 + rnorm(N, 0, .3),\n  S3 = .9*F2 + 0.7*M3 + rnorm(N, 0, .3) \n) \n\nFun! Now we have our fake data to play with. For starters, since we actually do have the values of the latent variables in our dataset, we can demonstrate how directly controlling for the measurement effects in a regression model can close the backdoor path between the factors.\n\nlist(\n  lm(H1 ~ S1, dat_fake), \n  lm(H1 ~ S1 + M1, dat_fake)\n) %>% \n  \n  map(broom::tidy) %>% \n  \n  knitr::kable()\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.0249271\n0.0572795\n0.435184\n0.6637387\n\n\nS1\n0.4555127\n0.0491323\n9.271137\n0.0000000\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.0466841\n0.0456881\n1.0217995\n0.3076936\n\n\nS1\n-0.0137610\n0.0528505\n-0.2603763\n0.7947509\n\n\nM1\n0.7636731\n0.0577500\n13.2237680\n0.0000000\n\n\n\n\n\n\n\n\n\n\nWhen we just do the simple regression of H1 on S1 we get a big effect with a highly statistically significant p-value, despite the fact that we know there’s no causal relationship there! But then when we include the confounding measurement effect in the model this effect vanishes in smoke.\nThat’s all well and good. But in reality we won’t have measurements of the latent variables, so we won’t be able to directly control for them. Thankfully, we have Factor Analysis. We can control for the measurement effects by estimating the residual correlation between each pair of variables that share a measurement effect. Since, under the DAG, the measurement effects are the only source of correlation between these variables, this should close the backdoor path, IE we should get unbiased estimates of the factor loadings.\n….@Brown2006 calls this an “error theory”…..\n\n\nCorrelated Uniqueness Model\nTo illustrate, we’ll fit 2 models: The first is a basic CFA model that just loads each measured variable on its corresponding factor. The second specifies that the residual correlation between the measurement-confounded variables should be freely estimated, IE not fixed at 0.\nFirst let’s define our utility function like we did in the previous chapter:\n\n### Define a custom function\nfit_measures <- function(fit){\n  \n  summary <- summary(fit, fit.measures = TRUE, standardized = TRUE)\n  \n  res <- list(\n    \n    # Chi-Squared\n    chi_squared = tibble(\n      Test             = \"standard chi-squared\",\n      `DF`             = summary$test$standard$df,\n      `Test Statistic` = round(summary$test$standard$stat, 2),\n      `p-value`        = summary$test$standard$pvalue) %>% \n      \n      mutate(across(everything(), as.character)) %>% \n      \n      pivot_longer(everything()),\n    \n    # RMSEA\n    rmsea = summary$fit %>% \n      \n      as_tibble(rownames = \"stat\") %>% \n      \n      filter(str_detect(stat, \"rmsea\")),\n    \n    # CFI and TLI\n    cfi_tli = summary$fit %>% \n      \n      as_tibble(rownames = \"stat\") %>% \n      \n      filter(str_detect(stat, \"cfi|tli\")) \n    \n  )\n  \n  res\n  \n}\n\n\nbasic.definition <- \n  'happy =~ H1 + H2 + H3\n   sad =~ S1 + S2 + S3\n   '\n\ncorrelated_uniqueness.definition <- \n  'happy =~ H1 + H2 + H3\n   sad =~ S1 + S2 + S3\n   \n   H1 ~~ S1\n   H2 ~~ S2\n   H3 ~~ S3\n   '\nbasic.fit <- cfa(\n  data = dat_fake %>% select(matches(\"^(H|S)\")),\n  model = basic.definition\n)\n\ncorrelated_uniqueness.fit <- cfa(\n  data = dat_fake %>% select(matches(\"^(H|S)\")),\n  model = correlated_uniqueness.definition\n)\n\nsummary.basic.fit <- summary(basic.fit, standardized = TRUE)\nsummary.correlated_uniqueness.fit <- summary(correlated_uniqueness.fit, standardized = TRUE)\n\nsummary.basic.fit\n\nlavaan 0.6.16 ended normally after 23 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        13\n\n  Number of observations                           305\n\nModel Test User Model:\n                                                      \n  Test statistic                               802.905\n  Degrees of freedom                                 8\n  P-value (Chi-square)                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  happy =~                                                              \n    H1                1.000                               0.816    0.723\n    H2                0.741    0.090    8.236    0.000    0.605    0.615\n    H3                1.044    0.123    8.496    0.000    0.852    0.741\n  sad =~                                                                \n    S1                1.000                               0.906    0.778\n    S2                0.818    0.079   10.302    0.000    0.742    0.704\n    S3                0.980    0.093   10.522    0.000    0.888    0.752\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  happy ~~                                                              \n    sad               0.236    0.060    3.934    0.000    0.319    0.319\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .H1                0.609    0.084    7.214    0.000    0.609    0.478\n   .H2                0.601    0.062    9.662    0.000    0.601    0.621\n   .H3                0.597    0.089    6.721    0.000    0.597    0.451\n   .S1                0.537    0.077    6.989    0.000    0.537    0.395\n   .S2                0.560    0.062    8.964    0.000    0.560    0.505\n   .S3                0.604    0.078    7.726    0.000    0.604    0.434\n    happy             0.667    0.114    5.860    0.000    1.000    1.000\n    sad               0.822    0.119    6.886    0.000    1.000    1.000\n\nsummary.correlated_uniqueness.fit\n\nlavaan 0.6.16 ended normally after 47 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        16\n\n  Number of observations                           305\n\nModel Test User Model:\n                                                      \n  Test statistic                                13.448\n  Degrees of freedom                                 5\n  P-value (Chi-square)                           0.020\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  happy =~                                                              \n    H1                1.000                               0.738    0.669\n    H2                0.915    0.055   16.787    0.000    0.676    0.657\n    H3                1.157    0.066   17.625    0.000    0.854    0.755\n  sad =~                                                                \n    S1                1.000                               0.866    0.744\n    S2                0.863    0.042   20.433    0.000    0.747    0.724\n    S3                1.057    0.048   21.972    0.000    0.915    0.761\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n .H1 ~~                                                                 \n   .S1                0.551    0.061    9.007    0.000    0.551    0.866\n .H2 ~~                                                                 \n   .S2                0.458    0.051    8.973    0.000    0.458    0.831\n .H3 ~~                                                                 \n   .S3                0.501    0.062    8.133    0.000    0.501    0.868\n  happy ~~                                                              \n    sad               0.032    0.050    0.638    0.524    0.050    0.050\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .H1                0.672    0.070    9.641    0.000    0.672    0.552\n   .H2                0.600    0.061    9.813    0.000    0.600    0.568\n   .H3                0.550    0.071    7.769    0.000    0.550    0.430\n   .S1                0.604    0.066    9.121    0.000    0.604    0.446\n   .S2                0.507    0.053    9.538    0.000    0.507    0.476\n   .S3                0.607    0.069    8.789    0.000    0.607    0.420\n    happy             0.545    0.073    7.450    0.000    1.000    1.000\n    sad               0.749    0.086    8.762    0.000    1.000    1.000\n\nfit_measures(basic.fit) %>% \n  \n  knitr::kable(caption = \"Basic Model\")\n\n\n\n\nBasic Model\n\n\n\n\n\n\n\nname\nvalue\n\n\n\n\nTest\nstandard chi-squared\n\n\nDF\n8\n\n\nTest Statistic\n802.9\n\n\np-value\n0\n\n\n\n\n\n\n\n\nstat\nvalue\n\n\n\n\nrmsea\n0.5707720\n\n\nrmsea.ci.lower\n0.5377551\n\n\nrmsea.ci.upper\n0.6044999\n\n\nrmsea.ci.level\n0.9000000\n\n\nrmsea.pvalue\n0.0000000\n\n\nrmsea.close.h0\n0.0500000\n\n\nrmsea.notclose.pvalue\n1.0000000\n\n\nrmsea.notclose.h0\n0.0800000\n\n\n\n\n\n\n\n\nstat\nvalue\n\n\n\n\ncfi\n0.3742618\n\n\ntli\n-0.1732591\n\n\n\n\n\n\n\n\n\nfit_measures(correlated_uniqueness.fit) %>% \n  \n  knitr::kable(caption = \"Correlated Uniqueness Model\")\n\n\n\n\nCorrelated Uniqueness Model\n\n\n\n\n\n\n\nname\nvalue\n\n\n\n\nTest\nstandard chi-squared\n\n\nDF\n5\n\n\nTest Statistic\n13.45\n\n\np-value\n0.0195200124719597\n\n\n\n\n\n\n\n\nstat\nvalue\n\n\n\n\nrmsea\n0.07443086\n\n\nrmsea.ci.lower\n0.02734066\n\n\nrmsea.ci.upper\n0.12377511\n\n\nrmsea.ci.level\n0.90000000\n\n\nrmsea.pvalue\n0.16609111\n\n\nrmsea.close.h0\n0.05000000\n\n\nrmsea.notclose.pvalue\n0.47823144\n\n\nrmsea.notclose.h0\n0.08000000\n\n\n\n\n\n\n\n\nstat\nvalue\n\n\n\n\ncfi\n0.9933495\n\n\ntli\n0.9800485\n\n\n\n\n\n\n\n\n\n\nHere we see that under the basic model we have some moderate correlation between the happy and sad factors, which is a bit of a murky result: it doesn’t tell us one way or the other whether happiness and sadness are separate constructs I can feel together or two extremes of the same feeling. But under the correlated uniqueness model this correlation evaporates because we’ve controlled for the measurement effects, closing the backdoor path between happy and sad. This model also greatly improves goodness-of-fit, which makes sense because it better reflects the true data-generating process we coded up.\nWe also could have controlled for the measurement effects by including measurement factors, IE by adopting a ‘Correlated Methods Model’. I tried this but I actually I couldn’t get this model to converge, regardless of whether its method factors were correlated or uncorrelated (an ‘Uncorrelated Methods Model’. Brown (2006) actually mentions this as a common issue, and favours the Correlated Uniqueness Model for that reason. In his words:\n\n“an overriding drawback of the correlated methods model is that it is usually empirically underidentified. Consequently, a correlated methods solution will typically fail to converge. If it does converge, the solution will usually be associated with Heywood cases [negative variance estimates] and large standard errors”\n\nNow let’s consider the other case in which measurement effects might be hurting us: the case in which within-factor measurements are confounded by measurement effects. Here’s the DAG:\n\ndag_coords <- list(\n  x = c(\n    F1 = 1, \n    F2 = 1,\n    H1 = 2,\n    H2 = 2,\n    H3 = 2,\n    S1 = 2,\n    S2 = 2,\n    S3 = 2,\n    M1 = 3,\n    M2 = 3),\n  y = c(\n    F1 = 2.5,\n    F2 = 1.5,\n    H1 = 2.8,\n    H2 = 2.5,\n    H3 = 2.2,\n    S1 = 1.8,\n    S2 = 1.5,\n    S3 = 1.2,\n    M1 = 2.5,\n    M2 = 1.5\n  )\n)\n\n# Set DAG relationships and aesthetics\nmeasurement_confounding_dag <- ggdag::dagify(\n  H3 ~ F1,\n  S2 ~ F2,\n  H1 ~ M1,\n  H2 ~ M1,\n  H3 ~ M1,\n  S1 ~ M2,\n  S2 ~ M2,\n  S3 ~ M2,\n  coords = dag_coords\n) %>% \n  \n  tidy_dagitty() %>% \n  \n  mutate(\n    \n    node_colour = case_when(\n      grepl(\"^F|M\", name) ~ \"latent\",\n      grepl(\"^H|S\", name) ~ \"observed\"\n    ),\n    \n    edge_colour = case_when(\n      grepl(\"M1\", name)  ~ \"cornflower blue\",\n      grepl(\"M2\", name) ~ \"#ed7864\",\n      grepl(\"^XX\", name) & grepl(\"3$\", to) ~ \"#ed7864\",\n      grepl(\"^F\", name)                   ~ \"black\"\n    )\n  )\n\n# Plot the DAG\nmeasurement_confounding_dag %>%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(colour = node_colour)) +\n  scale_colour_manual(values = c(\"dark blue\", \"#edbc64\")) + \n  geom_dag_edges(aes(edge_colour = edge_colour)) +\n  geom_dag_text() +\n  theme_void()\n\n\n\n\nThis is the ‘true’ data-generating process we’ll be simulating data from in a moment. Notice that even though the researcher (who can’t see this DAG) might think that the unobseved factor causally influences all 3 measured variables, the reality is that each factor only influences one of the measured variables. However, the purported within-factor variables are confounded by measurement method.\nLet’s simulate the data and analyze:\n\n# Set seed for replicable results\nset.seed(233)\n\n# Set sample size\nN <- 30000\n\n# Create the dataset\ndat_fake <- tibble(\n  \n  # Create some uncorrelated factors\n  F1 = rnorm(N, 0, 1),\n  F2 = rnorm(N, 0, 1),\n  \n  # Create some measurement effects\n  M1 = rnorm(N, 0, 1),\n  M2 = rnorm(N, 0, 1),\n  \n  # The DAG says only H3 and S2 are influenced by the factors, but all variables are influenced by a measurement effect.\n  H1 = 0.7*M1 + rnorm(N, 0, .3),\n  H2 = 0.8*M1 + rnorm(N, 0, .3),\n  H3 = 0.9*F1 + 0.8*M1 + rnorm(N, 0, .3),\n  S1 = 0.7*M2 + rnorm(N, 0, .3),\n  S2 = 0.7*F2 + 0.8*M2 + rnorm(N, 0, .3),\n  S3 = 0.7*M2 + rnorm(N, 0, .3) \n) \n\nFirst let’s fit a basic naive CFA model that does the standard thing of keeping the covariances between variables fixed at 0. Based on the DAG, we should expect this model to return a strong (publishable) but misleading answer – it will notice the correlation between variables that are considered within-factor under our hypothesis, and say ‘wow so correlated, that’s consistent with them being caused by that factor’. But we know this is wrong: their correlation is simply driven by the shared measurement method:\n\nbasic.definition <- \n  'happy =~ H1 + H2 + H3\n   sad =~ S1 + S2 + S3\n   '\n\nbasic.fit <- cfa(\n  data = dat_fake,\n  model = basic.definition\n)\n\nsummary(basic.fit, standardized = TRUE)\n\nlavaan 0.6.16 ended normally after 25 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        13\n\n  Number of observations                         30000\n\nModel Test User Model:\n                                                      \n  Test statistic                                 7.086\n  Degrees of freedom                                 8\n  P-value (Chi-square)                           0.527\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  happy =~                                                              \n    H1                1.000                               0.705    0.920\n    H2                1.141    0.006  196.951    0.000    0.804    0.936\n    H3                1.142    0.009  129.172    0.000    0.805    0.646\n  sad =~                                                                \n    S1                1.000                               0.696    0.917\n    S2                1.142    0.008  151.721    0.000    0.795    0.722\n    S3                1.004    0.005  206.981    0.000    0.699    0.922\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  happy ~~                                                              \n    sad              -0.002    0.003   -0.765    0.444   -0.005   -0.005\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .H1                0.090    0.002   42.749    0.000    0.090    0.154\n   .H2                0.091    0.003   34.103    0.000    0.091    0.124\n   .H3                0.903    0.008  115.595    0.000    0.903    0.582\n   .S1                0.091    0.002   49.153    0.000    0.091    0.158\n   .S2                0.581    0.005  110.971    0.000    0.581    0.479\n   .S3                0.086    0.002   46.614    0.000    0.086    0.150\n    happy             0.497    0.005   96.783    0.000    1.000    1.000\n    sad               0.484    0.005   98.040    0.000    1.000    1.000\n\n\nAnd there you have it – just as foretold, we have super strong factor loadings for all the variables, even those that are not actually causally influenced by the factor! So it may look like I have strong convergent validity, but hopefully if we try to publish this, a reviewer will raise the possibility that these correlations are confounded by measurement effects.\nNow I’m going to try closing the backdoor paths between the non-factor-caused variables by allowing the model to learn the covariances, thereby hopefully controlling for unobserved sources of confounding (like the measurement effect). If the loadings stay strong, then my claims to convergent validity are more reasonable.\n\ncorrelated_uniqueness.definition <- \n  'happy =~ H1 + H2 + H3\n   sad   =~ S1 + S2 + S3\n   \n   H1 ~~ H2\n   H1 ~~ H3\n   H2 ~~ H3\n   S1 ~~ S2\n   S1 ~~ S3\n   S2 ~~ S3\n   '\n\ncorrelated_uniqueness.fit <- cfa(\n  data = dat_fake,\n  model = correlated_uniqueness.definition\n)\n\nWarning in lav_model_vcov(lavmodel = lavmodel, lavsamplestats = lavsamplestats, : lavaan WARNING:\n    Could not compute standard errors! The information matrix could\n    not be inverted. This may be a symptom that the model is not\n    identified.\n\n\nWarning in lav_object_post_check(object): lavaan WARNING: some estimated ov\nvariances are negative\n\nsummary(correlated_uniqueness.fit, standardized = TRUE)\n\nlavaan 0.6.16 ended normally after 593 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        19\n\n  Number of observations                         30000\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.453\n  Degrees of freedom                                 2\n  P-value (Chi-square)                           0.797\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  happy =~                                                              \n    H1                1.000                               0.922    1.203\n    H2                0.712       NA                      0.656    0.764\n    H3               -2.144       NA                     -1.976   -1.587\n  sad =~                                                                \n    S1                1.000                               0.571    0.752\n    S2                0.393       NA                      0.224    0.204\n    S3                2.194       NA                      1.252    1.653\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n .H1 ~~                                                                 \n   .H2               -0.038       NA                     -0.038   -0.135\n   .H3                2.388       NA                      2.388    3.038\n .H2 ~~                                                                 \n   .H3                1.944       NA                      1.944    2.288\n .S1 ~~                                                                 \n   .S2                0.425       NA                      0.425    0.789\n   .S3               -0.228       NA                     -0.228   -0.459\n .S2 ~~                                                                 \n   .S3                0.274       NA                      0.274    0.255\n  happy ~~                                                              \n    sad              -0.001       NA                     -0.002   -0.002\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .H1               -0.263       NA                     -0.263   -0.447\n   .H2                0.307       NA                      0.307    0.416\n   .H3               -2.353       NA                     -2.353   -1.517\n   .S1                0.250       NA                      0.250    0.434\n   .S2                1.162       NA                      1.162    0.958\n   .S3               -0.994       NA                     -0.994   -1.731\n    happy             0.850       NA                      1.000    1.000\n    sad               0.326       NA                      1.000    1.000\n\n\nUh-oh…the model failed to converge :(. Apparently this is a common thing with CFA models that try to learn the correlation between within-factor variables – the parameters are non-identified because you’re asking the model to learn their correlation simultaneously in two different parameters: the factor loading and the covariance parameter. This Stack Exchange thread explains it nicely.\n\n\n\n\nal, Green et. n.d. “Measurement Error Masks Bipolarity in Affect Ratings.” Journal of Personality and Social Psychology 64(6).\n\n\nBrown, Timothy A. 2006. Confirmatory Factor Analysis for Applied Research."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Latent Variable Modelling Workflow Reference",
    "section": "",
    "text": "This document does two things:\n\nGive a conceptual overview of latent variable modelling, especially confirmatory factor analysis (CFA).\nProvide workflows I’ve cobbled together from a few different textbooks, including examples with data from those textbooks or from open datasets I found online.\n\n\n\n\nThe first book on latent variable modelling I read was Gorsuch (1983). This was a nice conceptual introduction, but the applied examples weren’t great. I’ve since found a few sources with data and R code to work with. I also cite these sources throughout as I cobble together the workflows.\n\nLatent Variable Modelling with R, by Finch (2015). They helpfully provide all of the datasets here.\nPrinciples and Practice of Structural Equation Modeling, by Kline (2011). The publisher provides data and code here.\nConfirmatory Factor Analysis for Applied Research, by Brown (2006). No R code available, but there’s some data at the university website\nQuantitative Analysis Using Structural Equation Modelling, a free online course provided by the Wetland and Aquatic Research Center of the United States Geological Survey.\nThe lavaan documentation has some nice worked examples too.\n\nI’ll mostly be using lavaan and tidyverse, but maybe also some brms at some point.\n\n\n\n\nBrown, Timothy A. 2006. Confirmatory Factor Analysis for Applied Research.\n\n\nFinch, French, W. Holmes. 2015. Latent Variable Modeling with r.\n\n\nGorsuch, Richard L. 1983. Factor Analysis, 2nd Edition.\n\n\nKline, Rex B. 2011. Principles and Practice of Structural Equation Modeling."
  },
  {
    "objectID": "bayesian-cfa.html#first-well-need-to-add-a-new-parameter-to-our-stan-model-to-account-for-the-possibility-of-shared-method-confounding.-this-only-involves-a-few-changes-namely",
    "href": "bayesian-cfa.html#first-well-need-to-add-a-new-parameter-to-our-stan-model-to-account-for-the-possibility-of-shared-method-confounding.-this-only-involves-a-few-changes-namely",
    "title": "6  Bayesian CFA",
    "section": "6.4 First we’ll need to add a new parameter to our Stan model to account for the possibility of shared method confounding. This only involves a few changes, namely:",
    "text": "6.4 First we’ll need to add a new parameter to our Stan model to account for the possibility of shared method confounding. This only involves a few changes, namely:\ndata {\n  int N; // sample size\n  int P; // number of variables\n  int D; // number of factors\n  array[N] vector[P] X; // data matrix of order [N,P]\n  int n_lam; // how many factor loadings are estimated\n}\nparameters {\n  matrix[N, D] FS_UNC; // factor scores, matrix of order [N,D]\n  cholesky_factor_corr[D] L_corr_d; // cholesky correlation between factors\n  cholesky_factor_corr[2] L_corr_m1_m4; // cholesky correlation between the m1 and m4\n  vector<lower=0>[P] sd_p; // residual sd for each variable\n  vector<lower=-30, upper=30>[n_lam] lam_UNC; // A bunch of lightly-constrained factor loadings\n}\ntransformed parameters {\n  // a vector to hold the factor means, which will be a bunch of 0s\n  vector[D] M;\n  M = rep_vector(0, D);\n  \n  // a vector to hold the factor SDs, which will be a bunch of 1s. \n  vector<lower=0>[D] Sd_d;\n  Sd_d = rep_vector(1, D);\n  \n  // A vector to hold the linear predictors for the manifest variables, which are each a deterministic function of loading*factor_score\n  array[N] vector[P] mu_UNC;\n  for (i in 1 : N) {\n    mu_UNC[i, 1] = lam_UNC[1] * FS_UNC[i, 1];\n    mu_UNC[i, 2] = lam_UNC[2] * FS_UNC[i, 1];\n    mu_UNC[i, 3] = lam_UNC[3] * FS_UNC[i, 1];\n    mu_UNC[i, 4] = lam_UNC[4] * FS_UNC[i, 2];\n    mu_UNC[i, 5] = lam_UNC[5] * FS_UNC[i, 2];\n    mu_UNC[i, 6] = lam_UNC[6] * FS_UNC[i, 2];\n  }\n  \n  // the var-covar matrix for the factors, a deterministic function of the deterministic SDs and the estimated rhos defined in the parameters block\n  cholesky_factor_cov[D] L_Sigma;\n  L_Sigma = diag_pre_multiply(Sd_d, L_corr_d);\n\n  // Same, but for the var-covar matrix of m1 and m4\n  cholesky_factor_cov[D] L_Sigma_m1_m4;\n  L_Sigma_m1_m4 = diag_pre_multiply(sd_p[{1, 4}], L_corr_m1_m4);\n}\nmodel {\n  // Declare some priors\n  L_corr_d ~ lkj_corr_cholesky(1); // Prior on factor corrs\n  L_corr_m1_m4 ~ lkj_corr_cholesky(1); // Prior on corr between m1 and m4\n  lam_UNC ~ normal(0, 10); // Prior on loadings\n  sd_p ~ cauchy(0, 2.5); // Prior on residual SDs of manifest variables\n\n  // Set up the joint log likelihood function\n  for (i in 1 : N) {\n    // The uncorrelated measured variables\n    for (j in {2,3,5,6}) {\n      // Manifest variables\n      X[i, j] ~ normal(mu_UNC[i, j], sd_p[j]);\n    }\n\n    // m1 and m4, which get special treatment for being correlated\n    to_vector({X[i, 1], X[i, 4]}) ~ multi_normal_cholesky([mu_UNC[i, 1], mu_UNC[i, 4]]', L_Sigma_m1_m4);\n\n    // Latent factors\n    FS_UNC[i] ~ multi_normal_cholesky(M, L_Sigma);\n  }\n}\ngenerated quantities {\n  corr_matrix[D] Rho_UNC; /// correlation matrix for factors\n  corr_matrix[D] Rho_factors; /// correlation matrix for factors\n  corr_matrix[D] Rho_m1_m4; /// correlation matrix for m1 and m4\n  matrix[P, D] lam; // factor loadings\n  matrix[N, D] FS; // factor scores, matrix of order [N,D]\n  \n  // Do some fancy things to sign-correct the parameter estimates.\n  // The idea seems to be that when we estimate the loadings with unconstrained signs\n  // It can lead to identification issues. So we take these steps to correct the signs.\n  // See this Stan forum comment for more: https://discourse.mc-stan.org/t/non-convergence-of-latent-variable-model/12450/10\n  Rho_UNC = multiply_lower_tri_self_transpose(L_corr_d);\n  Rho_factors = Rho_UNC;\n  Rho_m1_m4 = multiply_lower_tri_self_transpose(L_corr_m1_m4);\n  FS = FS_UNC;\n  lam = rep_matrix(0, P, D);\n  lam[1 : 3, 1] = to_vector(lam_UNC[1 : 3]);\n  lam[4 : 6, 2] = to_vector(lam_UNC[4 : 6]);\n  \n  // factor 1\n  if (lam_UNC[1] < 0) {\n    lam[1 : 3, 1] = to_vector(-1 * lam_UNC[1 : 3]);\n    FS[ : , 1] = to_vector(-1 * FS_UNC[ : , 1]);\n    \n    if (lam_UNC[4] > 0) {\n      Rho_factors[1, 2] = -1 * Rho_UNC[1, 2];\n      Rho_factors[2, 1] = -1 * Rho_UNC[2, 1];\n    }\n  }\n  // factor 2\n  if (lam_UNC[4] < 0) {\n    lam[4 : 6, 2] = to_vector(-1 * lam_UNC[4 : 6]);\n    FS[ : , 2] = to_vector(-1 * FS_UNC[ : , 2]);\n    \n    if (lam_UNC[1] > 0) {\n      Rho_factors[2, 1] = -1 * Rho_UNC[2, 1];\n      Rho_factors[1, 2] = -1 * Rho_UNC[1, 2];\n    }\n  }\n}\nNow we can compile and run the model\n\n# Write the raw Stan code from an R string to a Stan file\nstan_file <- cmdstanr::write_stan_file(stan_model_code)\n\n# Autoformat the model syntax to preempt any warnings at compile time\nmodel <- cmdstanr::cmdstan_model(stan_file, compile = FALSE)\nmodel$format(canonicalize = TRUE)\n\n# Compile the model from the Stan file\nmodel <- cmdstanr::cmdstan_model(stan_file)\n\n# Declare the new lavaan model where m1 and m4 covary\nmtmm.model <- ' \n  f1 =~ .8*m1 + .6*m2 + .9*m3\n  f2 =~ .1*m4 + .2*m5 + -.4*m6\n  f1 ~~ .4*f2\n  m1 ~~ .4*m4\n'\n\n# Simulate data from the lavaan model\nfake_dat <- lavaan::simulateData(model = mtmm.model, sample.nobs = 4000)\n\n# Put the data into a list format Stan can understand\nX <- as.matrix(fake_dat)\nP <- ncol(fake_dat) \nN <- nrow(fake_dat)\nD <- 2\n\ndata_list <- list(N=N,P=P,D=D,X=X, n_lam=6)\n\n# Fit the model\nfit <- model$sample(\n  data = data_list,\n  seed = 199,\n  chains = 4,\n  parallel_chains = 4,\n  iter_warmup = 1000,\n  iter_sampling = 6000,\n # adapt_delta=0.99, \n # max_treedepth = 12,\n  refresh = 1 # print update at each iter\n)\n\n# Save the resulting model fit\nsaveRDS(fit, \"fits/b07.03.rds\")\n\nOnce again, the resulting object is enormous. So let’s pare it down to only contain the draws of substantive interest.\n\n# Load the cmdstanr model object\nfit <- readRDS(\"fits/b07.03.rds\")\n\n# Load the MCMC draws from the variables we care about\ndraws <- fit$draws(\n  variables = c(\n  \"lam[1,1]\",\n  \"lam[2,1]\",\n  \"lam[3,1]\",\n  \"lam[4,2]\",\n  \"lam[5,2]\",\n  \"lam[6,2]\",\n  \"Rho_factors[2,1]\", \n  \"Rho_m1_m4[2,1]\"\n  )\n)\n\n# Save the result\nsaveRDS(draws, \"fits/b07.03.samples.rds\")\n\nHow did that go? We can plot the draws and look at the MCMC diagnostics like we did above.\n\n# Load the draws saved in the previous block\ndraws <- readRDS(\"fits/b07.03.samples.rds\")\n\n# Plot\ndraws |>\n\n  # Get the raw draws for the factor loadings and the beween-factor correlation coefficient\n  gather_draws(\n   `lam[1,1]`,\n   `lam[2,1]`,\n   `lam[3,1]`,\n   `lam[4,2]`,\n   `lam[5,2]`,\n   `lam[6,2]`,\n   `Rho_factors[2,1]`,\n   `Rho_m1_m4[2,1]`,\n  ) |>\n\n  # Do some renaming for clarity\n  mutate(.variable = case_when(\n    .variable == \"lam[1,1]\" ~ \"Loading for m1\",\n    .variable == \"lam[2,1]\" ~ \"Loading for m2\",\n    .variable == \"lam[3,1]\" ~ \"Loading for m3\",\n    .variable == \"lam[4,2]\" ~ \"Loading for m4\",\n    .variable == \"lam[5,2]\" ~ \"Loading for m5\",\n    .variable == \"lam[6,2]\" ~ \"Loading for m6\",\n    .variable == \"Rho_factors[2,1]\" ~ \"Covariance for f1 ~ f2\",\n    .variable == \"Rho_m1_m4[2,1]\" ~ \"Covariance for m1 ~ m4\"\n  )) |>\n\n  # Add the true factor loadings for plotting\n  mutate(true_loading = case_when(\n    .variable == \"Loading for m1\" ~ .8,\n    .variable == \"Loading for m2\" ~ .6,\n    .variable == \"Loading for m3\" ~ .9,\n    .variable == \"Loading for m4\" ~ .1,\n    .variable == \"Loading for m5\" ~ .2,\n    .variable == \"Loading for m6\" ~ -.4,\n    .variable == \"Covariance for f1 ~ f2\" ~ .4,\n    .variable == \"Covariance for m1 ~ m4\" ~ .4\n  )) |>\n\n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(fill = \"mediumorchid\") +\n  geom_vline(aes(xintercept = true_loading), linetype = 2) + \n  scale_x_continuous(expand = c(0, 0.015)) +\n  scale_y_continuous(expand = c(0, 0.015)) +\n  guides(fill = \"none\") +\n  labs(x = \"lab\",\n     y = NULL)  +\n  facet_wrap(~.variable) +\n  theme_minimal() + \n  theme(panel.grid.major = element_blank())"
  },
  {
    "objectID": "bayesian-cfa.html#multilevel-survival-analysis",
    "href": "bayesian-cfa.html#multilevel-survival-analysis",
    "title": "6  Bayesian CFA",
    "section": "6.5 Multilevel Survival Analysis",
    "text": "6.5 Multilevel Survival Analysis\nLet’s say like 30 clusters. How to simulate?\n\n\n\n\nBrown, Timothy A. 2006. Confirmatory Factor Analysis for Applied Research."
  }
]