{
  "hash": "194e8ab0fc0eeb2901eced0210b5737f",
  "result": {
    "markdown": "# Modification Indexes\n\nIn this chapter we'll work through another example of the Traditional CFA Workflow to get more practice. We'll also introduce the concept of 'Modification Indexes', which researchers often use to improve their model goodness of fit in a way that seems a bit suss to me. Probably a good thing to know about. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(ggdag)\n```\n:::\n\n\n## Example 2: Biodiversity {.unnumbered}\n\nHere's [a fun example from the Wetland and Aquatic Research Center of the U.S. Geological Survey](https://d9-wret.s3.us-west-2.amazonaws.com/assets/palladium/production/s3fs-public/atoms/files/SEM_09_2-Ex1_CFA_exercise.pdf): given counts of different types of animals, can we fit a convincing CFA model for 'diversity'? In other words: is the correlation structure of all my counts of various types of animals consistent with the possibility that those counts are confounded by a single unobserved thing called 'diversity'?\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_raw <- read.csv('data/grace/SEM_09_2-Ex1_CFA_exercise_data.csv')\n\ndat_clean <- dat_raw %>%  \n  \n  janitor::clean_names()\n```\n:::\n\n\n### Data Exploration {.unnumbered}\n\nJust for fun let's see if the relative proportions of the different animals varies between countries:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Proportions\ndat_clean %>% \n  \n  pivot_longer(\n    cols      = !matches(\"^c\"),\n    names_to  = \"animal\",\n    values_to = \"count\"\n  ) %>% \n  \n  group_by(country) %>% \n  mutate(\n    total = sum(count), \n    prop  = round(count / total, 2)\n  ) %>% \n  ungroup() %>% \n\n  ggplot() + \n  geom_bar(aes(x = animal, y = prop), stat = \"identity\") + \n  theme_bw() + \n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n  facet_wrap(~country)\n```\n\n::: {.cell-output-display}\n![](modification-indexes_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nThe proportions are pretty stable. Finland seems like the weirdest one, and it isn't even that weird.\n\n### Model Fitting {.unnumbered}\n\nThe hypothesis we want to test here is simply that all of these counts are confounded by a single unmeasured 'biodiversity' variable. This is straightforward to fit:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nh1.definition <- \n'diversity =~ mammals + birds + amphibians + reptiles + beetles + butterflies'\n\nh1.fit <- cfa(\n  data  = dat_clean %>% select(-country) %>% scale(),\n  model = h1.definition\n)\n\nh1.summary <- summary(h1.fit)\n\nh1.summary\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nlavaan 0.6.16 ended normally after 23 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        12\n\n  Number of observations                            20\n\nModel Test User Model:\n                                                      \n  Test statistic                                20.817\n  Degrees of freedom                                 9\n  P-value (Chi-square)                           0.013\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  diversity =~                                        \n    mammals           1.000                           \n    birds             0.825    0.277    2.978    0.003\n    amphibians        1.115    0.260    4.281    0.000\n    reptiles          0.780    0.279    2.793    0.005\n    beetles           1.135    0.259    4.380    0.000\n    butterflies       1.261    0.254    4.960    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .mammals           0.387    0.131    2.958    0.003\n   .birds             0.566    0.184    3.073    0.002\n   .amphibians        0.250    0.092    2.727    0.006\n   .reptiles          0.608    0.197    3.089    0.002\n   .beetles           0.224    0.085    2.645    0.008\n   .butterflies       0.054    0.054    1.010    0.313\n    diversity         0.563    0.278    2.025    0.043\n```\n:::\n:::\n\n\nLet's have a look at the same 4 goodness-of-fit measures we used in the previous example. We can bring them all together with a nice utility function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Define a custom function\nfit_measures <- function(fit){\n  \n  summary <- summary(fit, fit.measures = TRUE, standardized = TRUE)\n  \n  res <- list(\n    \n    # Chi-Squared\n    chi_squared = tibble(\n      Test             = \"standard chi-squared\",\n      `DF`             = summary$test$standard$df,\n      `Test Statistic` = round(summary$test$standard$stat, 2),\n      `p-value`        = summary$test$standard$pvalue) %>% \n      \n      mutate(across(everything(), as.character)) %>% \n      \n      pivot_longer(everything()),\n    \n    # RMSEA\n    rmsea = summary$fit %>% \n      \n      as_tibble(rownames = \"stat\") %>% \n      \n      filter(str_detect(stat, \"rmsea\")),\n    \n    # CFI and TLI\n    cfi_tli = summary$fit %>% \n      \n      as_tibble(rownames = \"stat\") %>% \n      \n      filter(str_detect(stat, \"cfi|tli\")) \n    \n  )\n  \n  res\n  \n}\n\n### Call the function, then send its outputs to clean tables\nfit_measures(h1.fit) %>% \n  \n  map(knitr::kable)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$chi_squared\n\n\n|name           |value                |\n|:--------------|:--------------------|\n|Test           |standard chi-squared |\n|DF             |9                    |\n|Test Statistic |20.82                |\n|p-value        |0.0134888288206897   |\n\n$rmsea\n\n\n|stat                  |      value|\n|:---------------------|----------:|\n|rmsea                 | 0.25622117|\n|rmsea.ci.lower        | 0.11034617|\n|rmsea.ci.upper        | 0.40224444|\n|rmsea.ci.level        | 0.90000000|\n|rmsea.pvalue          | 0.01890337|\n|rmsea.close.h0        | 0.05000000|\n|rmsea.notclose.pvalue | 0.97055608|\n|rmsea.notclose.h0     | 0.08000000|\n\n$cfi_tli\n\n\n|stat |     value|\n|:----|---------:|\n|cfi  | 0.8695573|\n|tli  | 0.7825955|\n```\n:::\n:::\n\n\nThe model isn't fitting very well -- Chi-Squared is highly statistically significant (we fail to reject the null hypothesis that there is residual variance left to explain), RMSEA is well above its conventional threshold, and CFI and TLI are both well below their conventional thresholds.\n\n### Modification Indexes {-}\n\nHere @Grace introduces a new method for tweaking our CFA model to improve goodness of fit. The idea is that we can use fancy math to ask \"if I took a certain fixed parameter from my model definition and allowed it to be freely estimated, how much would my model's chi-squared goodness of fit change?\" People like to take this estimated change in goodness-of-fit and call it a **modification index.** As @Brown2006 puts it:\n\n> \"The modification index reflects an approximation of how much the overall model $Ï‡^2$ would decrease if the fixed or constrained parameter was freely estimated.\"\n\nApparently conventional cutoff for a 'good' modification index is 3.84. So to get some ideas on how we might improve our goodness-of-fit, let's print out the modification indexes for each of the fixed parameters in the model and see which of them pass that threshold:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get the estimated change in chi-squared for each fixed parameter\nmodindices(h1.fit) %>% \n  \n  # Arrange them in order of modification index\n  arrange(desc(mi)) %>% \n  \n  select(lhs, op, rhs, mi) %>% \n  \n  knitr::kable(digits = 2)\n```\n\n::: {.cell-output-display}\n|lhs        |op |rhs         |   mi|\n|:----------|:--|:-----------|----:|\n|birds      |~~ |beetles     | 4.44|\n|birds      |~~ |amphibians  | 3.99|\n|mammals    |~~ |butterflies | 2.84|\n|beetles    |~~ |butterflies | 2.78|\n|mammals    |~~ |amphibians  | 2.31|\n|birds      |~~ |reptiles    | 2.05|\n|amphibians |~~ |butterflies | 1.72|\n|birds      |~~ |butterflies | 1.55|\n|mammals    |~~ |reptiles    | 1.29|\n|mammals    |~~ |birds       | 1.20|\n|amphibians |~~ |beetles     | 0.58|\n|mammals    |~~ |beetles     | 0.38|\n|reptiles   |~~ |butterflies | 0.22|\n|reptiles   |~~ |beetles     | 0.15|\n|amphibians |~~ |reptiles    | 0.14|\n:::\n:::\n\n\nBased on the operation symbol \"\\~\\~\", it seems like all of the modification indexes correspond to residual correlations between observed variables. This teaches me something about CFA models! I guess in the typical CFA model we fix the residual correlations to 0? This helps me understand why the [Bayesian CFA model as implemented in **brms**](https://discourse.mc-stan.org/t/confirmatory-factor-analysis-using-brms/23139) specifies `rescor = FALSE` . I was confused about this!\n\nActually, I just realized @gorsuch1983 already explained this to me! Think back to where he showed us the definition of the 'Common Factor Model':\n\n$R_{vv} = PR_{ff}P' + U_{vv}$\n\nAnd remember how Gorsuch specified that $U_{vv}$ is assumed to be a diagonal matrix, IE the residual correlations is assumed to be uncorrelated for each variable. This is the whole thing about the 'unique factors', IE the error terms, of the linear models of each measured variable are gonna be uncorrelated. [This recorded seminar and notes from UCLA](https://stats.oarc.ucla.edu/r/seminars/rcfa/) give a nice clear walkthrough of the notation in a slightly different form from @gorsuch1983.\n\nFrom the DAGs perspective of CFA, assuming uncorrelated residuals sort of makes sense to me: if I want to convince you that my measured variables are all confounded by the same single unmeasured variable, then I think fixing the residual errors at 0 is a way of committing my model to the idea that there aren't *other* unmeasured variables confounding certain of my measured guys. It is a strong assumption that, if it holds up, provides better evidence that my variables really truly are just confounded by a single unmeasured thing.\n\nSo I guess I could write out this standard CFA model in a more McElreath fashion like so:\n\n\\[\n\\begin{align*}\n\\begin{bmatrix} \\text{mammals}_i \\\\ \\text{birds}_i \\\\ \\text{amphibians}_i \\\\ \\text{reptiles}_i \\\\ \\text{beetles}_i \\\\ \\end{bmatrix} & \\sim\n\\operatorname{MVNormal} \\left( \\begin{bmatrix} \\mu_{mammals} \\\\ \\mu_{birds} \\\\ \\mu_{amphibians} \\\\ \\mu_{reptiles} \\\\ \\mu_{beetles} \\end{bmatrix}, \\mathbf{\\Sigma} \\right)\\\\\n\\mu_{mammals} & = \\lambda_{mammals} F_i \\\\\n\\mu_{birds} & = \\lambda_{birds} F_i \\\\\n\\mu_{amphibians} & = \\lambda_{amphibians} F_i \\\\\n\\mu_{reptiles} & = \\lambda_{reptiles} F_i \\\\\n\\mu_{beetles} & = \\lambda_{beetles} F_i \\\\\n\\Sigma & = \\begin{pmatrix} \n\\sigma_{mammals}&0 &0 &0 &0 \\\\ \n0 & \\sigma_{birds} &0 &0 &0 \\\\ \n0 & 0 & \\sigma_{amphibians} &0 &0 \\\\ \n0 & 0 & 0 & \\sigma_{reptiles} &0 \\\\ \n0 & 0 & 0 & 0 & \\sigma_{beetles} \n\\end{pmatrix} \\\\\n\\end{align*}\n\\]\n\nIn human words: the observed counts of each of the 5 animal types are imagined to be drawn from a shared multivariate normal distribution. The mean of each dimension of that distribution is a linear function of a single shared factor, which we're calling 'biodiversity'. The variance of each dimension of that distribution is unique, and there is no covariance between the dimensions.\n\nBut now think back to our modification indexes: a few of them are saying that if we allow the residual covariances to be freely estimated rather than fixed at 0, then we can improve model fit by a whole lot. Specifically, if we allow the residual covariance between birds and beetles and/or between birds and amphibians to be freely estimated, then model fit as measured by the chi-squared statistic might be significantly improved. Here's what the model is gonna look like now:\n\n\\[\n\\begin{align*}\n\\begin{bmatrix} \\text{mammals}_i \\\\ \\text{birds}_i \\\\ \\text{amphibians}_i \\\\ \\text{reptiles}_i \\\\ \\text{beetles}_i \\\\ \\end{bmatrix} & \\sim\n\\operatorname{MVNormal} \\left( \\begin{bmatrix} \\mu_{mammals} \\\\ \\mu_{birds} \\\\ \\mu_{amphibians} \\\\ \\mu_{reptiles} \\\\ \\mu_{beetles} \\end{bmatrix}, \\mathbf{\\Sigma} \\right)\\\\\n\\mu_{mammals} & = \\lambda_{mammals} F_i \\\\\n\\mu_{birds} & = \\lambda_{birds} F_i \\\\\n\\mu_{amphibians} & = \\lambda_{amphibians} F_i \\\\\n\\mu_{reptiles} & = \\lambda_{reptiles} F_i \\\\\n\\mu_{beetles} & = \\lambda_{beetles} F_i \\\\\n\\Sigma & = \\begin{pmatrix} \n\\sigma_{mammals}&0 &0 &0 &0 \\\\ \n0 & \\sigma_{birds} &\\theta_\\text{b\\&a} &0 &\\theta_\\text{b\\&b} \\\\ \n0 &\\theta_\\text{b\\&a} & \\sigma_{amphibians} &0 &0 \\\\ \n0 & 0 & 0 & \\sigma_{reptiles} &0 \\\\ \n0 &\\theta_\\text{b\\&b} & 0 & 0 & \\sigma_{beetles} \n\\end{pmatrix} \\\\\n\\end{align*}\n\\]\n\nSee how I've filled in the variance-covariance matrix of the likelihood to include a few more free parameters?\n\nActually, Grace proceeds by fitting two more models, one with each of these two candidate covariance parameters as freely fitting. Then he uses `anova()` to do a likelihood-ratio test for them. We can't test all 3 models at once because models 2 and 3 aren't nested with each other.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Letting the covariance between birds and beetles be freely estimated\nh2.definition <- \n'diversity =~ mammals + birds + amphibians + \n              reptiles + beetles + butterflies\n \n birds ~~ beetles'\n\n\nh2.fit <- cfa(\n  data  = dat_clean %>% select(-country) %>% scale(),\n  model = h2.definition\n)\n\n### Letting the covariance between birds and amphibians be freely estimated\nh3.definition <- \n  'diversity =~ mammals + birds + amphibians + \n              reptiles + beetles + butterflies\n \n birds ~~ amphibians'\n\n\nh3.fit <- cfa(\n data  = dat_clean %>% select(-country) %>% scale(),\n  model = h3.definition\n)\n\nanova(h1.fit, h2.fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nChi-Squared Difference Test\n\n       Df    AIC    BIC  Chisq Chisq diff   RMSEA Df diff Pr(>Chisq)  \nh2.fit  8 270.81 283.76 16.013                                        \nh1.fit  9 273.62 285.56 20.817      4.804 0.43612       1    0.02839 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\nanova(h1.fit, h3.fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nChi-Squared Difference Test\n\n       Df    AIC    BIC  Chisq Chisq diff   RMSEA Df diff Pr(>Chisq)   \nh3.fit  8 267.72 280.67 12.924                                         \nh1.fit  9 273.62 285.56 20.817     7.8934 0.58709       1   0.004961 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nLooks like model H3 has the lowest AIC and the more significant improvement in chi-squared fit. So let's continue working with that one in the following sections.\n\n### Validity {.unnumbered}\n\nWe can do the same 5 checks of validity we used in the previous 'Mastery and Performance' example. Let's start with the big summary printout:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary.h3 <- summary(h3.fit, fit.measures = TRUE, standardized = TRUE)\n\nsummary.h3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nlavaan 0.6.16 ended normally after 25 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        13\n\n  Number of observations                            20\n\nModel Test User Model:\n                                                      \n  Test statistic                                12.923\n  Degrees of freedom                                 8\n  P-value (Chi-square)                           0.115\n\nModel Test Baseline Model:\n\n  Test statistic                               105.591\n  Degrees of freedom                                15\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.946\n  Tucker-Lewis Index (TLI)                       0.898\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)               -120.861\n  Loglikelihood unrestricted model (H1)       -114.400\n                                                      \n  Akaike (AIC)                                 267.723\n  Bayesian (BIC)                               280.667\n  Sample-size adjusted Bayesian (SABIC)        240.592\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.175\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.344\n  P-value H_0: RMSEA <= 0.050                    0.138\n  P-value H_0: RMSEA >= 0.080                    0.824\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.055\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  diversity =~                                                          \n    mammals           1.000                               0.706    0.725\n    birds             1.013    0.310    3.266    0.001    0.716    0.734\n    amphibians        1.209    0.306    3.956    0.000    0.854    0.876\n    reptiles          0.899    0.307    2.928    0.003    0.635    0.652\n    beetles           1.264    0.301    4.196    0.000    0.893    0.916\n    butterflies       1.261    0.301    4.187    0.000    0.891    0.914\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n .birds ~~                                                              \n   .amphibians       -0.245    0.094   -2.615    0.009   -0.245   -0.789\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .mammals           0.451    0.148    3.048    0.002    0.451    0.475\n   .birds             0.438    0.152    2.877    0.004    0.438    0.461\n   .amphibians        0.221    0.090    2.451    0.014    0.221    0.232\n   .reptiles          0.546    0.177    3.087    0.002    0.546    0.575\n   .beetles           0.153    0.061    2.506    0.012    0.153    0.161\n   .butterflies       0.156    0.062    2.526    0.012    0.156    0.165\n    diversity         0.499    0.267    1.866    0.062    1.000    1.000\n```\n:::\n:::\n\n\nThe factor loadings are all highly statistically significant, which is the first thing to check to make sure nothing is going horribly wrong.\n\nThe standardized loadings are pretty big as well, but not super great for 'reptiles'. Also there's a lot of variance in the loadings, which is evidence that my simple DAG of confounding may not be perfect -- there are other unmeasured variables influencing some of my animal counts to different degrees. I mean of course there are, but the degree to which this is apparent based on the factor loadings undermines my claims to convergent validity.\n\nNext we can look at the standardized residual variances. Some of them look great, and all but 'reptiles' pass the threshold of 0.5.\n\nI could look at the 'reliability' statistics too, but can't be bothered right now. Onwards to another example!",
    "supporting": [
      "modification-indexes_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}