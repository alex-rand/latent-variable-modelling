{
  "hash": "c07645c2a97bc52cc768afef8e89e3b8",
  "result": {
    "markdown": "# MTMM and Error Structure Modelling\n\nIn this chapter we'll learn some workflows for situations where we're worried our measured variables are confounded by other unmeasured things besides just the unmeasured 'factors' we're interested in, and how we can address that and reassure ourselves that our inferences about the factor structure are ok. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(ggdag)\n```\n:::\n\n\nIn the previous example we saw how we can sometimes improve model fit by freeing-up some of the residual covariance terms, rather than doing the typical thing of fixing them at 0. But this feels a bit icky to me -- just pumping out some modification indexes and using that as a basis for opening up some free parameters feels pretty overfitty, because we don't have a strong theory-driven reason for changing the model in that way.\n\nBut there *are* more kosher-feeling theory-driven reasons for freeing up some of the residual covariance parameters. Let's talk about two of them: the first relates to convergent validity, the second relates to discriminant validity.\n\nHere's the first example: imagine I have a theory where there's a thing called 'exceptional leadership', and it is made up of 3 unobservable features, like 'self-confidence', 'oratorical skill', and 'robust compassionateness'. So I make up a survey where I ask 12 questions total, 4 per imagined factor. Then I fit a CFA model and find that it does a great job recreating the empirical variance-covariance matrix. There's lots of great convergent validity between the questions I imagine to define the 3 factors. So I get published! But there's a first problem: what if my within-factor variables are correlated not because they are cleanly confounded by 'self-confidence' (which is what I'm trying to convince you of), but instead because the within-factor survey questions are just worded in a really similar way, IE they are confounded by a latent factor we might call 'wording similarity'? This possibility undermines my case for clean confounding.\n\nNow the second example: imagine I do the same analysis described above, but I find my discriminant validity actually doesn't look so hot, IE there are some high between-factor correlations. It is possible that this is just being caused by some of the variables used in different factors being confounded by their shared **measurement approach,** which creates a backdoor path between the factors.\n\nAs @Brown2006 puts it:\n\n> \"when each construct is assessed by the same measurement approach (e.g., observer rating), it cannot be determined how much of the observed overlap (i.e., factor correlations) is due to method effects as opposed to \"true\" covariance of the traits.\"\n\nSo we have these two risks:\n\n1.  Maybe some of my within-factor variables are confounded by method effects, which creates the *illusion* of convergent validity. If I go to publish my paper and someone raises this concern, then maybe I won't get published! I'll need to find a way to make my model control for possible method-confounding and *still* show good convergent validity.\n2.  Maybe some of my variables of different factors are confounded by method effects, so I don't end up with great discriminant validity. This would be bad, but fitting a model that controls for method effects can maybe make things better.\n\nFear not: there are two ways of adjusting the model to control for measurement confounding, thereby addressing the above risks.\n\n1.  Add method-specific factors to my model (to control for them in the linear model of each variable). @Brown2006 calls this a **Correlated Methods Model**;\n\n2.  Just freely fit the residual covariances between the observed variables that share a method. @Brown2006 calls this a **Correlated Uniqueness Model.** Because remember, 'Uniqueness' is just a fancy term for variable-specific residual variance.\n\nIt's all still just basic linear modelling, and trying to show that the model's results are consistent with the DAG of clean confounding. By adding a method factors or allowing some of the error residuals to be freely fit, I'm controlling for sources of confounding that a reviewer might bring up as a concern, or that might be pulling down my discriminant validity.\n\nHere's how these approaches can improve convergent or divergent validity:\n\n**Convergent validity:** By adding method-factors to the model or freely fitting the residual covariances between the within-factor questions can help me make the case that \"see, even when I allow for correlated errors due to *other* unobserved confounders (like common wording or common methods), the factors still do a good job recreating the empirical covariance structure, IE the loadings still look good, so my argument for *mostly* clean confounding is still reasonable.\" I think this makes sense?\n\n**Divergent validity:** Maybe I can get better discriminant validity, IE reduce the between-factor correlations, by adding those method effects to the linear models, thereby controlling for them. I can do this either by literally adding in some new factors to represent each method, or just by allowing the residual covariances of like-method variables to be freely estimated.\n\n### Simulating Data Based on a DAG {.unnumbered}\n\nNow let's look at an example in detail. This example is taken from @Brown2006, chapter 6.\n\nSome researchers were curious about whether 'happiness' and 'sadness' are totally separate things vs two sides of a single shared spectrum. I guess the implication is that if they are totally separate things then I could be [happy and sad at the same time](https://www.youtube.com/watch?v=U5oIvfraRrU), whereas if they're two sides of a spectrum then I can only ever be one or the other.\n\nThis feels like a good factor analysis question! I can collect a bunch of data that I think map to 'happy' and a bunch of other data that I think map to 'sad', fit a CFA, and see whether the two factors have discriminant validity.\n\nThis is exactly what @Green-et-al-1993 did. They collected a few columns each for 'happy' and 'sad', fit a factor model, and fit a CFA. Each within-factor column had its own measurement approach, but shared a measurement approach with one of the columns of the other factor. So we are at risk of our estimate of between-factor correlations being confounding due to shared measurement approach, which could be hurting my case for discriminant validity!\n\nHere's how we can show this situation in a DAG:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set DAG coordinates\ndag_coords <- list(\n  x = c(\n    F1 = 1, \n    F2 = 1,\n    H1 = 2,\n    H2 = 2,\n    H3 = 2,\n    S1 = 2,\n    S2 = 2,\n    S3 = 2,\n    M1 = 3,\n    M2 = 3,\n    M3 = 3),\n  y = c(\n    F1 = 2.5,\n    F2 = 1.5,\n    H1 = 2.8,\n    H2 = 2.5,\n    H3 = 2.2,\n    S1 = 1.8,\n    S2 = 1.5,\n    S3 = 1.2,\n    M1 = 2.6,\n    M2 = 2,\n    M3 = 1.4\n  )\n)\n\n# Set DAG relationships and aesthetics\nmeasurement_confounding_dag <- ggdag::dagify(\n  H1 ~ F1,\n  H2 ~ F1,\n  H3 ~ F1,\n  S1 ~ F2,\n  S2 ~ F2,\n  S3 ~ F2,\n  H1 ~ M1,\n  S1 ~ M1,\n  H2 ~ M2,\n  S2 ~ M2,\n  H3 ~ M3,\n  S3 ~ M3,\n  coords = dag_coords\n) %>% \n  \n  tidy_dagitty() %>% \n  \n  mutate(\n    \n    node_colour = case_when(\n      grepl(\"^F|M\", name) ~ \"latent\",\n      grepl(\"^H|S\", name) ~ \"observed\"\n    ),\n    \n    edge_colour = case_when(\n      grepl(\"^M\", name) & grepl(\"1$\", to) ~ \"cornflower blue\",\n      grepl(\"^M\", name) & grepl(\"2$\", to) ~ \"#daed64\",\n      grepl(\"^M\", name) & grepl(\"3$\", to) ~ \"#ed7864\",\n      grepl(\"^F\", name)                   ~ \"black\"\n    )\n  )\n\n# Plot the DAG\nmeasurement_confounding_dag %>%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(colour = node_colour)) +\n  scale_colour_manual(values = c(\"dark blue\", \"#edbc64\")) + \n  geom_dag_edges(aes(edge_colour = edge_colour)) +\n  geom_dag_text() +\n  theme_void()\n```\n\n::: {.cell-output-display}\n![](mtmm-and-error-structure-modelling_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nSee how the measurment effects M1, M2, and M3 each create a backdoor path between the two factors F1 and F2. So if I want to get better-seeming (and, under the DAG, more accurate) estimate of between-factor correlation, then I need to find a way to close those backdoor paths. The classic way to close these paths would be to condition on the measurement effects by adding them to the linear model, but I can't directly do this because they are unmeasured. But, as discussed above, I can still sort of do it by adding them as factors to my CFA model, or by freely estimating residual correlation between the observed variables that share a measurement approach, which should work if my DAG is mostly accurate.\n\nUnfortunately, the authors of this paper haven't published their data. But we can take this as an opportunity to practice simulating a dataset with relationships implied by a DAG.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Simulate Data from the DAG\n\n# Set seed for replicable results\nset.seed(233)\n\n# Set sample size\nN <- 305\n\n# Create the dataset\ndat_fake <- tibble(\n  \n  # The factors are uncorrelated in reality, but\n  # will be confounded by the measurement effects!\n  F1 = rnorm(N, 0, 1),\n  F2 = rnorm(N, 0, 1),\n  \n  # The measurement effects\n  M1 = rnorm(N, 0, 1),\n  M2 = rnorm(N, 0, 1),\n  M3 = rnorm(N, 0, 1),\n  \n  # The DAG says the measurements are fully determined by the latent factors and measurement effects\n  H1 = .8*F1 + 0.7*M1 + rnorm(N, 0, .3),\n  H2 = .7*F1 + 0.7*M2 + rnorm(N, 0, .3),\n  H3 = .9*F1 + 0.7*M3 + rnorm(N, 0, .3),\n  S1 = .8*F2 + 0.7*M1 + rnorm(N, 0, .3),\n  S2 = .7*F2 + 0.7*M2 + rnorm(N, 0, .3),\n  S3 = .9*F2 + 0.7*M3 + rnorm(N, 0, .3) \n) \n```\n:::\n\n\nFun! Now we have our fake data to play with. For starters, since we actually *do* have the values of the latent variables in our dataset, we can demonstrate how directly controlling for the measurement effects in a regression model can close the backdoor path between the factors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlist(\n  lm(H1 ~ S1, dat_fake), \n  lm(H1 ~ S1 + M1, dat_fake)\n) %>% \n  \n  map(broom::tidy) %>% \n  \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n<table class=\"kable_wrapper\">\n<tbody>\n  <tr>\n   <td> \n\n|term        |  estimate| std.error| statistic|   p.value|\n|:-----------|---------:|---------:|---------:|---------:|\n|(Intercept) | 0.0249271| 0.0572795|  0.435184| 0.6637387|\n|S1          | 0.4555127| 0.0491323|  9.271137| 0.0000000|\n\n </td>\n   <td> \n\n|term        |   estimate| std.error|  statistic|   p.value|\n|:-----------|----------:|---------:|----------:|---------:|\n|(Intercept) |  0.0466841| 0.0456881|  1.0217995| 0.3076936|\n|S1          | -0.0137610| 0.0528505| -0.2603763| 0.7947509|\n|M1          |  0.7636731| 0.0577500| 13.2237680| 0.0000000|\n\n </td>\n  </tr>\n</tbody>\n</table>\n:::\n:::\n\n\nWhen we just do the simple regression of H1 on S1 we get a big effect with a highly statistically significant p-value, despite the fact that we *know* there's no causal relationship there! But then when we include the confounding measurement effect in the model this effect vanishes in smoke.\n\nThat's all well and good. But in reality we won't have measurements of the latent variables, so we won't be able to directly control for them. Thankfully, we have Factor Analysis. We can control for the measurement effects by estimating the residual correlation between each pair of variables that share a measurement effect. Since, under the DAG, the measurement effects are the only source of correlation between these variables, this should close the backdoor path, IE we should get unbiased estimates of the factor loadings.\n\n....\\@Brown2006 calls this an \"error theory\".....\n\n\n### Correlated Uniqueness Model {.unnumbered}\n\nTo illustrate, we'll fit 2 models: The first is a basic CFA model that just loads each measured variable on its corresponding factor. The second specifies that the residual correlation between the measurement-confounded variables should be freely estimated, IE not fixed at 0.\n\nFirst let's define our utility function like we did in the previous chapter:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Define a custom function\nfit_measures <- function(fit){\n  \n  summary <- summary(fit, fit.measures = TRUE, standardized = TRUE)\n  \n  res <- list(\n    \n    # Chi-Squared\n    chi_squared = tibble(\n      Test             = \"standard chi-squared\",\n      `DF`             = summary$test$standard$df,\n      `Test Statistic` = round(summary$test$standard$stat, 2),\n      `p-value`        = summary$test$standard$pvalue) %>% \n      \n      mutate(across(everything(), as.character)) %>% \n      \n      pivot_longer(everything()),\n    \n    # RMSEA\n    rmsea = summary$fit %>% \n      \n      as_tibble(rownames = \"stat\") %>% \n      \n      filter(str_detect(stat, \"rmsea\")),\n    \n    # CFI and TLI\n    cfi_tli = summary$fit %>% \n      \n      as_tibble(rownames = \"stat\") %>% \n      \n      filter(str_detect(stat, \"cfi|tli\")) \n    \n  )\n  \n  res\n  \n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbasic.definition <- \n  'happy =~ H1 + H2 + H3\n   sad =~ S1 + S2 + S3\n   '\n\ncorrelated_uniqueness.definition <- \n  'happy =~ H1 + H2 + H3\n   sad =~ S1 + S2 + S3\n   \n   H1 ~~ S1\n   H2 ~~ S2\n   H3 ~~ S3\n   '\nbasic.fit <- cfa(\n  data = dat_fake %>% select(matches(\"^(H|S)\")),\n  model = basic.definition\n)\n\ncorrelated_uniqueness.fit <- cfa(\n  data = dat_fake %>% select(matches(\"^(H|S)\")),\n  model = correlated_uniqueness.definition\n)\n\nsummary.basic.fit <- summary(basic.fit, standardized = TRUE)\nsummary.correlated_uniqueness.fit <- summary(correlated_uniqueness.fit, standardized = TRUE)\n\nsummary.basic.fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nlavaan 0.6.16 ended normally after 23 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        13\n\n  Number of observations                           305\n\nModel Test User Model:\n                                                      \n  Test statistic                               802.905\n  Degrees of freedom                                 8\n  P-value (Chi-square)                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  happy =~                                                              \n    H1                1.000                               0.816    0.723\n    H2                0.741    0.090    8.236    0.000    0.605    0.615\n    H3                1.044    0.123    8.496    0.000    0.852    0.741\n  sad =~                                                                \n    S1                1.000                               0.906    0.778\n    S2                0.818    0.079   10.302    0.000    0.742    0.704\n    S3                0.980    0.093   10.522    0.000    0.888    0.752\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  happy ~~                                                              \n    sad               0.236    0.060    3.934    0.000    0.319    0.319\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .H1                0.609    0.084    7.214    0.000    0.609    0.478\n   .H2                0.601    0.062    9.662    0.000    0.601    0.621\n   .H3                0.597    0.089    6.721    0.000    0.597    0.451\n   .S1                0.537    0.077    6.989    0.000    0.537    0.395\n   .S2                0.560    0.062    8.964    0.000    0.560    0.505\n   .S3                0.604    0.078    7.726    0.000    0.604    0.434\n    happy             0.667    0.114    5.860    0.000    1.000    1.000\n    sad               0.822    0.119    6.886    0.000    1.000    1.000\n```\n:::\n\n```{.r .cell-code}\nsummary.correlated_uniqueness.fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nlavaan 0.6.16 ended normally after 47 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        16\n\n  Number of observations                           305\n\nModel Test User Model:\n                                                      \n  Test statistic                                13.448\n  Degrees of freedom                                 5\n  P-value (Chi-square)                           0.020\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  happy =~                                                              \n    H1                1.000                               0.738    0.669\n    H2                0.915    0.055   16.787    0.000    0.676    0.657\n    H3                1.157    0.066   17.625    0.000    0.854    0.755\n  sad =~                                                                \n    S1                1.000                               0.866    0.744\n    S2                0.863    0.042   20.433    0.000    0.747    0.724\n    S3                1.057    0.048   21.972    0.000    0.915    0.761\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n .H1 ~~                                                                 \n   .S1                0.551    0.061    9.007    0.000    0.551    0.866\n .H2 ~~                                                                 \n   .S2                0.458    0.051    8.973    0.000    0.458    0.831\n .H3 ~~                                                                 \n   .S3                0.501    0.062    8.133    0.000    0.501    0.868\n  happy ~~                                                              \n    sad               0.032    0.050    0.638    0.524    0.050    0.050\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .H1                0.672    0.070    9.641    0.000    0.672    0.552\n   .H2                0.600    0.061    9.813    0.000    0.600    0.568\n   .H3                0.550    0.071    7.769    0.000    0.550    0.430\n   .S1                0.604    0.066    9.121    0.000    0.604    0.446\n   .S2                0.507    0.053    9.538    0.000    0.507    0.476\n   .S3                0.607    0.069    8.789    0.000    0.607    0.420\n    happy             0.545    0.073    7.450    0.000    1.000    1.000\n    sad               0.749    0.086    8.762    0.000    1.000    1.000\n```\n:::\n\n```{.r .cell-code}\nfit_measures(basic.fit) %>% \n  \n  knitr::kable(caption = \"Basic Model\")\n```\n\n::: {.cell-output-display}\n<table class=\"kable_wrapper\">\n<caption>Basic Model</caption>\n<tbody>\n  <tr>\n   <td> \n\n|name           |value                |\n|:--------------|:--------------------|\n|Test           |standard chi-squared |\n|DF             |8                    |\n|Test Statistic |802.9                |\n|p-value        |0                    |\n\n </td>\n   <td> \n\n|stat                  |     value|\n|:---------------------|---------:|\n|rmsea                 | 0.5707720|\n|rmsea.ci.lower        | 0.5377551|\n|rmsea.ci.upper        | 0.6044999|\n|rmsea.ci.level        | 0.9000000|\n|rmsea.pvalue          | 0.0000000|\n|rmsea.close.h0        | 0.0500000|\n|rmsea.notclose.pvalue | 1.0000000|\n|rmsea.notclose.h0     | 0.0800000|\n\n </td>\n   <td> \n\n|stat |      value|\n|:----|----------:|\n|cfi  |  0.3742618|\n|tli  | -0.1732591|\n\n </td>\n  </tr>\n</tbody>\n</table>\n:::\n\n```{.r .cell-code}\nfit_measures(correlated_uniqueness.fit) %>% \n  \n  knitr::kable(caption = \"Correlated Uniqueness Model\")\n```\n\n::: {.cell-output-display}\n<table class=\"kable_wrapper\">\n<caption>Correlated Uniqueness Model</caption>\n<tbody>\n  <tr>\n   <td> \n\n|name           |value                |\n|:--------------|:--------------------|\n|Test           |standard chi-squared |\n|DF             |5                    |\n|Test Statistic |13.45                |\n|p-value        |0.0195200124719597   |\n\n </td>\n   <td> \n\n|stat                  |      value|\n|:---------------------|----------:|\n|rmsea                 | 0.07443086|\n|rmsea.ci.lower        | 0.02734066|\n|rmsea.ci.upper        | 0.12377511|\n|rmsea.ci.level        | 0.90000000|\n|rmsea.pvalue          | 0.16609111|\n|rmsea.close.h0        | 0.05000000|\n|rmsea.notclose.pvalue | 0.47823144|\n|rmsea.notclose.h0     | 0.08000000|\n\n </td>\n   <td> \n\n|stat |     value|\n|:----|---------:|\n|cfi  | 0.9933495|\n|tli  | 0.9800485|\n\n </td>\n  </tr>\n</tbody>\n</table>\n:::\n:::\n\n\nHere we see that under the basic model we have some moderate correlation between the `happy` and `sad` factors, which is a bit of a murky result: it doesn't tell us one way or the other whether happiness and sadness are separate constructs I can feel together or two extremes of the same feeling. But under the correlated uniqueness model this correlation evaporates because we've controlled for the measurement effects, closing the backdoor path between `happy` and `sad`. This model also greatly improves goodness-of-fit, which makes sense because it better reflects the true data-generating process we coded up.\n\nWe also could have controlled for the measurement effects by including measurement factors, IE by adopting a 'Correlated Methods Model'. I tried this but I actually I couldn't get this model to converge, regardless of whether its method factors were correlated or uncorrelated (an 'Uncorrelated Methods Model'. @Brown2006 actually mentions this as a common issue, and favours the Correlated Uniqueness Model for that reason. In his words:\n\n> \"an overriding drawback of the correlated methods model is that it is usually empirically underidentified. Consequently, a correlated methods solution will typically fail to converge. If it does converge, the solution will usually be associated with Heywood cases \\[negative variance estimates\\] and large standard errors\"\n\nNow let's consider the other case in which measurement effects might be hurting us: the case in which *within*-factor measurements are confounded by measurement effects. Here's the DAG:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndag_coords <- list(\n  x = c(\n    F1 = 1, \n    F2 = 1,\n    H1 = 2,\n    H2 = 2,\n    H3 = 2,\n    S1 = 2,\n    S2 = 2,\n    S3 = 2,\n    M1 = 3,\n    M2 = 3),\n  y = c(\n    F1 = 2.5,\n    F2 = 1.5,\n    H1 = 2.8,\n    H2 = 2.5,\n    H3 = 2.2,\n    S1 = 1.8,\n    S2 = 1.5,\n    S3 = 1.2,\n    M1 = 2.5,\n    M2 = 1.5\n  )\n)\n\n# Set DAG relationships and aesthetics\nmeasurement_confounding_dag <- ggdag::dagify(\n  H3 ~ F1,\n  S2 ~ F2,\n  H1 ~ M1,\n  H2 ~ M1,\n  H3 ~ M1,\n  S1 ~ M2,\n  S2 ~ M2,\n  S3 ~ M2,\n  coords = dag_coords\n) %>% \n  \n  tidy_dagitty() %>% \n  \n  mutate(\n    \n    node_colour = case_when(\n      grepl(\"^F|M\", name) ~ \"latent\",\n      grepl(\"^H|S\", name) ~ \"observed\"\n    ),\n    \n    edge_colour = case_when(\n      grepl(\"M1\", name)  ~ \"cornflower blue\",\n      grepl(\"M2\", name) ~ \"#ed7864\",\n      grepl(\"^XX\", name) & grepl(\"3$\", to) ~ \"#ed7864\",\n      grepl(\"^F\", name)                   ~ \"black\"\n    )\n  )\n\n# Plot the DAG\nmeasurement_confounding_dag %>%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(colour = node_colour)) +\n  scale_colour_manual(values = c(\"dark blue\", \"#edbc64\")) + \n  geom_dag_edges(aes(edge_colour = edge_colour)) +\n  geom_dag_text() +\n  theme_void()\n```\n\n::: {.cell-output-display}\n![](mtmm-and-error-structure-modelling_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nThis is the 'true' data-generating process we'll be simulating data from in a moment. Notice that even though the researcher (who can't see this DAG) might think that the unobseved factor causally influences all 3 measured variables, the reality is that each factor only influences one of the measured variables. However, the purported within-factor variables are confounded by measurement method.\n\nLet's simulate the data and analyze:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set seed for replicable results\nset.seed(233)\n\n# Set sample size\nN <- 30000\n\n# Create the dataset\ndat_fake <- tibble(\n  \n  # Create some uncorrelated factors\n  F1 = rnorm(N, 0, 1),\n  F2 = rnorm(N, 0, 1),\n  \n  # Create some measurement effects\n  M1 = rnorm(N, 0, 1),\n  M2 = rnorm(N, 0, 1),\n  \n  # The DAG says only H3 and S2 are influenced by the factors, but all variables are influenced by a measurement effect.\n  H1 = 0.7*M1 + rnorm(N, 0, .3),\n  H2 = 0.8*M1 + rnorm(N, 0, .3),\n  H3 = 0.9*F1 + 0.8*M1 + rnorm(N, 0, .3),\n  S1 = 0.7*M2 + rnorm(N, 0, .3),\n  S2 = 0.7*F2 + 0.8*M2 + rnorm(N, 0, .3),\n  S3 = 0.7*M2 + rnorm(N, 0, .3) \n) \n```\n:::\n\n\nFirst let's fit a basic naive CFA model that does the standard thing of keeping the covariances between variables fixed at 0. Based on the DAG, we should expect this model to return a strong (publishable) but misleading answer -- it will notice the correlation between variables that are considered within-factor under our hypothesis, and say 'wow so correlated, that's consistent with them being *caused* by that factor'. But we know this is wrong: their correlation is simply driven by the shared measurement method:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbasic.definition <- \n  'happy =~ H1 + H2 + H3\n   sad =~ S1 + S2 + S3\n   '\n\nbasic.fit <- cfa(\n  data = dat_fake,\n  model = basic.definition\n)\n\nsummary(basic.fit, standardized = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nlavaan 0.6.16 ended normally after 25 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        13\n\n  Number of observations                         30000\n\nModel Test User Model:\n                                                      \n  Test statistic                                 7.086\n  Degrees of freedom                                 8\n  P-value (Chi-square)                           0.527\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  happy =~                                                              \n    H1                1.000                               0.705    0.920\n    H2                1.141    0.006  196.951    0.000    0.804    0.936\n    H3                1.142    0.009  129.172    0.000    0.805    0.646\n  sad =~                                                                \n    S1                1.000                               0.696    0.917\n    S2                1.142    0.008  151.721    0.000    0.795    0.722\n    S3                1.004    0.005  206.981    0.000    0.699    0.922\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  happy ~~                                                              \n    sad              -0.002    0.003   -0.765    0.444   -0.005   -0.005\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .H1                0.090    0.002   42.749    0.000    0.090    0.154\n   .H2                0.091    0.003   34.103    0.000    0.091    0.124\n   .H3                0.903    0.008  115.595    0.000    0.903    0.582\n   .S1                0.091    0.002   49.153    0.000    0.091    0.158\n   .S2                0.581    0.005  110.971    0.000    0.581    0.479\n   .S3                0.086    0.002   46.614    0.000    0.086    0.150\n    happy             0.497    0.005   96.783    0.000    1.000    1.000\n    sad               0.484    0.005   98.040    0.000    1.000    1.000\n```\n:::\n:::\n\n\nAnd there you have it -- just as foretold, we have super strong factor loadings for all the variables, even those that are not actually causally influenced by the factor! So it may *look* like I have strong convergent validity, but hopefully if we try to publish this, a reviewer will raise the possibility that these correlations are confounded by measurement effects.\n\nNow I'm going to try closing the backdoor paths between the non-factor-caused variables by allowing the model to learn the covariances, thereby hopefully controlling for unobserved sources of confounding (like the measurement effect). If the loadings stay strong, then my claims to convergent validity are more reasonable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorrelated_uniqueness.definition <- \n  'happy =~ H1 + H2 + H3\n   sad   =~ S1 + S2 + S3\n   \n   H1 ~~ H2\n   H1 ~~ H3\n   H2 ~~ H3\n   S1 ~~ S2\n   S1 ~~ S3\n   S2 ~~ S3\n   '\n\ncorrelated_uniqueness.fit <- cfa(\n  data = dat_fake,\n  model = correlated_uniqueness.definition\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in lav_model_vcov(lavmodel = lavmodel, lavsamplestats = lavsamplestats, : lavaan WARNING:\n    Could not compute standard errors! The information matrix could\n    not be inverted. This may be a symptom that the model is not\n    identified.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in lav_object_post_check(object): lavaan WARNING: some estimated ov\nvariances are negative\n```\n:::\n\n```{.r .cell-code}\nsummary(correlated_uniqueness.fit, standardized = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nlavaan 0.6.16 ended normally after 593 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        19\n\n  Number of observations                         30000\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.453\n  Degrees of freedom                                 2\n  P-value (Chi-square)                           0.797\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  happy =~                                                              \n    H1                1.000                               0.922    1.203\n    H2                0.712       NA                      0.656    0.764\n    H3               -2.144       NA                     -1.976   -1.587\n  sad =~                                                                \n    S1                1.000                               0.571    0.752\n    S2                0.393       NA                      0.224    0.204\n    S3                2.194       NA                      1.252    1.653\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n .H1 ~~                                                                 \n   .H2               -0.038       NA                     -0.038   -0.135\n   .H3                2.388       NA                      2.388    3.038\n .H2 ~~                                                                 \n   .H3                1.944       NA                      1.944    2.288\n .S1 ~~                                                                 \n   .S2                0.425       NA                      0.425    0.789\n   .S3               -0.228       NA                     -0.228   -0.459\n .S2 ~~                                                                 \n   .S3                0.274       NA                      0.274    0.255\n  happy ~~                                                              \n    sad              -0.001       NA                     -0.002   -0.002\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .H1               -0.263       NA                     -0.263   -0.447\n   .H2                0.307       NA                      0.307    0.416\n   .H3               -2.353       NA                     -2.353   -1.517\n   .S1                0.250       NA                      0.250    0.434\n   .S2                1.162       NA                      1.162    0.958\n   .S3               -0.994       NA                     -0.994   -1.731\n    happy             0.850       NA                      1.000    1.000\n    sad               0.326       NA                      1.000    1.000\n```\n:::\n:::\n\n\nUh-oh...the model failed to converge :(. Apparently this is a common thing with CFA models that try to learn the correlation between within-factor variables -- the parameters are non-identified because you're asking the model to learn their correlation simultaneously in two different parameters: the factor loading and the covariance parameter. [This Stack Exchange thread](https://stackoverflow.com/questions/44114501/model-identification-in-lavaan-for-r) explains it nicely.",
    "supporting": [
      "mtmm-and-error-structure-modelling_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}